{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[ 0.69521716 -0.91474643 -0.39967297]\n",
      " [-1.32394523  0.62886503  1.88322633]\n",
      " [-1.37325612  0.57819388  0.50216567]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "random = np.random.randn(3, 3) # 3x3 matrix of random values (normal distribution)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [ 7.44888617e+00  8.51439109e+00  4.76819250e+00  7.00848738e+00\n",
      "  9.59302865e+00  7.89170486e+00  1.03254880e+01  2.17497640e+00\n",
      "  1.79651512e+00  7.41609083e+00 -2.70663271e+00 -1.82559453e+00\n",
      "  1.02842936e+01  3.06614186e+00  3.60478338e+00  6.75823062e+00\n",
      "  1.23440031e+01  7.51657687e+00  2.54369234e+00  4.68547093e+00\n",
      "  3.03195548e+00  1.79810642e+00  1.88606865e+00  4.75193072e+00\n",
      " -4.47752253e+00  5.89062924e+00  1.18105049e+01 -2.35809341e-01\n",
      "  6.51226562e+00  8.28156408e+00  7.50817991e-01  3.52178182e+00\n",
      "  4.68621729e+00  8.85365535e+00  6.88061284e+00 -5.31682221e-01\n",
      "  2.89750466e+00  8.43102968e+00  3.50779472e+00 -2.09336377e-01\n",
      "  1.18691115e+01  3.42798148e+00  5.15044645e+00  9.52321516e+00\n",
      " -1.61317286e-01  1.03581631e+01  1.12762494e+01  1.35174276e+01\n",
      "  9.27959471e-01  9.77768304e-01  3.26837989e+00  3.36722661e+00\n",
      "  5.28617855e+00  1.09051200e+01  1.16346435e+01  5.79354077e+00\n",
      "  5.81302870e+00  4.14996338e+00  2.67774306e+00  1.29136208e+01\n",
      "  7.64504740e+00  7.81806532e+00  4.93118587e+00  4.81770607e+00\n",
      "  1.00480446e+01  9.09472345e+00  9.65349452e+00  3.91159278e+00\n",
      "  6.29960180e+00  1.00040022e+01  3.59830393e+00  8.54467869e+00\n",
      "  3.55516937e+00 -5.73845449e+00  1.13091281e+01  5.33630396e+00\n",
      "  9.52505126e-02  4.29601522e+00  4.03145772e+00  8.96781126e+00\n",
      "  4.57401699e+00  1.04307960e+01  8.98494305e+00  1.15998502e+01\n",
      "  3.35162903e+00 -1.46416137e+00  1.78602396e+00  1.46888035e+01\n",
      "  1.13107218e+01  3.87598381e+00  1.46528097e+01  9.14890161e+00\n",
      "  7.60358159e+00  4.82525811e+00  1.57039379e+01  2.50954804e+00\n",
      "  7.42924868e+00  7.35117793e+00  9.48861437e+00  3.71402489e+00\n",
      "  6.42358403e+00  6.46988718e+00  1.04464757e+01  7.00566418e+00\n",
      "  5.76176718e+00  2.18977215e+00  6.24932734e+00  1.21926674e+01\n",
      "  8.97479169e+00  5.34142429e+00  8.22960359e+00  1.21714378e-02\n",
      "  5.09325323e+00  7.11585310e+00  6.55767680e+00  8.42119941e+00\n",
      "  4.77653794e+00  4.70899596e+00  8.44305066e+00  2.05232135e+00\n",
      "  5.10966610e-01  2.78608877e+00  5.55062571e+00  4.11519226e+00\n",
      "  7.77461301e+00  6.87195654e-01  1.09490671e+01  1.04462172e+01\n",
      "  3.46799039e+00  4.56050276e+00 -4.98896254e-01  3.83111363e+00\n",
      " -3.33663214e+00  1.21570669e+01  4.17095711e+00  9.76880214e+00\n",
      "  4.50747766e+00  1.00366306e+01  1.06110589e+01  1.20508685e+00\n",
      "  2.60435668e+00  6.73727341e+00  2.25660970e+00  2.56507755e+00\n",
      "  4.20332401e+00 -2.62211704e+00  2.36375024e+00  1.20369778e+01\n",
      "  5.97104447e-01  3.32995436e+00  5.30546142e-02  6.67468545e+00\n",
      "  7.57881345e+00 -4.21939842e+00  2.68312722e+00  3.54473892e+00\n",
      "  5.54178400e+00  7.74963779e+00  1.00263970e+01 -3.47422015e+00\n",
      "  5.67888688e+00  5.92077432e+00  1.00229847e+01  9.01109997e+00\n",
      "  1.04539174e+01  9.42267361e+00  9.47782887e+00  1.45582736e+01\n",
      "  4.51506282e+00  1.85576759e+00  5.35832496e+00  6.33665917e+00\n",
      "  9.57227619e+00  1.31607330e+01  5.32768407e-01  7.15876175e+00\n",
      "  9.64244188e+00  9.17148711e-02  2.23981239e+00 -1.56473725e-01\n",
      "  1.14200645e+01  1.53385501e+01  7.42152192e+00  3.25544952e+00\n",
      "  6.95149516e+00  5.23403057e+00  7.69701225e+00  5.61282984e+00\n",
      "  5.77350485e+00  9.00432888e+00 -3.40302774e-01 -1.65782356e+00\n",
      "  5.55321569e-01  5.47983879e+00 -1.40906189e+00  5.82943777e+00\n",
      "  9.76431438e+00  2.86874887e+00  8.45532488e+00  5.16344972e+00\n",
      "  1.10459647e+01  9.07944728e+00  5.14063269e+00  5.61843182e+00\n",
      "  1.31677769e+01  5.96421052e+00  3.99271270e+00  1.37398105e+00\n",
      "  8.49122089e+00  6.19467624e+00 -3.97536723e+00  5.20631817e+00\n",
      "  4.45478693e+00  6.08862119e+00  9.37320304e+00  1.38122138e+01\n",
      "  4.87484281e+00  6.53445161e+00  9.96034903e+00  4.04191255e+00\n",
      "  5.71755238e+00  6.56775604e+00 -4.75038461e+00 -2.88369819e-01\n",
      "  3.50340030e+00  9.95978775e+00  3.50342205e+00  5.31177923e+00\n",
      " -3.00238221e-01  2.42839346e+00 -1.40118163e+00  3.69169715e+00\n",
      "  1.14172041e+01  8.03324014e+00  6.18598340e+00  1.33257139e+01\n",
      " -4.78492990e+00  7.31249475e+00  1.23451548e+00  6.48782783e+00\n",
      "  9.57891478e+00  5.61252415e+00  3.01069238e+00  1.29037299e+00\n",
      "  1.33192816e+01  6.52312737e+00  2.01934231e+00  4.66684766e+00\n",
      "  8.85557153e+00  3.51955480e+00  6.42061975e+00  1.43787528e+01\n",
      "  4.05217270e+00  8.59836687e+00  1.21802887e+01  6.37230782e+00\n",
      " -1.53587617e+00  3.12047104e+00  1.06487007e+01  5.69648788e+00\n",
      "  1.05994040e+01  5.88307601e+00  4.95546546e+00  4.85945897e-01\n",
      "  8.39019288e+00  8.26699979e+00  6.35991597e+00  2.93807489e+00\n",
      "  5.54631063e+00 -1.69455464e+00  8.33578224e+00 -4.46686895e-01\n",
      "  2.54381873e+00  1.10554915e+01  1.01994774e+01  1.59397640e+00\n",
      "  3.12919673e+00  5.52549244e+00  6.43662940e+00 -7.62819742e-01\n",
      "  8.55121326e-01  5.92091366e+00 -3.97182318e+00  5.28578225e+00\n",
      "  1.69442426e+00  7.47887525e+00  1.98467708e+00 -7.53939925e-01\n",
      "  5.56017616e+00  6.37931543e+00  5.37116706e+00  6.34592112e+00\n",
      "  3.15688523e+00 -2.26378010e+00  1.38939528e+00  6.11072787e+00\n",
      "  3.53157040e+00  4.03513003e+00  5.24315626e+00 -1.54757516e-01\n",
      "  7.15714970e+00  6.95771181e+00  6.36626509e-01  1.07324588e+01\n",
      " -1.65262410e-01  3.71031895e+00  5.03826542e+00  1.43139432e+00\n",
      "  1.18828647e+01  1.76464256e-01 -4.86646965e+00 -1.69074382e+00\n",
      "  8.43987871e-01  9.66159413e+00  9.41747337e+00  7.16130646e+00\n",
      "  3.69528581e+00  1.22501457e+01  1.43925025e+00  4.59088380e+00\n",
      "  1.92819342e+00  8.89225827e+00 -2.71631305e+00  6.68506857e-01\n",
      "  8.29619588e+00 -8.31828334e-01  3.49443294e-01  9.16376459e+00\n",
      "  1.13021688e+01  7.56842795e+00  1.21920716e+01  6.72005110e+00\n",
      "  5.49565455e+00  2.63437206e+00  8.43087443e+00  3.60655220e+00\n",
      "  1.09485198e+01 -2.90199557e+00  6.80211286e+00  5.78969283e+00\n",
      "  8.01708910e+00  3.89262513e+00  5.57617427e+00  1.35235664e+00\n",
      "  2.80848076e+00  1.02415910e+01  4.30995112e+00  2.49085853e-01\n",
      "  2.54041994e+00 -1.25053851e+00  3.10460751e+00  6.43998052e+00\n",
      "  1.65974052e+00 -2.71498808e+00  1.28428682e+01  9.03328556e+00\n",
      "  2.78145314e+00  3.76071670e+00  6.38580883e+00  1.66443024e-01\n",
      "  5.04264512e+00  8.46178436e+00  5.83624163e+00  4.80207136e+00\n",
      "  7.37240860e+00  1.56604265e+00  7.11696085e+00 -8.31120667e+00\n",
      "  1.17751972e+01  7.94740727e+00  5.16272174e+00  6.01233911e+00\n",
      "  1.03101455e+01  5.01251349e+00  1.27281249e+01  5.16671311e-01\n",
      "  4.20309069e+00  8.76153519e-01  2.92158737e+00  1.50564324e+00\n",
      " -6.35184234e-01  5.88027286e+00  1.02393855e+01  3.54586536e+00\n",
      "  1.08159135e+01  1.28068141e+01  4.35133224e+00  1.19772907e+01\n",
      "  5.82416670e-01  5.32630138e+00  5.68998885e+00  7.09267719e+00\n",
      "  4.75388405e-01  4.25119527e+00  1.08489431e+01  9.98329853e+00\n",
      "  9.64737305e+00  4.91640867e+00  2.42894252e-01  7.36749108e+00\n",
      "  6.93681465e+00  5.97647335e+00  3.81226193e+00  1.18133170e+01\n",
      " -1.14743679e-02  1.02711926e+01 -5.96011368e+00  4.02212098e+00\n",
      "  5.14951434e+00  4.64820359e+00  8.54690379e+00  1.54309549e+01\n",
      "  1.11464351e+01  9.27836980e+00  6.41437041e+00  1.27786748e+00\n",
      "  1.14095269e+01  1.42062175e+01  1.41979401e+01  8.22192590e+00\n",
      "  6.65876894e+00  4.45318184e-01  6.83792971e+00  4.56630811e+00\n",
      "  5.74245059e+00 -2.24424280e+00  6.66631721e+00  3.65173343e+00\n",
      "  5.33302836e+00  6.64842989e+00 -2.95744747e+00  6.24224431e+00\n",
      "  8.25520234e+00  9.46977572e+00  8.09995331e+00  3.09075149e+00\n",
      "  5.47280688e+00  2.60652164e+00  8.01446156e+00  6.24238634e+00\n",
      "  3.15362704e+00  2.78414299e+00  1.81109354e+00  2.67037232e+00\n",
      " -4.21426035e-01  6.01903285e+00  3.86805056e+00  5.26277786e+00\n",
      "  8.26536946e+00  1.13155672e+01  9.77889957e+00  4.64953328e+00\n",
      "  3.17829731e-01  6.28924156e+00  1.04397593e+01  3.37860480e+00\n",
      "  6.95082591e+00  1.08047289e+01  7.67136442e+00  1.19384783e+01\n",
      "  1.12995040e+01  1.64017522e+00  2.18619961e+00  8.66688704e-01\n",
      "  8.00091851e+00  8.62688157e+00  8.12959132e+00  3.00177093e+00\n",
      "  7.50816218e+00  8.75579722e+00  6.39341491e+00  1.29740787e+01\n",
      "  6.46665591e+00  5.74039624e+00  5.64011459e+00 -7.78425068e+00\n",
      "  5.98744935e+00  2.74520106e+00  3.30369006e+00  7.59859550e+00\n",
      "  2.49013637e+00  3.03929323e-02  3.74791718e+00  6.51495003e+00\n",
      " -2.77612007e+00 -5.86483615e-01  5.01568680e+00  2.00720425e+00\n",
      "  4.28378544e+00  1.06418378e+01  9.32647739e+00  8.74894114e+00\n",
      "  1.95850188e+00  1.03749892e+00  1.01896814e+01  9.18824851e+00\n",
      " -3.29814421e+00  1.23680921e+01  4.15515016e+00  4.72130404e+00\n",
      "  7.25857383e+00  4.90905519e+00  3.18181699e+00  1.16604829e+01\n",
      "  9.97521249e+00  4.05648409e-01 -1.70614709e+00  5.51773090e+00\n",
      "  3.27184574e+00  8.11803388e+00  1.19560618e+01  3.94386798e+00\n",
      "  6.13677790e+00  2.01015514e+00  9.01903724e+00  9.88458540e+00\n",
      "  2.08766316e+00  6.42527102e+00  4.63163988e+00 -2.23745329e+00\n",
      "  3.93614590e+00  3.76714582e+00  9.84375387e-01 -7.19465868e-01\n",
      "  1.10347972e+01  1.23287874e+01  6.66755417e+00  1.09451708e+01\n",
      "  3.69642571e+00  7.29172592e+00  1.37352884e+01  1.35635371e+01\n",
      " -9.31735060e-01  6.24644629e+00  5.78112838e+00  5.10101437e+00\n",
      "  6.09167890e+00 -1.33402163e+00 -6.08870695e+00  6.71133805e+00\n",
      "  1.16393571e+01 -1.27275829e+00 -2.76417792e-01  3.73944751e+00\n",
      " -3.94125524e-01  8.17103070e+00  1.28977442e+01  8.78174367e+00\n",
      "  1.09542171e+00  6.69995427e+00  5.31055569e+00  2.66587185e+00\n",
      " -7.84483270e-01  2.49275160e+00  4.82915898e+00  7.69465931e+00\n",
      "  4.40927442e+00  4.25388942e+00  1.11846108e+01  3.90111349e+00\n",
      "  4.53659201e+00  1.15568769e+01  3.79163842e+00  1.71640626e+00\n",
      "  1.16430928e+01  1.13624002e+01  5.92522461e+00  5.42139888e+00\n",
      "  7.67289966e+00  7.80218803e+00  4.37649109e+00  8.43269208e+00\n",
      "  6.06138951e+00  1.94383695e+00  2.92067671e+00  1.02599230e+01\n",
      " -1.03426722e+00  3.91456535e+00  1.06764473e+01  2.10423764e+00\n",
      "  5.48705584e+00  1.09270460e+01  5.48093809e+00  1.74802182e+01\n",
      "  1.08658677e+01  9.65769871e+00  6.08230272e+00  3.81728853e+00\n",
      "  1.07156522e+01  5.04423962e+00 -1.77944779e+00  6.00782819e+00\n",
      "  3.26213727e+00  8.41261080e+00 -5.45976992e-01  2.03889416e+00\n",
      " -1.86598624e+00  6.79752706e+00  7.57970501e+00 -3.26109226e-01\n",
      "  7.95208407e+00  5.90735617e+00  1.38192967e+00  8.70097983e+00\n",
      "  4.68270768e+00  9.67023947e+00  7.83598242e+00  4.96569583e+00\n",
      "  2.15491845e+00  8.19519766e+00  7.02987734e+00  8.26003443e+00\n",
      "  2.28274363e+00  2.21087401e+00  3.81737762e+00 -2.78459000e+00\n",
      "  4.47439317e+00  1.41070820e+01  1.14370333e+01  3.87996203e+00\n",
      "  3.08378664e+00 -5.55599278e+00  1.48517478e+00  7.72595622e+00\n",
      "  3.62058269e+00  9.44512290e+00  5.90094630e+00  1.16626903e+01\n",
      "  2.75221915e+00  7.48488140e+00  5.41551172e+00  4.53503420e+00\n",
      " -2.71478050e-01  1.46896971e+00  5.89154839e+00  9.70334918e+00\n",
      "  4.71062165e+00  7.98068815e+00  2.11556720e+00  3.63867230e+00\n",
      " -3.18540006e+00 -3.15987250e+00  2.59187918e+00  1.39651673e+01\n",
      "  8.40671335e+00  1.80751029e+00  7.29070742e+00  2.75997016e+00\n",
      "  8.10243055e+00  9.23207910e-01 -4.32138864e+00  7.83893899e+00\n",
      " -8.66938308e+00  6.06693794e+00  4.61739700e+00  5.64293591e+00\n",
      "  1.53232318e+00 -2.88432570e+00  8.00856615e+00  1.02107032e+01\n",
      "  6.94527427e+00  4.15847742e+00  3.68134518e+00  4.19384875e+00\n",
      "  5.42593559e+00  5.91031291e+00  6.26825792e+00  7.02249009e+00\n",
      "  4.68050903e-01  2.46481810e+00  3.30070347e+00 -7.37073571e-02\n",
      "  1.24376156e+01  2.72778441e+00  7.77292035e+00  9.24329484e-01\n",
      "  8.35072938e+00  2.14386065e+00 -1.94976637e+00  1.41796898e+00\n",
      "  4.25692967e+00  9.86472551e+00  4.54112318e+00  1.12184212e+00\n",
      "  2.72800174e+00  6.33984111e+00  6.12390491e+00  3.20379682e+00\n",
      "  3.91215426e+00  1.14114630e+00  2.73732383e+00  7.08230376e+00\n",
      "  1.80877736e+00  4.99958374e+00  9.32311843e+00  4.11539536e+00\n",
      " -5.00016042e-01  1.71497404e+00  8.34911950e+00  9.52127570e+00\n",
      "  7.60550487e+00  1.38778206e+01  4.71459077e-01  7.86658451e+00\n",
      "  9.70073096e+00  3.28214851e+00  4.16285298e+00  6.24228366e+00\n",
      "  1.35879566e+00  5.80208737e+00  6.81758503e+00  6.56236810e+00\n",
      " -2.07651855e+00  1.56461808e+00  4.15804375e+00  1.23401453e+01\n",
      "  1.02808697e+01  8.59588898e+00  4.07414083e+00  7.15353115e-01\n",
      "  2.66164998e+00  2.25764386e-01  7.42877770e+00  2.36311941e+00\n",
      "  2.57643763e+00  1.50830019e+01  8.73835443e+00  6.97575275e+00\n",
      " -6.92667751e+00  6.85188072e+00  3.22261140e+00  6.16707137e-01\n",
      "  4.82252683e+00 -6.25829502e-01  9.53131667e+00  1.28382648e+00\n",
      "  2.50075242e+00  2.44272944e+00  2.47426230e+00 -2.85183004e+00\n",
      "  7.66600397e+00  8.89231334e+00  7.14087856e+00  1.05971702e+01\n",
      " -3.88105621e+00  4.98066337e+00  1.94265445e+00 -3.09507850e+00\n",
      "  6.52100330e+00  4.38376928e+00 -2.72090403e+00  9.90464698e+00\n",
      "  4.34012027e+00  4.47575349e+00  1.22405764e+01  1.40895493e+01\n",
      "  4.79848442e+00  6.44174978e+00  7.64495600e+00  1.06420512e+00\n",
      "  5.71102693e+00  5.35617520e+00  6.55090183e+00  3.22687798e+00\n",
      "  6.75389970e+00  6.68303233e+00  1.31396027e+01  4.72307851e+00\n",
      " -9.83295378e-01  4.09432013e+00 -1.57946886e+00  4.29713121e+00\n",
      "  8.00081452e+00  2.42984198e+00  4.64281873e-02  9.57778682e+00\n",
      "  2.97826771e+00  9.63381581e+00  5.51854735e+00  4.08434461e+00\n",
      "  5.66227981e+00  1.31303596e+01  8.10225863e+00  1.43170630e+00\n",
      "  1.29658996e+01  7.03005375e+00 -1.52681257e+00  9.29194914e+00\n",
      " -2.43027597e+00  7.34683705e+00  6.84723652e+00  4.26541846e+00\n",
      "  8.30143187e+00  7.45799088e+00  5.90705449e+00  2.90306211e+00\n",
      "  3.87742357e-01  7.40379477e+00  8.52409261e+00  2.37066611e+00\n",
      "  2.11763171e+00  6.10299041e-01  7.13868895e-01  2.14464955e+00\n",
      "  7.49266246e+00  4.12370748e+00  1.19950209e+01  8.07683216e+00\n",
      "  9.95979511e-01  4.39265360e-01  9.66270881e+00  3.30842336e+00\n",
      "  1.19074335e+00 -2.26451675e+00 -7.52964864e-01  4.14557828e+00\n",
      "  4.47507448e+00  2.49783962e+00  2.88662726e-01  4.43791452e+00\n",
      "  3.27589550e+00  8.13565186e-01  4.36563886e+00  4.37175794e+00\n",
      "  8.71741194e+00  9.43104831e+00 -1.01121801e+00  3.14967195e-01\n",
      "  3.11632149e-01  1.94080836e+00  6.64668748e+00  1.35048459e+01\n",
      "  6.60679257e+00  5.13523409e+00  4.85248601e+00 -5.10563218e+00\n",
      "  7.64925321e+00  3.79100248e+00  3.73035265e+00  8.85883101e-02\n",
      " -4.54676117e+00  1.87066720e+00  6.80102725e+00 -3.09060834e-01\n",
      "  8.70037786e+00  2.37214332e+00  1.25192608e+00  2.77982567e+00\n",
      "  6.27164733e+00  3.42827462e+00  1.12142165e+01  9.62019655e+00\n",
      "  2.08422008e-01  1.05257069e+01  9.12288119e-01  8.64585657e+00\n",
      "  7.59543122e+00  3.07637851e+00  1.63297366e+01  8.64942086e+00\n",
      "  4.30247278e+00  2.67097952e+00  1.87188000e+00 -1.44035103e-01\n",
      "  5.98391999e+00  2.94659790e+00  6.30698742e+00  6.67869126e+00\n",
      "  3.49003098e+00 -6.38551241e-01  9.98270653e+00  3.89368774e+00\n",
      "  3.93058856e+00  6.62945679e+00  8.45690442e-01 -5.87115807e-01\n",
      "  6.23700066e+00  5.70280967e+00  4.93546710e+00  1.10933454e+01\n",
      "  8.11032244e+00  6.33239499e+00  9.20189587e+00  1.22606269e+01\n",
      "  1.42780030e+00  1.18661904e+01  1.64390750e+00 -3.02960455e-01\n",
      "  1.19836562e+01  1.54034999e+00  1.96002677e+00  4.50831914e+00\n",
      "  1.13191261e+01  1.64303560e+00  2.28214401e+00  1.68651717e+00\n",
      "  2.84491189e+00  4.44766938e+00  2.28058629e+00  2.19171663e+00\n",
      "  4.97359122e+00  7.74247172e+00  4.67899366e+00  1.32898786e+01\n",
      "  1.14487738e+01 -6.14838497e-02  6.31410730e+00  9.91717825e+00\n",
      "  1.04294341e+01  9.55368789e+00  2.40852034e+00 -4.51079894e+00\n",
      "  5.46278265e+00  5.95085550e+00 -6.60799080e-01  1.83563718e+00\n",
      "  9.12150946e+00  9.55010237e+00  6.72545051e+00  8.37465637e+00\n",
      "  1.04171975e+01  4.08128223e+00  2.56653721e+00  4.20770732e+00\n",
      "  9.32988510e+00  4.96463198e+00  4.51020190e+00  6.34860592e+00\n",
      "  4.10232467e+00  4.27632950e+00  7.52154288e+00  1.98399753e+00\n",
      "  1.18573438e+01  8.99834437e+00  9.07059477e+00  4.80022370e-01\n",
      "  8.57902264e+00 -2.83895956e+00  6.53868301e+00  1.46766688e+00\n",
      "  1.41083454e+00  4.92919416e+00  9.13797138e+00  8.84383753e+00\n",
      "  8.71967735e+00  1.74783917e-01  1.04376554e+01 -4.95422492e+00\n",
      "  8.39080973e+00  4.64435450e+00 -6.70500471e-01  4.08431274e-01\n",
      "  6.14058622e+00  7.97328027e+00  1.02356838e+00  5.98941197e+00\n",
      "  8.12241293e-01  1.23444414e+01  6.71701453e+00  2.36077281e+00\n",
      "  7.03569030e+00  1.47820449e+01 -1.48921065e+00  2.07572826e+00\n",
      "  6.54492962e+00  2.27339157e+01  3.35118543e+00  6.64020569e+00\n",
      " -1.28566609e+00  6.20063015e+00  1.57830775e+00  6.88947986e+00\n",
      "  2.45568160e+00  1.12251649e+01  4.97289275e+00  6.23335222e+00\n",
      "  6.01089410e+00  8.63421468e+00  6.91775281e-01  9.69039060e+00\n",
      "  9.58659454e+00  1.76325710e+00  1.53590506e+01 -1.31581627e+00\n",
      " -8.13554612e-01  4.87722299e+00  9.90649635e+00  4.04940607e+00\n",
      "  8.88091399e+00  1.06364949e+01 -6.36763348e-01  5.94341316e+00\n",
      "  1.13047383e+01  6.38408083e+00  2.97212023e+00  3.64944724e+00\n",
      " -3.42668473e+00  5.63379417e+00  3.21161008e+00  1.11397282e+00]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, true_weights, true_bias)\n",
    "print(\"y_pred:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "   # Your code here\n",
    "    errors = y_true - y_pred\n",
    "    mse = np.mean(errors ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.26357415817355806\n"
     ]
    }
   ],
   "source": [
    "loss = compute_mse(y, predict(X, true_weights, true_bias))\n",
    "print(f\"MSE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n_samples = X.shape[0]\n",
    "    errors = y_pred - y\n",
    "    \n",
    "    gradient_w = (2/n_samples) * (X.T @ errors)\n",
    "    gradient_b = (2/n_samples) * np.sum(errors)\n",
    "    return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01697225  0.02900164  0.03877828] 0.018719217786772317\n"
     ]
    }
   ],
   "source": [
    "gradient_w, gradient_b = compute_gradients(X, y, predict(X, true_weights, true_bias))\n",
    "print(gradient_w, gradient_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1.]), array([0.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.ones((3))\n",
    "b = np.array([0.0])\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), (1,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, w.shape, b.shape # Should be ((1000, 3), (3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-2.85186442,  9.29304439, -1.0522734 ]), np.float64(-10.19197986236907))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, w, b) # Initial predictions\n",
    "print(compute_gradients(X, y, y_pred)) # Should print gradients (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step = 25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        # Compute predictions\n",
    "        y_pred = predict(X, w, b)\n",
    "        \n",
    "\n",
    "        # Calculate gradients\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_pred)\n",
    "\n",
    "        # Gradient descent update\n",
    "        w -= learning_rate * grad_w\n",
    "        b -= learning_rate * grad_b\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            # Calculate loss (MSE)\n",
    "            loss = compute_mse(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.804935\n",
      "Iteration   25 | Loss: 45.329582\n",
      "Iteration   50 | Loss: 44.859193\n",
      "Iteration   75 | Loss: 44.393719\n",
      "Iteration  100 | Loss: 43.933107\n",
      "Iteration  125 | Loss: 43.477306\n",
      "Iteration  150 | Loss: 43.026267\n",
      "Iteration  175 | Loss: 42.579939\n",
      "Iteration  200 | Loss: 42.138273\n",
      "Iteration  225 | Loss: 41.701221\n",
      "Iteration  250 | Loss: 41.268734\n",
      "Iteration  275 | Loss: 40.840765\n",
      "Iteration  300 | Loss: 40.417266\n",
      "Iteration  325 | Loss: 39.998191\n",
      "Iteration  350 | Loss: 39.583493\n",
      "Iteration  375 | Loss: 39.173126\n",
      "Iteration  400 | Loss: 38.767046\n",
      "Iteration  425 | Loss: 38.365208\n",
      "Iteration  450 | Loss: 37.967566\n",
      "Iteration  475 | Loss: 37.574078\n",
      "Iteration  500 | Loss: 37.184700\n",
      "Iteration  525 | Loss: 36.799389\n",
      "Iteration  550 | Loss: 36.418102\n",
      "Iteration  575 | Loss: 36.040797\n",
      "Iteration  600 | Loss: 35.667434\n",
      "Iteration  625 | Loss: 35.297970\n",
      "Iteration  650 | Loss: 34.932364\n",
      "Iteration  675 | Loss: 34.570577\n",
      "Iteration  700 | Loss: 34.212569\n",
      "Iteration  725 | Loss: 33.858300\n",
      "Iteration  750 | Loss: 33.507730\n",
      "Iteration  775 | Loss: 33.160822\n",
      "Iteration  800 | Loss: 32.817537\n",
      "Iteration  825 | Loss: 32.477838\n",
      "Iteration  850 | Loss: 32.141686\n",
      "Iteration  875 | Loss: 31.809044\n",
      "Iteration  900 | Loss: 31.479877\n",
      "Iteration  925 | Loss: 31.154147\n",
      "Iteration  950 | Loss: 30.831818\n",
      "Iteration  975 | Loss: 30.512856\n",
      "Iteration 1000 | Loss: 30.197225\n",
      "Iteration 1025 | Loss: 29.884890\n",
      "Iteration 1050 | Loss: 29.575817\n",
      "Iteration 1075 | Loss: 29.269972\n",
      "Iteration 1100 | Loss: 28.967320\n",
      "Iteration 1125 | Loss: 28.667829\n",
      "Iteration 1150 | Loss: 28.371465\n",
      "Iteration 1175 | Loss: 28.078196\n",
      "Iteration 1200 | Loss: 27.787990\n",
      "Iteration 1225 | Loss: 27.500814\n",
      "Iteration 1250 | Loss: 27.216636\n",
      "Iteration 1275 | Loss: 26.935426\n",
      "Iteration 1300 | Loss: 26.657153\n",
      "Iteration 1325 | Loss: 26.381785\n",
      "Iteration 1350 | Loss: 26.109293\n",
      "Iteration 1375 | Loss: 25.839646\n",
      "Iteration 1400 | Loss: 25.572815\n",
      "Iteration 1425 | Loss: 25.308770\n",
      "Iteration 1450 | Loss: 25.047482\n",
      "Iteration 1475 | Loss: 24.788922\n",
      "Iteration 1500 | Loss: 24.533062\n",
      "Iteration 1525 | Loss: 24.279873\n",
      "Iteration 1550 | Loss: 24.029329\n",
      "Iteration 1575 | Loss: 23.781400\n",
      "Iteration 1600 | Loss: 23.536059\n",
      "Iteration 1625 | Loss: 23.293281\n",
      "Iteration 1650 | Loss: 23.053037\n",
      "Iteration 1675 | Loss: 22.815301\n",
      "Iteration 1700 | Loss: 22.580048\n",
      "Iteration 1725 | Loss: 22.347251\n",
      "Iteration 1750 | Loss: 22.116884\n",
      "Iteration 1775 | Loss: 21.888923\n",
      "Iteration 1800 | Loss: 21.663341\n",
      "Iteration 1825 | Loss: 21.440115\n",
      "Iteration 1850 | Loss: 21.219220\n",
      "Iteration 1875 | Loss: 21.000630\n",
      "Iteration 1900 | Loss: 20.784323\n",
      "Iteration 1925 | Loss: 20.570273\n",
      "Iteration 1950 | Loss: 20.358459\n",
      "Iteration 1975 | Loss: 20.148856\n",
      "Iteration 2000 | Loss: 19.941441\n",
      "Iteration 2025 | Loss: 19.736191\n",
      "Iteration 2050 | Loss: 19.533084\n",
      "Iteration 2075 | Loss: 19.332097\n",
      "Iteration 2100 | Loss: 19.133209\n",
      "Iteration 2125 | Loss: 18.936397\n",
      "Iteration 2150 | Loss: 18.741639\n",
      "Iteration 2175 | Loss: 18.548914\n",
      "Iteration 2200 | Loss: 18.358202\n",
      "Iteration 2225 | Loss: 18.169480\n",
      "Iteration 2250 | Loss: 17.982728\n",
      "Iteration 2275 | Loss: 17.797926\n",
      "Iteration 2300 | Loss: 17.615052\n",
      "Iteration 2325 | Loss: 17.434088\n",
      "Iteration 2350 | Loss: 17.255013\n",
      "Iteration 2375 | Loss: 17.077807\n",
      "Iteration 2400 | Loss: 16.902450\n",
      "Iteration 2425 | Loss: 16.728925\n",
      "Iteration 2450 | Loss: 16.557210\n",
      "Iteration 2475 | Loss: 16.387288\n",
      "Iteration 2500 | Loss: 16.219139\n",
      "Iteration 2525 | Loss: 16.052746\n",
      "Iteration 2550 | Loss: 15.888089\n",
      "Iteration 2575 | Loss: 15.725151\n",
      "Iteration 2600 | Loss: 15.563913\n",
      "Iteration 2625 | Loss: 15.404359\n",
      "Iteration 2650 | Loss: 15.246470\n",
      "Iteration 2675 | Loss: 15.090229\n",
      "Iteration 2700 | Loss: 14.935618\n",
      "Iteration 2725 | Loss: 14.782621\n",
      "Iteration 2750 | Loss: 14.631221\n",
      "Iteration 2775 | Loss: 14.481402\n",
      "Iteration 2800 | Loss: 14.333146\n",
      "Iteration 2825 | Loss: 14.186437\n",
      "Iteration 2850 | Loss: 14.041259\n",
      "Iteration 2875 | Loss: 13.897597\n",
      "Iteration 2900 | Loss: 13.755433\n",
      "Iteration 2925 | Loss: 13.614754\n",
      "Iteration 2950 | Loss: 13.475543\n",
      "Iteration 2975 | Loss: 13.337784\n",
      "Iteration 3000 | Loss: 13.201463\n",
      "Iteration 3025 | Loss: 13.066565\n",
      "Iteration 3050 | Loss: 12.933075\n",
      "Iteration 3075 | Loss: 12.800978\n",
      "Iteration 3100 | Loss: 12.670259\n",
      "Iteration 3125 | Loss: 12.540904\n",
      "Iteration 3150 | Loss: 12.412900\n",
      "Iteration 3175 | Loss: 12.286231\n",
      "Iteration 3200 | Loss: 12.160884\n",
      "Iteration 3225 | Loss: 12.036845\n",
      "Iteration 3250 | Loss: 11.914100\n",
      "Iteration 3275 | Loss: 11.792636\n",
      "Iteration 3300 | Loss: 11.672440\n",
      "Iteration 3325 | Loss: 11.553498\n",
      "Iteration 3350 | Loss: 11.435797\n",
      "Iteration 3375 | Loss: 11.319324\n",
      "Iteration 3400 | Loss: 11.204067\n",
      "Iteration 3425 | Loss: 11.090012\n",
      "Iteration 3450 | Loss: 10.977148\n",
      "Iteration 3475 | Loss: 10.865461\n",
      "Iteration 3500 | Loss: 10.754939\n",
      "Iteration 3525 | Loss: 10.645571\n",
      "Iteration 3550 | Loss: 10.537344\n",
      "Iteration 3575 | Loss: 10.430246\n",
      "Iteration 3600 | Loss: 10.324265\n",
      "Iteration 3625 | Loss: 10.219391\n",
      "Iteration 3650 | Loss: 10.115610\n",
      "Iteration 3675 | Loss: 10.012912\n",
      "Iteration 3700 | Loss: 9.911286\n",
      "Iteration 3725 | Loss: 9.810720\n",
      "Iteration 3750 | Loss: 9.711204\n",
      "Iteration 3775 | Loss: 9.612725\n",
      "Iteration 3800 | Loss: 9.515274\n",
      "Iteration 3825 | Loss: 9.418840\n",
      "Iteration 3850 | Loss: 9.323412\n",
      "Iteration 3875 | Loss: 9.228980\n",
      "Iteration 3900 | Loss: 9.135532\n",
      "Iteration 3925 | Loss: 9.043060\n",
      "Iteration 3950 | Loss: 8.951552\n",
      "Iteration 3975 | Loss: 8.860999\n",
      "Iteration 4000 | Loss: 8.771391\n",
      "Iteration 4025 | Loss: 8.682717\n",
      "Iteration 4050 | Loss: 8.594969\n",
      "Iteration 4075 | Loss: 8.508136\n",
      "Iteration 4100 | Loss: 8.422208\n",
      "Iteration 4125 | Loss: 8.337177\n",
      "Iteration 4150 | Loss: 8.253034\n",
      "Iteration 4175 | Loss: 8.169768\n",
      "Iteration 4200 | Loss: 8.087370\n",
      "Iteration 4225 | Loss: 8.005832\n",
      "Iteration 4250 | Loss: 7.925145\n",
      "Iteration 4275 | Loss: 7.845299\n",
      "Iteration 4300 | Loss: 7.766286\n",
      "Iteration 4325 | Loss: 7.688098\n",
      "Iteration 4350 | Loss: 7.610725\n",
      "Iteration 4375 | Loss: 7.534159\n",
      "Iteration 4400 | Loss: 7.458391\n",
      "Iteration 4425 | Loss: 7.383414\n",
      "Iteration 4450 | Loss: 7.309219\n",
      "Iteration 4475 | Loss: 7.235798\n",
      "Iteration 4500 | Loss: 7.163143\n",
      "Iteration 4525 | Loss: 7.091246\n",
      "Iteration 4550 | Loss: 7.020098\n",
      "Iteration 4575 | Loss: 6.949693\n",
      "Iteration 4600 | Loss: 6.880021\n",
      "Iteration 4625 | Loss: 6.811077\n",
      "Iteration 4650 | Loss: 6.742852\n",
      "Iteration 4675 | Loss: 6.675338\n",
      "Iteration 4700 | Loss: 6.608528\n",
      "Iteration 4725 | Loss: 6.542415\n",
      "Iteration 4750 | Loss: 6.476992\n",
      "Iteration 4775 | Loss: 6.412251\n",
      "Iteration 4800 | Loss: 6.348185\n",
      "Iteration 4825 | Loss: 6.284787\n",
      "Iteration 4850 | Loss: 6.222051\n",
      "Iteration 4875 | Loss: 6.159969\n",
      "Iteration 4900 | Loss: 6.098534\n",
      "Iteration 4925 | Loss: 6.037740\n",
      "Iteration 4950 | Loss: 5.977580\n",
      "Iteration 4975 | Loss: 5.918047\n",
      "Iteration 5000 | Loss: 5.859135\n",
      "Iteration 5025 | Loss: 5.800838\n",
      "Iteration 5050 | Loss: 5.743148\n",
      "Iteration 5075 | Loss: 5.686060\n",
      "Iteration 5100 | Loss: 5.629567\n",
      "Iteration 5125 | Loss: 5.573664\n",
      "Iteration 5150 | Loss: 5.518343\n",
      "Iteration 5175 | Loss: 5.463599\n",
      "Iteration 5200 | Loss: 5.409426\n",
      "Iteration 5225 | Loss: 5.355818\n",
      "Iteration 5250 | Loss: 5.302769\n",
      "Iteration 5275 | Loss: 5.250273\n",
      "Iteration 5300 | Loss: 5.198325\n",
      "Iteration 5325 | Loss: 5.146918\n",
      "Iteration 5350 | Loss: 5.096047\n",
      "Iteration 5375 | Loss: 5.045707\n",
      "Iteration 5400 | Loss: 4.995891\n",
      "Iteration 5425 | Loss: 4.946595\n",
      "Iteration 5450 | Loss: 4.897813\n",
      "Iteration 5475 | Loss: 4.849540\n",
      "Iteration 5500 | Loss: 4.801770\n",
      "Iteration 5525 | Loss: 4.754497\n",
      "Iteration 5550 | Loss: 4.707718\n",
      "Iteration 5575 | Loss: 4.661427\n",
      "Iteration 5600 | Loss: 4.615618\n",
      "Iteration 5625 | Loss: 4.570287\n",
      "Iteration 5650 | Loss: 4.525428\n",
      "Iteration 5675 | Loss: 4.481037\n",
      "Iteration 5700 | Loss: 4.437109\n",
      "Iteration 5725 | Loss: 4.393638\n",
      "Iteration 5750 | Loss: 4.350621\n",
      "Iteration 5775 | Loss: 4.308053\n",
      "Iteration 5800 | Loss: 4.265928\n",
      "Iteration 5825 | Loss: 4.224242\n",
      "Iteration 5850 | Loss: 4.182991\n",
      "Iteration 5875 | Loss: 4.142170\n",
      "Iteration 5900 | Loss: 4.101775\n",
      "Iteration 5925 | Loss: 4.061800\n",
      "Iteration 5950 | Loss: 4.022243\n",
      "Iteration 5975 | Loss: 3.983097\n",
      "Iteration 6000 | Loss: 3.944360\n",
      "Iteration 6025 | Loss: 3.906027\n",
      "Iteration 6050 | Loss: 3.868093\n",
      "Iteration 6075 | Loss: 3.830554\n",
      "Iteration 6100 | Loss: 3.793407\n",
      "Iteration 6125 | Loss: 3.756647\n",
      "Iteration 6150 | Loss: 3.720271\n",
      "Iteration 6175 | Loss: 3.684273\n",
      "Iteration 6200 | Loss: 3.648651\n",
      "Iteration 6225 | Loss: 3.613400\n",
      "Iteration 6250 | Loss: 3.578516\n",
      "Iteration 6275 | Loss: 3.543996\n",
      "Iteration 6300 | Loss: 3.509836\n",
      "Iteration 6325 | Loss: 3.476031\n",
      "Iteration 6350 | Loss: 3.442580\n",
      "Iteration 6375 | Loss: 3.409476\n",
      "Iteration 6400 | Loss: 3.376718\n",
      "Iteration 6425 | Loss: 3.344301\n",
      "Iteration 6450 | Loss: 3.312222\n",
      "Iteration 6475 | Loss: 3.280478\n",
      "Iteration 6500 | Loss: 3.249064\n",
      "Iteration 6525 | Loss: 3.217978\n",
      "Iteration 6550 | Loss: 3.187215\n",
      "Iteration 6575 | Loss: 3.156774\n",
      "Iteration 6600 | Loss: 3.126649\n",
      "Iteration 6625 | Loss: 3.096838\n",
      "Iteration 6650 | Loss: 3.067339\n",
      "Iteration 6675 | Loss: 3.038146\n",
      "Iteration 6700 | Loss: 3.009258\n",
      "Iteration 6725 | Loss: 2.980671\n",
      "Iteration 6750 | Loss: 2.952381\n",
      "Iteration 6775 | Loss: 2.924387\n",
      "Iteration 6800 | Loss: 2.896684\n",
      "Iteration 6825 | Loss: 2.869270\n",
      "Iteration 6850 | Loss: 2.842141\n",
      "Iteration 6875 | Loss: 2.815296\n",
      "Iteration 6900 | Loss: 2.788730\n",
      "Iteration 6925 | Loss: 2.762441\n",
      "Iteration 6950 | Loss: 2.736425\n",
      "Iteration 6975 | Loss: 2.710681\n",
      "Iteration 7000 | Loss: 2.685205\n",
      "Iteration 7025 | Loss: 2.659995\n",
      "Iteration 7050 | Loss: 2.635047\n",
      "Iteration 7075 | Loss: 2.610359\n",
      "Iteration 7100 | Loss: 2.585928\n",
      "Iteration 7125 | Loss: 2.561752\n",
      "Iteration 7150 | Loss: 2.537828\n",
      "Iteration 7175 | Loss: 2.514153\n",
      "Iteration 7200 | Loss: 2.490725\n",
      "Iteration 7225 | Loss: 2.467541\n",
      "Iteration 7250 | Loss: 2.444598\n",
      "Iteration 7275 | Loss: 2.421895\n",
      "Iteration 7300 | Loss: 2.399428\n",
      "Iteration 7325 | Loss: 2.377195\n",
      "Iteration 7350 | Loss: 2.355194\n",
      "Iteration 7375 | Loss: 2.333421\n",
      "Iteration 7400 | Loss: 2.311876\n",
      "Iteration 7425 | Loss: 2.290555\n",
      "Iteration 7450 | Loss: 2.269457\n",
      "Iteration 7475 | Loss: 2.248578\n",
      "Iteration 7500 | Loss: 2.227916\n",
      "Iteration 7525 | Loss: 2.207470\n",
      "Iteration 7550 | Loss: 2.187237\n",
      "Iteration 7575 | Loss: 2.167215\n",
      "Iteration 7600 | Loss: 2.147401\n",
      "Iteration 7625 | Loss: 2.127793\n",
      "Iteration 7650 | Loss: 2.108390\n",
      "Iteration 7675 | Loss: 2.089189\n",
      "Iteration 7700 | Loss: 2.070188\n",
      "Iteration 7725 | Loss: 2.051385\n",
      "Iteration 7750 | Loss: 2.032778\n",
      "Iteration 7775 | Loss: 2.014364\n",
      "Iteration 7800 | Loss: 1.996143\n",
      "Iteration 7825 | Loss: 1.978111\n",
      "Iteration 7850 | Loss: 1.960267\n",
      "Iteration 7875 | Loss: 1.942609\n",
      "Iteration 7900 | Loss: 1.925134\n",
      "Iteration 7925 | Loss: 1.907842\n",
      "Iteration 7950 | Loss: 1.890730\n",
      "Iteration 7975 | Loss: 1.873796\n",
      "Iteration 8000 | Loss: 1.857039\n",
      "Iteration 8025 | Loss: 1.840456\n",
      "Iteration 8050 | Loss: 1.824045\n",
      "Iteration 8075 | Loss: 1.807806\n",
      "Iteration 8100 | Loss: 1.791736\n",
      "Iteration 8125 | Loss: 1.775833\n",
      "Iteration 8150 | Loss: 1.760096\n",
      "Iteration 8175 | Loss: 1.744522\n",
      "Iteration 8200 | Loss: 1.729111\n",
      "Iteration 8225 | Loss: 1.713860\n",
      "Iteration 8250 | Loss: 1.698769\n",
      "Iteration 8275 | Loss: 1.683834\n",
      "Iteration 8300 | Loss: 1.669055\n",
      "Iteration 8325 | Loss: 1.654429\n",
      "Iteration 8350 | Loss: 1.639956\n",
      "Iteration 8375 | Loss: 1.625634\n",
      "Iteration 8400 | Loss: 1.611461\n",
      "Iteration 8425 | Loss: 1.597436\n",
      "Iteration 8450 | Loss: 1.583556\n",
      "Iteration 8475 | Loss: 1.569821\n",
      "Iteration 8500 | Loss: 1.556229\n",
      "Iteration 8525 | Loss: 1.542779\n",
      "Iteration 8550 | Loss: 1.529469\n",
      "Iteration 8575 | Loss: 1.516297\n",
      "Iteration 8600 | Loss: 1.503262\n",
      "Iteration 8625 | Loss: 1.490363\n",
      "Iteration 8650 | Loss: 1.477599\n",
      "Iteration 8675 | Loss: 1.464967\n",
      "Iteration 8700 | Loss: 1.452467\n",
      "Iteration 8725 | Loss: 1.440097\n",
      "Iteration 8750 | Loss: 1.427856\n",
      "Iteration 8775 | Loss: 1.415742\n",
      "Iteration 8800 | Loss: 1.403754\n",
      "Iteration 8825 | Loss: 1.391891\n",
      "Iteration 8850 | Loss: 1.380152\n",
      "Iteration 8875 | Loss: 1.368535\n",
      "Iteration 8900 | Loss: 1.357039\n",
      "Iteration 8925 | Loss: 1.345662\n",
      "Iteration 8950 | Loss: 1.334404\n",
      "Iteration 8975 | Loss: 1.323264\n",
      "Iteration 9000 | Loss: 1.312239\n",
      "Iteration 9025 | Loss: 1.301329\n",
      "Iteration 9050 | Loss: 1.290532\n",
      "Iteration 9075 | Loss: 1.279848\n",
      "Iteration 9100 | Loss: 1.269275\n",
      "Iteration 9125 | Loss: 1.258812\n",
      "Iteration 9150 | Loss: 1.248458\n",
      "Iteration 9175 | Loss: 1.238212\n",
      "Iteration 9200 | Loss: 1.228073\n",
      "Iteration 9225 | Loss: 1.218039\n",
      "Iteration 9250 | Loss: 1.208109\n",
      "Iteration 9275 | Loss: 1.198283\n",
      "Iteration 9300 | Loss: 1.188559\n",
      "Iteration 9325 | Loss: 1.178937\n",
      "Iteration 9350 | Loss: 1.169414\n",
      "Iteration 9375 | Loss: 1.159991\n",
      "Iteration 9400 | Loss: 1.150665\n",
      "Iteration 9425 | Loss: 1.141437\n",
      "Iteration 9450 | Loss: 1.132305\n",
      "Iteration 9475 | Loss: 1.123268\n",
      "Iteration 9500 | Loss: 1.114325\n",
      "Iteration 9525 | Loss: 1.105475\n",
      "Iteration 9550 | Loss: 1.096717\n",
      "Iteration 9575 | Loss: 1.088050\n",
      "Iteration 9600 | Loss: 1.079473\n",
      "Iteration 9625 | Loss: 1.070986\n",
      "Iteration 9650 | Loss: 1.062587\n",
      "Iteration 9675 | Loss: 1.054276\n",
      "Iteration 9700 | Loss: 1.046051\n",
      "Iteration 9725 | Loss: 1.037911\n",
      "Iteration 9750 | Loss: 1.029856\n",
      "Iteration 9775 | Loss: 1.021885\n",
      "Iteration 9800 | Loss: 1.013997\n",
      "Iteration 9825 | Loss: 1.006192\n",
      "Iteration 9850 | Loss: 0.998467\n",
      "Iteration 9875 | Loss: 0.990823\n",
      "Iteration 9900 | Loss: 0.983258\n",
      "Iteration 9925 | Loss: 0.975772\n",
      "Iteration 9950 | Loss: 0.968364\n",
      "Iteration 9975 | Loss: 0.961033\n",
      "Iteration 10000 | Loss: 0.953778\n",
      "Iteration 10025 | Loss: 0.946599\n",
      "Iteration 10050 | Loss: 0.939494\n",
      "Iteration 10075 | Loss: 0.932463\n",
      "Iteration 10100 | Loss: 0.925506\n",
      "Iteration 10125 | Loss: 0.918621\n",
      "Iteration 10150 | Loss: 0.911807\n",
      "Iteration 10175 | Loss: 0.905065\n",
      "Iteration 10200 | Loss: 0.898392\n",
      "Iteration 10225 | Loss: 0.891789\n",
      "Iteration 10250 | Loss: 0.885255\n",
      "Iteration 10275 | Loss: 0.878788\n",
      "Iteration 10300 | Loss: 0.872389\n",
      "Iteration 10325 | Loss: 0.866057\n",
      "Iteration 10350 | Loss: 0.859790\n",
      "Iteration 10375 | Loss: 0.853589\n",
      "Iteration 10400 | Loss: 0.847452\n",
      "Iteration 10425 | Loss: 0.841379\n",
      "Iteration 10450 | Loss: 0.835369\n",
      "Iteration 10475 | Loss: 0.829422\n",
      "Iteration 10500 | Loss: 0.823536\n",
      "Iteration 10525 | Loss: 0.817712\n",
      "Iteration 10550 | Loss: 0.811948\n",
      "Iteration 10575 | Loss: 0.806244\n",
      "Iteration 10600 | Loss: 0.800600\n",
      "Iteration 10625 | Loss: 0.795014\n",
      "Iteration 10650 | Loss: 0.789487\n",
      "Iteration 10675 | Loss: 0.784017\n",
      "Iteration 10700 | Loss: 0.778604\n",
      "Iteration 10725 | Loss: 0.773247\n",
      "Iteration 10750 | Loss: 0.767945\n",
      "Iteration 10775 | Loss: 0.762699\n",
      "Iteration 10800 | Loss: 0.757508\n",
      "Iteration 10825 | Loss: 0.752371\n",
      "Iteration 10850 | Loss: 0.747286\n",
      "Iteration 10875 | Loss: 0.742255\n",
      "Iteration 10900 | Loss: 0.737276\n",
      "Iteration 10925 | Loss: 0.732349\n",
      "Iteration 10950 | Loss: 0.727474\n",
      "Iteration 10975 | Loss: 0.722649\n",
      "Iteration 11000 | Loss: 0.717874\n",
      "Iteration 11025 | Loss: 0.713148\n",
      "Iteration 11050 | Loss: 0.708472\n",
      "Iteration 11075 | Loss: 0.703845\n",
      "Iteration 11100 | Loss: 0.699265\n",
      "Iteration 11125 | Loss: 0.694734\n",
      "Iteration 11150 | Loss: 0.690249\n",
      "Iteration 11175 | Loss: 0.685811\n",
      "Iteration 11200 | Loss: 0.681419\n",
      "Iteration 11225 | Loss: 0.677073\n",
      "Iteration 11250 | Loss: 0.672772\n",
      "Iteration 11275 | Loss: 0.668516\n",
      "Iteration 11300 | Loss: 0.664304\n",
      "Iteration 11325 | Loss: 0.660136\n",
      "Iteration 11350 | Loss: 0.656011\n",
      "Iteration 11375 | Loss: 0.651929\n",
      "Iteration 11400 | Loss: 0.647889\n",
      "Iteration 11425 | Loss: 0.643892\n",
      "Iteration 11450 | Loss: 0.639936\n",
      "Iteration 11475 | Loss: 0.636021\n",
      "Iteration 11500 | Loss: 0.632147\n",
      "Iteration 11525 | Loss: 0.628313\n",
      "Iteration 11550 | Loss: 0.624519\n",
      "Iteration 11575 | Loss: 0.620764\n",
      "Iteration 11600 | Loss: 0.617049\n",
      "Iteration 11625 | Loss: 0.613372\n",
      "Iteration 11650 | Loss: 0.609733\n",
      "Iteration 11675 | Loss: 0.606133\n",
      "Iteration 11700 | Loss: 0.602569\n",
      "Iteration 11725 | Loss: 0.599043\n",
      "Iteration 11750 | Loss: 0.595553\n",
      "Iteration 11775 | Loss: 0.592100\n",
      "Iteration 11800 | Loss: 0.588682\n",
      "Iteration 11825 | Loss: 0.585300\n",
      "Iteration 11850 | Loss: 0.581953\n",
      "Iteration 11875 | Loss: 0.578641\n",
      "Iteration 11900 | Loss: 0.575364\n",
      "Iteration 11925 | Loss: 0.572120\n",
      "Iteration 11950 | Loss: 0.568910\n",
      "Iteration 11975 | Loss: 0.565734\n",
      "Iteration 12000 | Loss: 0.562591\n",
      "Iteration 12025 | Loss: 0.559480\n",
      "Iteration 12050 | Loss: 0.556401\n",
      "Iteration 12075 | Loss: 0.553355\n",
      "Iteration 12100 | Loss: 0.550340\n",
      "Iteration 12125 | Loss: 0.547357\n",
      "Iteration 12150 | Loss: 0.544404\n",
      "Iteration 12175 | Loss: 0.541483\n",
      "Iteration 12200 | Loss: 0.538591\n",
      "Iteration 12225 | Loss: 0.535730\n",
      "Iteration 12250 | Loss: 0.532898\n",
      "Iteration 12275 | Loss: 0.530096\n",
      "Iteration 12300 | Loss: 0.527323\n",
      "Iteration 12325 | Loss: 0.524579\n",
      "Iteration 12350 | Loss: 0.521863\n",
      "Iteration 12375 | Loss: 0.519176\n",
      "Iteration 12400 | Loss: 0.516516\n",
      "Iteration 12425 | Loss: 0.513884\n",
      "Iteration 12450 | Loss: 0.511280\n",
      "Iteration 12475 | Loss: 0.508702\n",
      "Iteration 12500 | Loss: 0.506151\n",
      "Iteration 12525 | Loss: 0.503627\n",
      "Iteration 12550 | Loss: 0.501129\n",
      "Iteration 12575 | Loss: 0.498657\n",
      "Iteration 12600 | Loss: 0.496211\n",
      "Iteration 12625 | Loss: 0.493790\n",
      "Iteration 12650 | Loss: 0.491394\n",
      "Iteration 12675 | Loss: 0.489023\n",
      "Iteration 12700 | Loss: 0.486677\n",
      "Iteration 12725 | Loss: 0.484355\n",
      "Iteration 12750 | Loss: 0.482057\n",
      "Iteration 12775 | Loss: 0.479784\n",
      "Iteration 12800 | Loss: 0.477533\n",
      "Iteration 12825 | Loss: 0.475307\n",
      "Iteration 12850 | Loss: 0.473103\n",
      "Iteration 12875 | Loss: 0.470922\n",
      "Iteration 12900 | Loss: 0.468764\n",
      "Iteration 12925 | Loss: 0.466628\n",
      "Iteration 12950 | Loss: 0.464514\n",
      "Iteration 12975 | Loss: 0.462423\n",
      "Iteration 13000 | Loss: 0.460353\n",
      "Iteration 13025 | Loss: 0.458305\n",
      "Iteration 13050 | Loss: 0.456277\n",
      "Iteration 13075 | Loss: 0.454271\n",
      "Iteration 13100 | Loss: 0.452286\n",
      "Iteration 13125 | Loss: 0.450322\n",
      "Iteration 13150 | Loss: 0.448377\n",
      "Iteration 13175 | Loss: 0.446453\n",
      "Iteration 13200 | Loss: 0.444550\n",
      "Iteration 13225 | Loss: 0.442665\n",
      "Iteration 13250 | Loss: 0.440801\n",
      "Iteration 13275 | Loss: 0.438955\n",
      "Iteration 13300 | Loss: 0.437129\n",
      "Iteration 13325 | Loss: 0.435322\n",
      "Iteration 13350 | Loss: 0.433534\n",
      "Iteration 13375 | Loss: 0.431764\n",
      "Iteration 13400 | Loss: 0.430012\n",
      "Iteration 13425 | Loss: 0.428279\n",
      "Iteration 13450 | Loss: 0.426564\n",
      "Iteration 13475 | Loss: 0.424867\n",
      "Iteration 13500 | Loss: 0.423187\n",
      "Iteration 13525 | Loss: 0.421524\n",
      "Iteration 13550 | Loss: 0.419879\n",
      "Iteration 13575 | Loss: 0.418251\n",
      "Iteration 13600 | Loss: 0.416640\n",
      "Iteration 13625 | Loss: 0.415046\n",
      "Iteration 13650 | Loss: 0.413468\n",
      "Iteration 13675 | Loss: 0.411907\n",
      "Iteration 13700 | Loss: 0.410362\n",
      "Iteration 13725 | Loss: 0.408832\n",
      "Iteration 13750 | Loss: 0.407319\n",
      "Iteration 13775 | Loss: 0.405821\n",
      "Iteration 13800 | Loss: 0.404339\n",
      "Iteration 13825 | Loss: 0.402873\n",
      "Iteration 13850 | Loss: 0.401421\n",
      "Iteration 13875 | Loss: 0.399985\n",
      "Iteration 13900 | Loss: 0.398564\n",
      "Iteration 13925 | Loss: 0.397157\n",
      "Iteration 13950 | Loss: 0.395765\n",
      "Iteration 13975 | Loss: 0.394387\n",
      "Iteration 14000 | Loss: 0.393024\n",
      "Iteration 14025 | Loss: 0.391675\n",
      "Iteration 14050 | Loss: 0.390340\n",
      "Iteration 14075 | Loss: 0.389018\n",
      "Iteration 14100 | Loss: 0.387711\n",
      "Iteration 14125 | Loss: 0.386417\n",
      "Iteration 14150 | Loss: 0.385136\n",
      "Iteration 14175 | Loss: 0.383869\n",
      "Iteration 14200 | Loss: 0.382615\n",
      "Iteration 14225 | Loss: 0.381374\n",
      "Iteration 14250 | Loss: 0.380146\n",
      "Iteration 14275 | Loss: 0.378930\n",
      "Iteration 14300 | Loss: 0.377727\n",
      "Iteration 14325 | Loss: 0.376537\n",
      "Iteration 14350 | Loss: 0.375359\n",
      "Iteration 14375 | Loss: 0.374193\n",
      "Iteration 14400 | Loss: 0.373040\n",
      "Iteration 14425 | Loss: 0.371898\n",
      "Iteration 14450 | Loss: 0.370768\n",
      "Iteration 14475 | Loss: 0.369650\n",
      "Iteration 14500 | Loss: 0.368543\n",
      "Iteration 14525 | Loss: 0.367448\n",
      "Iteration 14550 | Loss: 0.366365\n",
      "Iteration 14575 | Loss: 0.365292\n",
      "Iteration 14600 | Loss: 0.364231\n",
      "Iteration 14625 | Loss: 0.363181\n",
      "Iteration 14650 | Loss: 0.362141\n",
      "Iteration 14675 | Loss: 0.361113\n",
      "Iteration 14700 | Loss: 0.360095\n",
      "Iteration 14725 | Loss: 0.359088\n",
      "Iteration 14750 | Loss: 0.358091\n",
      "Iteration 14775 | Loss: 0.357104\n",
      "Iteration 14800 | Loss: 0.356128\n",
      "Iteration 14825 | Loss: 0.355162\n",
      "Iteration 14850 | Loss: 0.354205\n",
      "Iteration 14875 | Loss: 0.353259\n",
      "Iteration 14900 | Loss: 0.352323\n",
      "Iteration 14925 | Loss: 0.351396\n",
      "Iteration 14950 | Loss: 0.350479\n",
      "Iteration 14975 | Loss: 0.349571\n",
      "Iteration 15000 | Loss: 0.348673\n",
      "Iteration 15025 | Loss: 0.347784\n",
      "Iteration 15050 | Loss: 0.346905\n",
      "Iteration 15075 | Loss: 0.346034\n",
      "Iteration 15100 | Loss: 0.345173\n",
      "Iteration 15125 | Loss: 0.344320\n",
      "Iteration 15150 | Loss: 0.343477\n",
      "Iteration 15175 | Loss: 0.342642\n",
      "Iteration 15200 | Loss: 0.341815\n",
      "Iteration 15225 | Loss: 0.340998\n",
      "Iteration 15250 | Loss: 0.340189\n",
      "Iteration 15275 | Loss: 0.339388\n",
      "Iteration 15300 | Loss: 0.338595\n",
      "Iteration 15325 | Loss: 0.337811\n",
      "Iteration 15350 | Loss: 0.337035\n",
      "Iteration 15375 | Loss: 0.336267\n",
      "Iteration 15400 | Loss: 0.335507\n",
      "Iteration 15425 | Loss: 0.334754\n",
      "Iteration 15450 | Loss: 0.334010\n",
      "Iteration 15475 | Loss: 0.333273\n",
      "Iteration 15500 | Loss: 0.332544\n",
      "Iteration 15525 | Loss: 0.331823\n",
      "Iteration 15550 | Loss: 0.331109\n",
      "Iteration 15575 | Loss: 0.330402\n",
      "Iteration 15600 | Loss: 0.329703\n",
      "Iteration 15625 | Loss: 0.329011\n",
      "Iteration 15650 | Loss: 0.328326\n",
      "Iteration 15675 | Loss: 0.327648\n",
      "Iteration 15700 | Loss: 0.326977\n",
      "Iteration 15725 | Loss: 0.326313\n",
      "Iteration 15750 | Loss: 0.325657\n",
      "Iteration 15775 | Loss: 0.325007\n",
      "Iteration 15800 | Loss: 0.324363\n",
      "Iteration 15825 | Loss: 0.323727\n",
      "Iteration 15850 | Loss: 0.323096\n",
      "Iteration 15875 | Loss: 0.322473\n",
      "Iteration 15900 | Loss: 0.321856\n",
      "Iteration 15925 | Loss: 0.321245\n",
      "Iteration 15950 | Loss: 0.320641\n",
      "Iteration 15975 | Loss: 0.320043\n",
      "Iteration 16000 | Loss: 0.319451\n",
      "Iteration 16025 | Loss: 0.318865\n",
      "Iteration 16050 | Loss: 0.318285\n",
      "Iteration 16075 | Loss: 0.317712\n",
      "Iteration 16100 | Loss: 0.317144\n",
      "Iteration 16125 | Loss: 0.316582\n",
      "Iteration 16150 | Loss: 0.316026\n",
      "Iteration 16175 | Loss: 0.315476\n",
      "Iteration 16200 | Loss: 0.314932\n",
      "Iteration 16225 | Loss: 0.314393\n",
      "Iteration 16250 | Loss: 0.313859\n",
      "Iteration 16275 | Loss: 0.313332\n",
      "Iteration 16300 | Loss: 0.312809\n",
      "Iteration 16325 | Loss: 0.312292\n",
      "Iteration 16350 | Loss: 0.311781\n",
      "Iteration 16375 | Loss: 0.311275\n",
      "Iteration 16400 | Loss: 0.310774\n",
      "Iteration 16425 | Loss: 0.310278\n",
      "Iteration 16450 | Loss: 0.309787\n",
      "Iteration 16475 | Loss: 0.309302\n",
      "Iteration 16500 | Loss: 0.308821\n",
      "Iteration 16525 | Loss: 0.308346\n",
      "Iteration 16550 | Loss: 0.307875\n",
      "Iteration 16575 | Loss: 0.307409\n",
      "Iteration 16600 | Loss: 0.306948\n",
      "Iteration 16625 | Loss: 0.306492\n",
      "Iteration 16650 | Loss: 0.306041\n",
      "Iteration 16675 | Loss: 0.305594\n",
      "Iteration 16700 | Loss: 0.305152\n",
      "Iteration 16725 | Loss: 0.304715\n",
      "Iteration 16750 | Loss: 0.304282\n",
      "Iteration 16775 | Loss: 0.303853\n",
      "Iteration 16800 | Loss: 0.303429\n",
      "Iteration 16825 | Loss: 0.303009\n",
      "Iteration 16850 | Loss: 0.302594\n",
      "Iteration 16875 | Loss: 0.302183\n",
      "Iteration 16900 | Loss: 0.301776\n",
      "Iteration 16925 | Loss: 0.301374\n",
      "Iteration 16950 | Loss: 0.300976\n",
      "Iteration 16975 | Loss: 0.300581\n",
      "Iteration 17000 | Loss: 0.300191\n",
      "Iteration 17025 | Loss: 0.299805\n",
      "Iteration 17050 | Loss: 0.299423\n",
      "Iteration 17075 | Loss: 0.299045\n",
      "Iteration 17100 | Loss: 0.298671\n",
      "Iteration 17125 | Loss: 0.298300\n",
      "Iteration 17150 | Loss: 0.297934\n",
      "Iteration 17175 | Loss: 0.297571\n",
      "Iteration 17200 | Loss: 0.297212\n",
      "Iteration 17225 | Loss: 0.296857\n",
      "Iteration 17250 | Loss: 0.296505\n",
      "Iteration 17275 | Loss: 0.296157\n",
      "Iteration 17300 | Loss: 0.295813\n",
      "Iteration 17325 | Loss: 0.295472\n",
      "Iteration 17350 | Loss: 0.295135\n",
      "Iteration 17375 | Loss: 0.294801\n",
      "Iteration 17400 | Loss: 0.294471\n",
      "Iteration 17425 | Loss: 0.294144\n",
      "Iteration 17450 | Loss: 0.293821\n",
      "Iteration 17475 | Loss: 0.293501\n",
      "Iteration 17500 | Loss: 0.293184\n",
      "Iteration 17525 | Loss: 0.292870\n",
      "Iteration 17550 | Loss: 0.292560\n",
      "Iteration 17575 | Loss: 0.292253\n",
      "Iteration 17600 | Loss: 0.291949\n",
      "Iteration 17625 | Loss: 0.291648\n",
      "Iteration 17650 | Loss: 0.291351\n",
      "Iteration 17675 | Loss: 0.291056\n",
      "Iteration 17700 | Loss: 0.290765\n",
      "Iteration 17725 | Loss: 0.290476\n",
      "Iteration 17750 | Loss: 0.290191\n",
      "Iteration 17775 | Loss: 0.289908\n",
      "Iteration 17800 | Loss: 0.289629\n",
      "Iteration 17825 | Loss: 0.289352\n",
      "Iteration 17850 | Loss: 0.289078\n",
      "Iteration 17875 | Loss: 0.288807\n",
      "Iteration 17900 | Loss: 0.288539\n",
      "Iteration 17925 | Loss: 0.288274\n",
      "Iteration 17950 | Loss: 0.288011\n",
      "Iteration 17975 | Loss: 0.287751\n",
      "Iteration 18000 | Loss: 0.287494\n",
      "Iteration 18025 | Loss: 0.287239\n",
      "Iteration 18050 | Loss: 0.286987\n",
      "Iteration 18075 | Loss: 0.286738\n",
      "Iteration 18100 | Loss: 0.286491\n",
      "Iteration 18125 | Loss: 0.286247\n",
      "Iteration 18150 | Loss: 0.286005\n",
      "Iteration 18175 | Loss: 0.285766\n",
      "Iteration 18200 | Loss: 0.285529\n",
      "Iteration 18225 | Loss: 0.285295\n",
      "Iteration 18250 | Loss: 0.285063\n",
      "Iteration 18275 | Loss: 0.284834\n",
      "Iteration 18300 | Loss: 0.284607\n",
      "Iteration 18325 | Loss: 0.284382\n",
      "Iteration 18350 | Loss: 0.284160\n",
      "Iteration 18375 | Loss: 0.283940\n",
      "Iteration 18400 | Loss: 0.283722\n",
      "Iteration 18425 | Loss: 0.283506\n",
      "Iteration 18450 | Loss: 0.283293\n",
      "Iteration 18475 | Loss: 0.283082\n",
      "Iteration 18500 | Loss: 0.282873\n",
      "Iteration 18525 | Loss: 0.282666\n",
      "Iteration 18550 | Loss: 0.282462\n",
      "Iteration 18575 | Loss: 0.282259\n",
      "Iteration 18600 | Loss: 0.282059\n",
      "Iteration 18625 | Loss: 0.281861\n",
      "Iteration 18650 | Loss: 0.281664\n",
      "Iteration 18675 | Loss: 0.281470\n",
      "Iteration 18700 | Loss: 0.281278\n",
      "Iteration 18725 | Loss: 0.281088\n",
      "Iteration 18750 | Loss: 0.280899\n",
      "Iteration 18775 | Loss: 0.280713\n",
      "Iteration 18800 | Loss: 0.280529\n",
      "Iteration 18825 | Loss: 0.280346\n",
      "Iteration 18850 | Loss: 0.280165\n",
      "Iteration 18875 | Loss: 0.279987\n",
      "Iteration 18900 | Loss: 0.279810\n",
      "Iteration 18925 | Loss: 0.279635\n",
      "Iteration 18950 | Loss: 0.279462\n",
      "Iteration 18975 | Loss: 0.279290\n",
      "Iteration 19000 | Loss: 0.279120\n",
      "Iteration 19025 | Loss: 0.278953\n",
      "Iteration 19050 | Loss: 0.278786\n",
      "Iteration 19075 | Loss: 0.278622\n",
      "Iteration 19100 | Loss: 0.278459\n",
      "Iteration 19125 | Loss: 0.278298\n",
      "Iteration 19150 | Loss: 0.278139\n",
      "Iteration 19175 | Loss: 0.277981\n",
      "Iteration 19200 | Loss: 0.277825\n",
      "Iteration 19225 | Loss: 0.277670\n",
      "Iteration 19250 | Loss: 0.277517\n",
      "Iteration 19275 | Loss: 0.277366\n",
      "Iteration 19300 | Loss: 0.277216\n",
      "Iteration 19325 | Loss: 0.277068\n",
      "Iteration 19350 | Loss: 0.276921\n",
      "Iteration 19375 | Loss: 0.276776\n",
      "Iteration 19400 | Loss: 0.276632\n",
      "Iteration 19425 | Loss: 0.276490\n",
      "Iteration 19450 | Loss: 0.276350\n",
      "Iteration 19475 | Loss: 0.276210\n",
      "Iteration 19500 | Loss: 0.276072\n",
      "Iteration 19525 | Loss: 0.275936\n",
      "Iteration 19550 | Loss: 0.275801\n",
      "Iteration 19575 | Loss: 0.275668\n",
      "Iteration 19600 | Loss: 0.275535\n",
      "Iteration 19625 | Loss: 0.275404\n",
      "Iteration 19650 | Loss: 0.275275\n",
      "Iteration 19675 | Loss: 0.275147\n",
      "Iteration 19700 | Loss: 0.275020\n",
      "Iteration 19725 | Loss: 0.274895\n",
      "Iteration 19750 | Loss: 0.274770\n",
      "Iteration 19775 | Loss: 0.274647\n",
      "Iteration 19800 | Loss: 0.274526\n",
      "Iteration 19825 | Loss: 0.274405\n",
      "Iteration 19850 | Loss: 0.274286\n",
      "Iteration 19875 | Loss: 0.274168\n",
      "Iteration 19900 | Loss: 0.274052\n",
      "Iteration 19925 | Loss: 0.273936\n",
      "Iteration 19950 | Loss: 0.273822\n",
      "Iteration 19975 | Loss: 0.273709\n",
      "Iteration 20000 | Loss: 0.273597\n",
      "Iteration 20025 | Loss: 0.273486\n",
      "Iteration 20050 | Loss: 0.273376\n",
      "Iteration 20075 | Loss: 0.273268\n",
      "Iteration 20100 | Loss: 0.273160\n",
      "Iteration 20125 | Loss: 0.273054\n",
      "Iteration 20150 | Loss: 0.272949\n",
      "Iteration 20175 | Loss: 0.272845\n",
      "Iteration 20200 | Loss: 0.272742\n",
      "Iteration 20225 | Loss: 0.272640\n",
      "Iteration 20250 | Loss: 0.272539\n",
      "Iteration 20275 | Loss: 0.272439\n",
      "Iteration 20300 | Loss: 0.272340\n",
      "Iteration 20325 | Loss: 0.272242\n",
      "Iteration 20350 | Loss: 0.272146\n",
      "Iteration 20375 | Loss: 0.272050\n",
      "Iteration 20400 | Loss: 0.271955\n",
      "Iteration 20425 | Loss: 0.271861\n",
      "Iteration 20450 | Loss: 0.271768\n",
      "Iteration 20475 | Loss: 0.271676\n",
      "Iteration 20500 | Loss: 0.271586\n",
      "Iteration 20525 | Loss: 0.271496\n",
      "Iteration 20550 | Loss: 0.271406\n",
      "Iteration 20575 | Loss: 0.271318\n",
      "Iteration 20600 | Loss: 0.271231\n",
      "Iteration 20625 | Loss: 0.271145\n",
      "Iteration 20650 | Loss: 0.271059\n",
      "Iteration 20675 | Loss: 0.270975\n",
      "Iteration 20700 | Loss: 0.270891\n",
      "Iteration 20725 | Loss: 0.270808\n",
      "Iteration 20750 | Loss: 0.270726\n",
      "Iteration 20775 | Loss: 0.270645\n",
      "Iteration 20800 | Loss: 0.270565\n",
      "Iteration 20825 | Loss: 0.270485\n",
      "Iteration 20850 | Loss: 0.270407\n",
      "Iteration 20875 | Loss: 0.270329\n",
      "Iteration 20900 | Loss: 0.270252\n",
      "Iteration 20925 | Loss: 0.270176\n",
      "Iteration 20950 | Loss: 0.270100\n",
      "Iteration 20975 | Loss: 0.270025\n",
      "Iteration 21000 | Loss: 0.269952\n",
      "Iteration 21025 | Loss: 0.269878\n",
      "Iteration 21050 | Loss: 0.269806\n",
      "Iteration 21075 | Loss: 0.269734\n",
      "Iteration 21100 | Loss: 0.269664\n",
      "Iteration 21125 | Loss: 0.269593\n",
      "Iteration 21150 | Loss: 0.269524\n",
      "Iteration 21175 | Loss: 0.269455\n",
      "Iteration 21200 | Loss: 0.269387\n",
      "Iteration 21225 | Loss: 0.269320\n",
      "Iteration 21250 | Loss: 0.269253\n",
      "Iteration 21275 | Loss: 0.269187\n",
      "Iteration 21300 | Loss: 0.269122\n",
      "Iteration 21325 | Loss: 0.269058\n",
      "Iteration 21350 | Loss: 0.268994\n",
      "Iteration 21375 | Loss: 0.268931\n",
      "Iteration 21400 | Loss: 0.268868\n",
      "Iteration 21425 | Loss: 0.268806\n",
      "Iteration 21450 | Loss: 0.268745\n",
      "Iteration 21475 | Loss: 0.268684\n",
      "Iteration 21500 | Loss: 0.268624\n",
      "Iteration 21525 | Loss: 0.268565\n",
      "Iteration 21550 | Loss: 0.268506\n",
      "Iteration 21575 | Loss: 0.268448\n",
      "Iteration 21600 | Loss: 0.268390\n",
      "Iteration 21625 | Loss: 0.268333\n",
      "Iteration 21650 | Loss: 0.268277\n",
      "Iteration 21675 | Loss: 0.268221\n",
      "Iteration 21700 | Loss: 0.268166\n",
      "Iteration 21725 | Loss: 0.268111\n",
      "Iteration 21750 | Loss: 0.268057\n",
      "Iteration 21775 | Loss: 0.268003\n",
      "Iteration 21800 | Loss: 0.267950\n",
      "Iteration 21825 | Loss: 0.267898\n",
      "Iteration 21850 | Loss: 0.267846\n",
      "Iteration 21875 | Loss: 0.267794\n",
      "Iteration 21900 | Loss: 0.267744\n",
      "Iteration 21925 | Loss: 0.267693\n",
      "Iteration 21950 | Loss: 0.267643\n",
      "Iteration 21975 | Loss: 0.267594\n",
      "Iteration 22000 | Loss: 0.267545\n",
      "Iteration 22025 | Loss: 0.267497\n",
      "Iteration 22050 | Loss: 0.267449\n",
      "Iteration 22075 | Loss: 0.267402\n",
      "Iteration 22100 | Loss: 0.267355\n",
      "Iteration 22125 | Loss: 0.267309\n",
      "Iteration 22150 | Loss: 0.267263\n",
      "Iteration 22175 | Loss: 0.267218\n",
      "Iteration 22200 | Loss: 0.267173\n",
      "Iteration 22225 | Loss: 0.267128\n",
      "Iteration 22250 | Loss: 0.267084\n",
      "Iteration 22275 | Loss: 0.267041\n",
      "Iteration 22300 | Loss: 0.266998\n",
      "Iteration 22325 | Loss: 0.266955\n",
      "Iteration 22350 | Loss: 0.266913\n",
      "Iteration 22375 | Loss: 0.266871\n",
      "Iteration 22400 | Loss: 0.266830\n",
      "Iteration 22425 | Loss: 0.266789\n",
      "Iteration 22450 | Loss: 0.266748\n",
      "Iteration 22475 | Loss: 0.266708\n",
      "Iteration 22500 | Loss: 0.266669\n",
      "Iteration 22525 | Loss: 0.266630\n",
      "Iteration 22550 | Loss: 0.266591\n",
      "Iteration 22575 | Loss: 0.266552\n",
      "Iteration 22600 | Loss: 0.266514\n",
      "Iteration 22625 | Loss: 0.266477\n",
      "Iteration 22650 | Loss: 0.266439\n",
      "Iteration 22675 | Loss: 0.266402\n",
      "Iteration 22700 | Loss: 0.266366\n",
      "Iteration 22725 | Loss: 0.266330\n",
      "Iteration 22750 | Loss: 0.266294\n",
      "Iteration 22775 | Loss: 0.266259\n",
      "Iteration 22800 | Loss: 0.266224\n",
      "Iteration 22825 | Loss: 0.266189\n",
      "Iteration 22850 | Loss: 0.266155\n",
      "Iteration 22875 | Loss: 0.266121\n",
      "Iteration 22900 | Loss: 0.266087\n",
      "Iteration 22925 | Loss: 0.266054\n",
      "Iteration 22950 | Loss: 0.266021\n",
      "Iteration 22975 | Loss: 0.265989\n",
      "Iteration 23000 | Loss: 0.265956\n",
      "Iteration 23025 | Loss: 0.265924\n",
      "Iteration 23050 | Loss: 0.265893\n",
      "Iteration 23075 | Loss: 0.265862\n",
      "Iteration 23100 | Loss: 0.265831\n",
      "Iteration 23125 | Loss: 0.265800\n",
      "Iteration 23150 | Loss: 0.265770\n",
      "Iteration 23175 | Loss: 0.265740\n",
      "Iteration 23200 | Loss: 0.265710\n",
      "Iteration 23225 | Loss: 0.265681\n",
      "Iteration 23250 | Loss: 0.265652\n",
      "Iteration 23275 | Loss: 0.265623\n",
      "Iteration 23300 | Loss: 0.265595\n",
      "Iteration 23325 | Loss: 0.265566\n",
      "Iteration 23350 | Loss: 0.265539\n",
      "Iteration 23375 | Loss: 0.265511\n",
      "Iteration 23400 | Loss: 0.265484\n",
      "Iteration 23425 | Loss: 0.265457\n",
      "Iteration 23450 | Loss: 0.265430\n",
      "Iteration 23475 | Loss: 0.265403\n",
      "Iteration 23500 | Loss: 0.265377\n",
      "Iteration 23525 | Loss: 0.265351\n",
      "Iteration 23550 | Loss: 0.265326\n",
      "Iteration 23575 | Loss: 0.265300\n",
      "Iteration 23600 | Loss: 0.265275\n",
      "Iteration 23625 | Loss: 0.265250\n",
      "Iteration 23650 | Loss: 0.265226\n",
      "Iteration 23675 | Loss: 0.265201\n",
      "Iteration 23700 | Loss: 0.265177\n",
      "Iteration 23725 | Loss: 0.265153\n",
      "Iteration 23750 | Loss: 0.265130\n",
      "Iteration 23775 | Loss: 0.265106\n",
      "Iteration 23800 | Loss: 0.265083\n",
      "Iteration 23825 | Loss: 0.265060\n",
      "Iteration 23850 | Loss: 0.265038\n",
      "Iteration 23875 | Loss: 0.265015\n",
      "Iteration 23900 | Loss: 0.264993\n",
      "Iteration 23925 | Loss: 0.264971\n",
      "Iteration 23950 | Loss: 0.264949\n",
      "Iteration 23975 | Loss: 0.264928\n",
      "Iteration 24000 | Loss: 0.264907\n",
      "Iteration 24025 | Loss: 0.264886\n",
      "Iteration 24050 | Loss: 0.264865\n",
      "Iteration 24075 | Loss: 0.264844\n",
      "Iteration 24100 | Loss: 0.264824\n",
      "Iteration 24125 | Loss: 0.264803\n",
      "Iteration 24150 | Loss: 0.264783\n",
      "Iteration 24175 | Loss: 0.264764\n",
      "Iteration 24200 | Loss: 0.264744\n",
      "Iteration 24225 | Loss: 0.264725\n",
      "Iteration 24250 | Loss: 0.264705\n",
      "Iteration 24275 | Loss: 0.264686\n",
      "Iteration 24300 | Loss: 0.264668\n",
      "Iteration 24325 | Loss: 0.264649\n",
      "Iteration 24350 | Loss: 0.264631\n",
      "Iteration 24375 | Loss: 0.264612\n",
      "Iteration 24400 | Loss: 0.264594\n",
      "Iteration 24425 | Loss: 0.264576\n",
      "Iteration 24450 | Loss: 0.264559\n",
      "Iteration 24475 | Loss: 0.264541\n",
      "Iteration 24500 | Loss: 0.264524\n",
      "Iteration 24525 | Loss: 0.264507\n",
      "Iteration 24550 | Loss: 0.264490\n",
      "Iteration 24575 | Loss: 0.264473\n",
      "Iteration 24600 | Loss: 0.264456\n",
      "Iteration 24625 | Loss: 0.264440\n",
      "Iteration 24650 | Loss: 0.264424\n",
      "Iteration 24675 | Loss: 0.264408\n",
      "Iteration 24700 | Loss: 0.264392\n",
      "Iteration 24725 | Loss: 0.264376\n",
      "Iteration 24750 | Loss: 0.264360\n",
      "Iteration 24775 | Loss: 0.264345\n",
      "Iteration 24800 | Loss: 0.264330\n",
      "Iteration 24825 | Loss: 0.264315\n",
      "Iteration 24850 | Loss: 0.264300\n",
      "Iteration 24875 | Loss: 0.264285\n",
      "Iteration 24900 | Loss: 0.264270\n",
      "Iteration 24925 | Loss: 0.264256\n",
      "Iteration 24950 | Loss: 0.264241\n",
      "Iteration 24975 | Loss: 0.264227\n",
      "Iteration 25000 | Loss: 0.264213\n",
      "Iteration 25025 | Loss: 0.264199\n",
      "Iteration 25050 | Loss: 0.264185\n",
      "Iteration 25075 | Loss: 0.264171\n",
      "Iteration 25100 | Loss: 0.264158\n",
      "Iteration 25125 | Loss: 0.264145\n",
      "Iteration 25150 | Loss: 0.264131\n",
      "Iteration 25175 | Loss: 0.264118\n",
      "Iteration 25200 | Loss: 0.264105\n",
      "Iteration 25225 | Loss: 0.264093\n",
      "Iteration 25250 | Loss: 0.264080\n",
      "Iteration 25275 | Loss: 0.264067\n",
      "Iteration 25300 | Loss: 0.264055\n",
      "Iteration 25325 | Loss: 0.264043\n",
      "Iteration 25350 | Loss: 0.264030\n",
      "Iteration 25375 | Loss: 0.264018\n",
      "Iteration 25400 | Loss: 0.264006\n",
      "Iteration 25425 | Loss: 0.263995\n",
      "Iteration 25450 | Loss: 0.263983\n",
      "Iteration 25475 | Loss: 0.263971\n",
      "Iteration 25500 | Loss: 0.263960\n",
      "Iteration 25525 | Loss: 0.263949\n",
      "Iteration 25550 | Loss: 0.263937\n",
      "Iteration 25575 | Loss: 0.263926\n",
      "Iteration 25600 | Loss: 0.263915\n",
      "Iteration 25625 | Loss: 0.263904\n",
      "Iteration 25650 | Loss: 0.263894\n",
      "Iteration 25675 | Loss: 0.263883\n",
      "Iteration 25700 | Loss: 0.263872\n",
      "Iteration 25725 | Loss: 0.263862\n",
      "Iteration 25750 | Loss: 0.263852\n",
      "Iteration 25775 | Loss: 0.263842\n",
      "Iteration 25800 | Loss: 0.263831\n",
      "Iteration 25825 | Loss: 0.263821\n",
      "Iteration 25850 | Loss: 0.263811\n",
      "Iteration 25875 | Loss: 0.263802\n",
      "Iteration 25900 | Loss: 0.263792\n",
      "Iteration 25925 | Loss: 0.263782\n",
      "Iteration 25950 | Loss: 0.263773\n",
      "Iteration 25975 | Loss: 0.263763\n",
      "Iteration 26000 | Loss: 0.263754\n",
      "Iteration 26025 | Loss: 0.263745\n",
      "Iteration 26050 | Loss: 0.263736\n",
      "Iteration 26075 | Loss: 0.263727\n",
      "Iteration 26100 | Loss: 0.263718\n",
      "Iteration 26125 | Loss: 0.263709\n",
      "Iteration 26150 | Loss: 0.263700\n",
      "Iteration 26175 | Loss: 0.263692\n",
      "Iteration 26200 | Loss: 0.263683\n",
      "Iteration 26225 | Loss: 0.263675\n",
      "Iteration 26250 | Loss: 0.263666\n",
      "Iteration 26275 | Loss: 0.263658\n",
      "Iteration 26300 | Loss: 0.263650\n",
      "Iteration 26325 | Loss: 0.263642\n",
      "Iteration 26350 | Loss: 0.263633\n",
      "Iteration 26375 | Loss: 0.263626\n",
      "Iteration 26400 | Loss: 0.263618\n",
      "Iteration 26425 | Loss: 0.263610\n",
      "Iteration 26450 | Loss: 0.263602\n",
      "Iteration 26475 | Loss: 0.263594\n",
      "Iteration 26500 | Loss: 0.263587\n",
      "Iteration 26525 | Loss: 0.263579\n",
      "Iteration 26550 | Loss: 0.263572\n",
      "Iteration 26575 | Loss: 0.263565\n",
      "Iteration 26600 | Loss: 0.263557\n",
      "Iteration 26625 | Loss: 0.263550\n",
      "Iteration 26650 | Loss: 0.263543\n",
      "Iteration 26675 | Loss: 0.263536\n",
      "Iteration 26700 | Loss: 0.263529\n",
      "Iteration 26725 | Loss: 0.263522\n",
      "Iteration 26750 | Loss: 0.263515\n",
      "Iteration 26775 | Loss: 0.263509\n",
      "Iteration 26800 | Loss: 0.263502\n",
      "Iteration 26825 | Loss: 0.263495\n",
      "Iteration 26850 | Loss: 0.263489\n",
      "Iteration 26875 | Loss: 0.263482\n",
      "Iteration 26900 | Loss: 0.263476\n",
      "Iteration 26925 | Loss: 0.263469\n",
      "Iteration 26950 | Loss: 0.263463\n",
      "Iteration 26975 | Loss: 0.263457\n",
      "Iteration 27000 | Loss: 0.263451\n",
      "Iteration 27025 | Loss: 0.263445\n",
      "Iteration 27050 | Loss: 0.263439\n",
      "Iteration 27075 | Loss: 0.263433\n",
      "Iteration 27100 | Loss: 0.263427\n",
      "Iteration 27125 | Loss: 0.263421\n",
      "Iteration 27150 | Loss: 0.263415\n",
      "Iteration 27175 | Loss: 0.263409\n",
      "Iteration 27200 | Loss: 0.263404\n",
      "Iteration 27225 | Loss: 0.263398\n",
      "Iteration 27250 | Loss: 0.263393\n",
      "Iteration 27275 | Loss: 0.263387\n",
      "Iteration 27300 | Loss: 0.263382\n",
      "Iteration 27325 | Loss: 0.263376\n",
      "Iteration 27350 | Loss: 0.263371\n",
      "Iteration 27375 | Loss: 0.263366\n",
      "Iteration 27400 | Loss: 0.263360\n",
      "Iteration 27425 | Loss: 0.263355\n",
      "Iteration 27450 | Loss: 0.263350\n",
      "Iteration 27475 | Loss: 0.263345\n",
      "Iteration 27500 | Loss: 0.263340\n",
      "Iteration 27525 | Loss: 0.263335\n",
      "Iteration 27550 | Loss: 0.263330\n",
      "Iteration 27575 | Loss: 0.263325\n",
      "Iteration 27600 | Loss: 0.263321\n",
      "Iteration 27625 | Loss: 0.263316\n",
      "Iteration 27650 | Loss: 0.263311\n",
      "Iteration 27675 | Loss: 0.263306\n",
      "Iteration 27700 | Loss: 0.263302\n",
      "Iteration 27725 | Loss: 0.263297\n",
      "Iteration 27750 | Loss: 0.263293\n",
      "Iteration 27775 | Loss: 0.263288\n",
      "Iteration 27800 | Loss: 0.263284\n",
      "Iteration 27825 | Loss: 0.263279\n",
      "Iteration 27850 | Loss: 0.263275\n",
      "Iteration 27875 | Loss: 0.263271\n",
      "Iteration 27900 | Loss: 0.263267\n",
      "Iteration 27925 | Loss: 0.263262\n",
      "Iteration 27950 | Loss: 0.263258\n",
      "Iteration 27975 | Loss: 0.263254\n",
      "Iteration 28000 | Loss: 0.263250\n",
      "Iteration 28025 | Loss: 0.263246\n",
      "Iteration 28050 | Loss: 0.263242\n",
      "Iteration 28075 | Loss: 0.263238\n",
      "Iteration 28100 | Loss: 0.263234\n",
      "Iteration 28125 | Loss: 0.263230\n",
      "Iteration 28150 | Loss: 0.263226\n",
      "Iteration 28175 | Loss: 0.263223\n",
      "Iteration 28200 | Loss: 0.263219\n",
      "Iteration 28225 | Loss: 0.263215\n",
      "Iteration 28250 | Loss: 0.263211\n",
      "Iteration 28275 | Loss: 0.263208\n",
      "Iteration 28300 | Loss: 0.263204\n",
      "Iteration 28325 | Loss: 0.263201\n",
      "Iteration 28350 | Loss: 0.263197\n",
      "Iteration 28375 | Loss: 0.263194\n",
      "Iteration 28400 | Loss: 0.263190\n",
      "Iteration 28425 | Loss: 0.263187\n",
      "Iteration 28450 | Loss: 0.263183\n",
      "Iteration 28475 | Loss: 0.263180\n",
      "Iteration 28500 | Loss: 0.263177\n",
      "Iteration 28525 | Loss: 0.263173\n",
      "Iteration 28550 | Loss: 0.263170\n",
      "Iteration 28575 | Loss: 0.263167\n",
      "Iteration 28600 | Loss: 0.263164\n",
      "Iteration 28625 | Loss: 0.263161\n",
      "Iteration 28650 | Loss: 0.263158\n",
      "Iteration 28675 | Loss: 0.263154\n",
      "Iteration 28700 | Loss: 0.263151\n",
      "Iteration 28725 | Loss: 0.263148\n",
      "Iteration 28750 | Loss: 0.263145\n",
      "Iteration 28775 | Loss: 0.263142\n",
      "Iteration 28800 | Loss: 0.263139\n",
      "Iteration 28825 | Loss: 0.263137\n",
      "Iteration 28850 | Loss: 0.263134\n",
      "Iteration 28875 | Loss: 0.263131\n",
      "Iteration 28900 | Loss: 0.263128\n",
      "Iteration 28925 | Loss: 0.263125\n",
      "Iteration 28950 | Loss: 0.263122\n",
      "Iteration 28975 | Loss: 0.263120\n",
      "Iteration 29000 | Loss: 0.263117\n",
      "Iteration 29025 | Loss: 0.263114\n",
      "Iteration 29050 | Loss: 0.263112\n",
      "Iteration 29075 | Loss: 0.263109\n",
      "Iteration 29100 | Loss: 0.263107\n",
      "Iteration 29125 | Loss: 0.263104\n",
      "Iteration 29150 | Loss: 0.263101\n",
      "Iteration 29175 | Loss: 0.263099\n",
      "Iteration 29200 | Loss: 0.263096\n",
      "Iteration 29225 | Loss: 0.263094\n",
      "Iteration 29250 | Loss: 0.263092\n",
      "Iteration 29275 | Loss: 0.263089\n",
      "Iteration 29300 | Loss: 0.263087\n",
      "Iteration 29325 | Loss: 0.263084\n",
      "Iteration 29350 | Loss: 0.263082\n",
      "Iteration 29375 | Loss: 0.263080\n",
      "Iteration 29400 | Loss: 0.263077\n",
      "Iteration 29425 | Loss: 0.263075\n",
      "Iteration 29450 | Loss: 0.263073\n",
      "Iteration 29475 | Loss: 0.263071\n",
      "Iteration 29500 | Loss: 0.263069\n",
      "Iteration 29525 | Loss: 0.263066\n",
      "Iteration 29550 | Loss: 0.263064\n",
      "Iteration 29575 | Loss: 0.263062\n",
      "Iteration 29600 | Loss: 0.263060\n",
      "Iteration 29625 | Loss: 0.263058\n",
      "Iteration 29650 | Loss: 0.263056\n",
      "Iteration 29675 | Loss: 0.263054\n",
      "Iteration 29700 | Loss: 0.263052\n",
      "Iteration 29725 | Loss: 0.263050\n",
      "Iteration 29750 | Loss: 0.263048\n",
      "Iteration 29775 | Loss: 0.263046\n",
      "Iteration 29800 | Loss: 0.263044\n",
      "Iteration 29825 | Loss: 0.263042\n",
      "Iteration 29850 | Loss: 0.263040\n",
      "Iteration 29875 | Loss: 0.263038\n",
      "Iteration 29900 | Loss: 0.263036\n",
      "Iteration 29925 | Loss: 0.263034\n",
      "Iteration 29950 | Loss: 0.263033\n",
      "Iteration 29975 | Loss: 0.263031\n",
      "Iteration 30000 | Loss: 0.263029\n",
      "Iteration 30025 | Loss: 0.263027\n",
      "Iteration 30050 | Loss: 0.263025\n",
      "Iteration 30075 | Loss: 0.263024\n",
      "Iteration 30100 | Loss: 0.263022\n",
      "Iteration 30125 | Loss: 0.263020\n",
      "Iteration 30150 | Loss: 0.263019\n",
      "Iteration 30175 | Loss: 0.263017\n",
      "Iteration 30200 | Loss: 0.263015\n",
      "Iteration 30225 | Loss: 0.263014\n",
      "Iteration 30250 | Loss: 0.263012\n",
      "Iteration 30275 | Loss: 0.263011\n",
      "Iteration 30300 | Loss: 0.263009\n",
      "Iteration 30325 | Loss: 0.263007\n",
      "Iteration 30350 | Loss: 0.263006\n",
      "Iteration 30375 | Loss: 0.263004\n",
      "Iteration 30400 | Loss: 0.263003\n",
      "Iteration 30425 | Loss: 0.263001\n",
      "Iteration 30450 | Loss: 0.263000\n",
      "Iteration 30475 | Loss: 0.262998\n",
      "Iteration 30500 | Loss: 0.262997\n",
      "Iteration 30525 | Loss: 0.262995\n",
      "Iteration 30550 | Loss: 0.262994\n",
      "Iteration 30575 | Loss: 0.262993\n",
      "Iteration 30600 | Loss: 0.262991\n",
      "Iteration 30625 | Loss: 0.262990\n",
      "Iteration 30650 | Loss: 0.262988\n",
      "Iteration 30675 | Loss: 0.262987\n",
      "Iteration 30700 | Loss: 0.262986\n",
      "Iteration 30725 | Loss: 0.262984\n",
      "Iteration 30750 | Loss: 0.262983\n",
      "Iteration 30775 | Loss: 0.262982\n",
      "Iteration 30800 | Loss: 0.262980\n",
      "Iteration 30825 | Loss: 0.262979\n",
      "Iteration 30850 | Loss: 0.262978\n",
      "Iteration 30875 | Loss: 0.262977\n",
      "Iteration 30900 | Loss: 0.262975\n",
      "Iteration 30925 | Loss: 0.262974\n",
      "Iteration 30950 | Loss: 0.262973\n",
      "Iteration 30975 | Loss: 0.262972\n",
      "Iteration 31000 | Loss: 0.262971\n",
      "Iteration 31025 | Loss: 0.262969\n",
      "Iteration 31050 | Loss: 0.262968\n",
      "Iteration 31075 | Loss: 0.262967\n",
      "Iteration 31100 | Loss: 0.262966\n",
      "Iteration 31125 | Loss: 0.262965\n",
      "Iteration 31150 | Loss: 0.262964\n",
      "Iteration 31175 | Loss: 0.262963\n",
      "Iteration 31200 | Loss: 0.262962\n",
      "Iteration 31225 | Loss: 0.262961\n",
      "Iteration 31250 | Loss: 0.262959\n",
      "Iteration 31275 | Loss: 0.262958\n",
      "Iteration 31300 | Loss: 0.262957\n",
      "Iteration 31325 | Loss: 0.262956\n",
      "Iteration 31350 | Loss: 0.262955\n",
      "Iteration 31375 | Loss: 0.262954\n",
      "Iteration 31400 | Loss: 0.262953\n",
      "Iteration 31425 | Loss: 0.262952\n",
      "Iteration 31450 | Loss: 0.262951\n",
      "Iteration 31475 | Loss: 0.262950\n",
      "Iteration 31500 | Loss: 0.262949\n",
      "Iteration 31525 | Loss: 0.262948\n",
      "Iteration 31550 | Loss: 0.262947\n",
      "Iteration 31575 | Loss: 0.262946\n",
      "Iteration 31600 | Loss: 0.262946\n",
      "Iteration 31625 | Loss: 0.262945\n",
      "Iteration 31650 | Loss: 0.262944\n",
      "Iteration 31675 | Loss: 0.262943\n",
      "Iteration 31700 | Loss: 0.262942\n",
      "Iteration 31725 | Loss: 0.262941\n",
      "Iteration 31750 | Loss: 0.262940\n",
      "Iteration 31775 | Loss: 0.262939\n",
      "Iteration 31800 | Loss: 0.262938\n",
      "Iteration 31825 | Loss: 0.262938\n",
      "Iteration 31850 | Loss: 0.262937\n",
      "Iteration 31875 | Loss: 0.262936\n",
      "Iteration 31900 | Loss: 0.262935\n",
      "Iteration 31925 | Loss: 0.262934\n",
      "Iteration 31950 | Loss: 0.262934\n",
      "Iteration 31975 | Loss: 0.262933\n",
      "Iteration 32000 | Loss: 0.262932\n",
      "Iteration 32025 | Loss: 0.262931\n",
      "Iteration 32050 | Loss: 0.262930\n",
      "Iteration 32075 | Loss: 0.262930\n",
      "Iteration 32100 | Loss: 0.262929\n",
      "Iteration 32125 | Loss: 0.262928\n",
      "Iteration 32150 | Loss: 0.262927\n",
      "Iteration 32175 | Loss: 0.262927\n",
      "Iteration 32200 | Loss: 0.262926\n",
      "Iteration 32225 | Loss: 0.262925\n",
      "Iteration 32250 | Loss: 0.262925\n",
      "Iteration 32275 | Loss: 0.262924\n",
      "Iteration 32300 | Loss: 0.262923\n",
      "Iteration 32325 | Loss: 0.262922\n",
      "Iteration 32350 | Loss: 0.262922\n",
      "Iteration 32375 | Loss: 0.262921\n",
      "Iteration 32400 | Loss: 0.262920\n",
      "Iteration 32425 | Loss: 0.262920\n",
      "Iteration 32450 | Loss: 0.262919\n",
      "Iteration 32475 | Loss: 0.262918\n",
      "Iteration 32500 | Loss: 0.262918\n",
      "Iteration 32525 | Loss: 0.262917\n",
      "Iteration 32550 | Loss: 0.262917\n",
      "Iteration 32575 | Loss: 0.262916\n",
      "Iteration 32600 | Loss: 0.262915\n",
      "Iteration 32625 | Loss: 0.262915\n",
      "Iteration 32650 | Loss: 0.262914\n",
      "Iteration 32675 | Loss: 0.262914\n",
      "Iteration 32700 | Loss: 0.262913\n",
      "Iteration 32725 | Loss: 0.262912\n",
      "Iteration 32750 | Loss: 0.262912\n",
      "Iteration 32775 | Loss: 0.262911\n",
      "Iteration 32800 | Loss: 0.262911\n",
      "Iteration 32825 | Loss: 0.262910\n",
      "Iteration 32850 | Loss: 0.262909\n",
      "Iteration 32875 | Loss: 0.262909\n",
      "Iteration 32900 | Loss: 0.262908\n",
      "Iteration 32925 | Loss: 0.262908\n",
      "Iteration 32950 | Loss: 0.262907\n",
      "Iteration 32975 | Loss: 0.262907\n",
      "Iteration 33000 | Loss: 0.262906\n",
      "Iteration 33025 | Loss: 0.262906\n",
      "Iteration 33050 | Loss: 0.262905\n",
      "Iteration 33075 | Loss: 0.262905\n",
      "Iteration 33100 | Loss: 0.262904\n",
      "Iteration 33125 | Loss: 0.262904\n",
      "Iteration 33150 | Loss: 0.262903\n",
      "Iteration 33175 | Loss: 0.262903\n",
      "Iteration 33200 | Loss: 0.262902\n",
      "Iteration 33225 | Loss: 0.262902\n",
      "Iteration 33250 | Loss: 0.262901\n",
      "Iteration 33275 | Loss: 0.262901\n",
      "Iteration 33300 | Loss: 0.262900\n",
      "Iteration 33325 | Loss: 0.262900\n",
      "Iteration 33350 | Loss: 0.262900\n",
      "Iteration 33375 | Loss: 0.262899\n",
      "Iteration 33400 | Loss: 0.262899\n",
      "Iteration 33425 | Loss: 0.262898\n",
      "Iteration 33450 | Loss: 0.262898\n",
      "Iteration 33475 | Loss: 0.262897\n",
      "Iteration 33500 | Loss: 0.262897\n",
      "Iteration 33525 | Loss: 0.262896\n",
      "Iteration 33550 | Loss: 0.262896\n",
      "Iteration 33575 | Loss: 0.262896\n",
      "Iteration 33600 | Loss: 0.262895\n",
      "Iteration 33625 | Loss: 0.262895\n",
      "Iteration 33650 | Loss: 0.262894\n",
      "Iteration 33675 | Loss: 0.262894\n",
      "Iteration 33700 | Loss: 0.262894\n",
      "Iteration 33725 | Loss: 0.262893\n",
      "Iteration 33750 | Loss: 0.262893\n",
      "Iteration 33775 | Loss: 0.262892\n",
      "Iteration 33800 | Loss: 0.262892\n",
      "Iteration 33825 | Loss: 0.262892\n",
      "Iteration 33850 | Loss: 0.262891\n",
      "Iteration 33875 | Loss: 0.262891\n",
      "Iteration 33900 | Loss: 0.262891\n",
      "Iteration 33925 | Loss: 0.262890\n",
      "Iteration 33950 | Loss: 0.262890\n",
      "Iteration 33975 | Loss: 0.262890\n",
      "Iteration 34000 | Loss: 0.262889\n",
      "Iteration 34025 | Loss: 0.262889\n",
      "Iteration 34050 | Loss: 0.262889\n",
      "Iteration 34075 | Loss: 0.262888\n",
      "Iteration 34100 | Loss: 0.262888\n",
      "Iteration 34125 | Loss: 0.262888\n",
      "Iteration 34150 | Loss: 0.262887\n",
      "Iteration 34175 | Loss: 0.262887\n",
      "Iteration 34200 | Loss: 0.262887\n",
      "Iteration 34225 | Loss: 0.262886\n",
      "Iteration 34250 | Loss: 0.262886\n",
      "Iteration 34275 | Loss: 0.262886\n",
      "Iteration 34300 | Loss: 0.262885\n",
      "Iteration 34325 | Loss: 0.262885\n",
      "Iteration 34350 | Loss: 0.262885\n",
      "Iteration 34375 | Loss: 0.262884\n",
      "Iteration 34400 | Loss: 0.262884\n",
      "Iteration 34425 | Loss: 0.262884\n",
      "Iteration 34450 | Loss: 0.262884\n",
      "Iteration 34475 | Loss: 0.262883\n",
      "Iteration 34500 | Loss: 0.262883\n",
      "Iteration 34525 | Loss: 0.262883\n",
      "Iteration 34550 | Loss: 0.262882\n",
      "Iteration 34575 | Loss: 0.262882\n",
      "Iteration 34600 | Loss: 0.262882\n",
      "Iteration 34625 | Loss: 0.262882\n",
      "Iteration 34650 | Loss: 0.262881\n",
      "Iteration 34675 | Loss: 0.262881\n",
      "Iteration 34700 | Loss: 0.262881\n",
      "Iteration 34725 | Loss: 0.262881\n",
      "Iteration 34750 | Loss: 0.262880\n",
      "Iteration 34775 | Loss: 0.262880\n",
      "Iteration 34800 | Loss: 0.262880\n",
      "Iteration 34825 | Loss: 0.262880\n",
      "Iteration 34850 | Loss: 0.262879\n",
      "Iteration 34875 | Loss: 0.262879\n",
      "Iteration 34900 | Loss: 0.262879\n",
      "Iteration 34925 | Loss: 0.262879\n",
      "Iteration 34950 | Loss: 0.262878\n",
      "Iteration 34975 | Loss: 0.262878\n",
      "Iteration 35000 | Loss: 0.262878\n",
      "Iteration 35025 | Loss: 0.262878\n",
      "Iteration 35050 | Loss: 0.262877\n",
      "Iteration 35075 | Loss: 0.262877\n",
      "Iteration 35100 | Loss: 0.262877\n",
      "Iteration 35125 | Loss: 0.262877\n",
      "Iteration 35150 | Loss: 0.262877\n",
      "Iteration 35175 | Loss: 0.262876\n",
      "Iteration 35200 | Loss: 0.262876\n",
      "Iteration 35225 | Loss: 0.262876\n",
      "Iteration 35250 | Loss: 0.262876\n",
      "Iteration 35275 | Loss: 0.262876\n",
      "Iteration 35300 | Loss: 0.262875\n",
      "Iteration 35325 | Loss: 0.262875\n",
      "Iteration 35350 | Loss: 0.262875\n",
      "Iteration 35375 | Loss: 0.262875\n",
      "Iteration 35400 | Loss: 0.262875\n",
      "Iteration 35425 | Loss: 0.262874\n",
      "Iteration 35450 | Loss: 0.262874\n",
      "Iteration 35475 | Loss: 0.262874\n",
      "Iteration 35500 | Loss: 0.262874\n",
      "Iteration 35525 | Loss: 0.262874\n",
      "Iteration 35550 | Loss: 0.262873\n",
      "Iteration 35575 | Loss: 0.262873\n",
      "Iteration 35600 | Loss: 0.262873\n",
      "Iteration 35625 | Loss: 0.262873\n",
      "Iteration 35650 | Loss: 0.262873\n",
      "Iteration 35675 | Loss: 0.262873\n",
      "Iteration 35700 | Loss: 0.262872\n",
      "Iteration 35725 | Loss: 0.262872\n",
      "Iteration 35750 | Loss: 0.262872\n",
      "Iteration 35775 | Loss: 0.262872\n",
      "Iteration 35800 | Loss: 0.262872\n",
      "Iteration 35825 | Loss: 0.262872\n",
      "Iteration 35850 | Loss: 0.262871\n",
      "Iteration 35875 | Loss: 0.262871\n",
      "Iteration 35900 | Loss: 0.262871\n",
      "Iteration 35925 | Loss: 0.262871\n",
      "Iteration 35950 | Loss: 0.262871\n",
      "Iteration 35975 | Loss: 0.262871\n",
      "Iteration 36000 | Loss: 0.262870\n",
      "Iteration 36025 | Loss: 0.262870\n",
      "Iteration 36050 | Loss: 0.262870\n",
      "Iteration 36075 | Loss: 0.262870\n",
      "Iteration 36100 | Loss: 0.262870\n",
      "Iteration 36125 | Loss: 0.262870\n",
      "Iteration 36150 | Loss: 0.262870\n",
      "Iteration 36175 | Loss: 0.262869\n",
      "Iteration 36200 | Loss: 0.262869\n",
      "Iteration 36225 | Loss: 0.262869\n",
      "Iteration 36250 | Loss: 0.262869\n",
      "Iteration 36275 | Loss: 0.262869\n",
      "Iteration 36300 | Loss: 0.262869\n",
      "Iteration 36325 | Loss: 0.262869\n",
      "Iteration 36350 | Loss: 0.262868\n",
      "Iteration 36375 | Loss: 0.262868\n",
      "Iteration 36400 | Loss: 0.262868\n",
      "Iteration 36425 | Loss: 0.262868\n",
      "Iteration 36450 | Loss: 0.262868\n",
      "Iteration 36475 | Loss: 0.262868\n",
      "Iteration 36500 | Loss: 0.262868\n",
      "Iteration 36525 | Loss: 0.262868\n",
      "Iteration 36550 | Loss: 0.262867\n",
      "Iteration 36575 | Loss: 0.262867\n",
      "Iteration 36600 | Loss: 0.262867\n",
      "Iteration 36625 | Loss: 0.262867\n",
      "Iteration 36650 | Loss: 0.262867\n",
      "Iteration 36675 | Loss: 0.262867\n",
      "Iteration 36700 | Loss: 0.262867\n",
      "Iteration 36725 | Loss: 0.262867\n",
      "Iteration 36750 | Loss: 0.262866\n",
      "Iteration 36775 | Loss: 0.262866\n",
      "Iteration 36800 | Loss: 0.262866\n",
      "Iteration 36825 | Loss: 0.262866\n",
      "Iteration 36850 | Loss: 0.262866\n",
      "Iteration 36875 | Loss: 0.262866\n",
      "Iteration 36900 | Loss: 0.262866\n",
      "Iteration 36925 | Loss: 0.262866\n",
      "Iteration 36950 | Loss: 0.262866\n",
      "Iteration 36975 | Loss: 0.262865\n",
      "Iteration 37000 | Loss: 0.262865\n",
      "Iteration 37025 | Loss: 0.262865\n",
      "Iteration 37050 | Loss: 0.262865\n",
      "Iteration 37075 | Loss: 0.262865\n",
      "Iteration 37100 | Loss: 0.262865\n",
      "Iteration 37125 | Loss: 0.262865\n",
      "Iteration 37150 | Loss: 0.262865\n",
      "Iteration 37175 | Loss: 0.262865\n",
      "Iteration 37200 | Loss: 0.262865\n",
      "Iteration 37225 | Loss: 0.262865\n",
      "Iteration 37250 | Loss: 0.262864\n",
      "Iteration 37275 | Loss: 0.262864\n",
      "Iteration 37300 | Loss: 0.262864\n",
      "Iteration 37325 | Loss: 0.262864\n",
      "Iteration 37350 | Loss: 0.262864\n",
      "Iteration 37375 | Loss: 0.262864\n",
      "Iteration 37400 | Loss: 0.262864\n",
      "Iteration 37425 | Loss: 0.262864\n",
      "Iteration 37450 | Loss: 0.262864\n",
      "Iteration 37475 | Loss: 0.262864\n",
      "Iteration 37500 | Loss: 0.262864\n",
      "Iteration 37525 | Loss: 0.262863\n",
      "Iteration 37550 | Loss: 0.262863\n",
      "Iteration 37575 | Loss: 0.262863\n",
      "Iteration 37600 | Loss: 0.262863\n",
      "Iteration 37625 | Loss: 0.262863\n",
      "Iteration 37650 | Loss: 0.262863\n",
      "Iteration 37675 | Loss: 0.262863\n",
      "Iteration 37700 | Loss: 0.262863\n",
      "Iteration 37725 | Loss: 0.262863\n",
      "Iteration 37750 | Loss: 0.262863\n",
      "Iteration 37775 | Loss: 0.262863\n",
      "Iteration 37800 | Loss: 0.262863\n",
      "Iteration 37825 | Loss: 0.262863\n",
      "Iteration 37850 | Loss: 0.262862\n",
      "Iteration 37875 | Loss: 0.262862\n",
      "Iteration 37900 | Loss: 0.262862\n",
      "Iteration 37925 | Loss: 0.262862\n",
      "Iteration 37950 | Loss: 0.262862\n",
      "Iteration 37975 | Loss: 0.262862\n",
      "Iteration 38000 | Loss: 0.262862\n",
      "Iteration 38025 | Loss: 0.262862\n",
      "Iteration 38050 | Loss: 0.262862\n",
      "Iteration 38075 | Loss: 0.262862\n",
      "Iteration 38100 | Loss: 0.262862\n",
      "Iteration 38125 | Loss: 0.262862\n",
      "Iteration 38150 | Loss: 0.262862\n",
      "Iteration 38175 | Loss: 0.262862\n",
      "Iteration 38200 | Loss: 0.262862\n",
      "Iteration 38225 | Loss: 0.262861\n",
      "Iteration 38250 | Loss: 0.262861\n",
      "Iteration 38275 | Loss: 0.262861\n",
      "Iteration 38300 | Loss: 0.262861\n",
      "Iteration 38325 | Loss: 0.262861\n",
      "Iteration 38350 | Loss: 0.262861\n",
      "Iteration 38375 | Loss: 0.262861\n",
      "Iteration 38400 | Loss: 0.262861\n",
      "Iteration 38425 | Loss: 0.262861\n",
      "Iteration 38450 | Loss: 0.262861\n",
      "Iteration 38475 | Loss: 0.262861\n",
      "Iteration 38500 | Loss: 0.262861\n",
      "Iteration 38525 | Loss: 0.262861\n",
      "Iteration 38550 | Loss: 0.262861\n",
      "Iteration 38575 | Loss: 0.262861\n",
      "Iteration 38600 | Loss: 0.262861\n",
      "Iteration 38625 | Loss: 0.262861\n",
      "Iteration 38650 | Loss: 0.262861\n",
      "Iteration 38675 | Loss: 0.262860\n",
      "Iteration 38700 | Loss: 0.262860\n",
      "Iteration 38725 | Loss: 0.262860\n",
      "Iteration 38750 | Loss: 0.262860\n",
      "Iteration 38775 | Loss: 0.262860\n",
      "Iteration 38800 | Loss: 0.262860\n",
      "Iteration 38825 | Loss: 0.262860\n",
      "Iteration 38850 | Loss: 0.262860\n",
      "Iteration 38875 | Loss: 0.262860\n",
      "Iteration 38900 | Loss: 0.262860\n",
      "Iteration 38925 | Loss: 0.262860\n",
      "Iteration 38950 | Loss: 0.262860\n",
      "Iteration 38975 | Loss: 0.262860\n",
      "Iteration 39000 | Loss: 0.262860\n",
      "Iteration 39025 | Loss: 0.262860\n",
      "Iteration 39050 | Loss: 0.262860\n",
      "Iteration 39075 | Loss: 0.262860\n",
      "Iteration 39100 | Loss: 0.262860\n",
      "Iteration 39125 | Loss: 0.262860\n",
      "Iteration 39150 | Loss: 0.262860\n",
      "Iteration 39175 | Loss: 0.262860\n",
      "Iteration 39200 | Loss: 0.262859\n",
      "Iteration 39225 | Loss: 0.262859\n",
      "Iteration 39250 | Loss: 0.262859\n",
      "Iteration 39275 | Loss: 0.262859\n",
      "Iteration 39300 | Loss: 0.262859\n",
      "Iteration 39325 | Loss: 0.262859\n",
      "Iteration 39350 | Loss: 0.262859\n",
      "Iteration 39375 | Loss: 0.262859\n",
      "Iteration 39400 | Loss: 0.262859\n",
      "Iteration 39425 | Loss: 0.262859\n",
      "Iteration 39450 | Loss: 0.262859\n",
      "Iteration 39475 | Loss: 0.262859\n",
      "Iteration 39500 | Loss: 0.262859\n",
      "Iteration 39525 | Loss: 0.262859\n",
      "Iteration 39550 | Loss: 0.262859\n",
      "Iteration 39575 | Loss: 0.262859\n",
      "Iteration 39600 | Loss: 0.262859\n",
      "Iteration 39625 | Loss: 0.262859\n",
      "Iteration 39650 | Loss: 0.262859\n",
      "Iteration 39675 | Loss: 0.262859\n",
      "Iteration 39700 | Loss: 0.262859\n",
      "Iteration 39725 | Loss: 0.262859\n",
      "Iteration 39750 | Loss: 0.262859\n",
      "Iteration 39775 | Loss: 0.262859\n",
      "Iteration 39800 | Loss: 0.262859\n",
      "Iteration 39825 | Loss: 0.262859\n",
      "Iteration 39850 | Loss: 0.262859\n",
      "Iteration 39875 | Loss: 0.262859\n",
      "Iteration 39900 | Loss: 0.262858\n",
      "Iteration 39925 | Loss: 0.262858\n",
      "Iteration 39950 | Loss: 0.262858\n",
      "Iteration 39975 | Loss: 0.262858\n",
      "Iteration 40000 | Loss: 0.262858\n",
      "Iteration 40025 | Loss: 0.262858\n",
      "Iteration 40050 | Loss: 0.262858\n",
      "Iteration 40075 | Loss: 0.262858\n",
      "Iteration 40100 | Loss: 0.262858\n",
      "Iteration 40125 | Loss: 0.262858\n",
      "Iteration 40150 | Loss: 0.262858\n",
      "Iteration 40175 | Loss: 0.262858\n",
      "Iteration 40200 | Loss: 0.262858\n",
      "Iteration 40225 | Loss: 0.262858\n",
      "Iteration 40250 | Loss: 0.262858\n",
      "Iteration 40275 | Loss: 0.262858\n",
      "Iteration 40300 | Loss: 0.262858\n",
      "Iteration 40325 | Loss: 0.262858\n",
      "Iteration 40350 | Loss: 0.262858\n",
      "Iteration 40375 | Loss: 0.262858\n",
      "Iteration 40400 | Loss: 0.262858\n",
      "Iteration 40425 | Loss: 0.262858\n",
      "Iteration 40450 | Loss: 0.262858\n",
      "Iteration 40475 | Loss: 0.262858\n",
      "Iteration 40500 | Loss: 0.262858\n",
      "Iteration 40525 | Loss: 0.262858\n",
      "Iteration 40550 | Loss: 0.262858\n",
      "Iteration 40575 | Loss: 0.262858\n",
      "Iteration 40600 | Loss: 0.262858\n",
      "Iteration 40625 | Loss: 0.262858\n",
      "Iteration 40650 | Loss: 0.262858\n",
      "Iteration 40675 | Loss: 0.262858\n",
      "Iteration 40700 | Loss: 0.262858\n",
      "Iteration 40725 | Loss: 0.262858\n",
      "Iteration 40750 | Loss: 0.262858\n",
      "Iteration 40775 | Loss: 0.262858\n",
      "Iteration 40800 | Loss: 0.262858\n",
      "Iteration 40825 | Loss: 0.262858\n",
      "Iteration 40850 | Loss: 0.262858\n",
      "Iteration 40875 | Loss: 0.262857\n",
      "Iteration 40900 | Loss: 0.262857\n",
      "Iteration 40925 | Loss: 0.262857\n",
      "Iteration 40950 | Loss: 0.262857\n",
      "Iteration 40975 | Loss: 0.262857\n",
      "Iteration 41000 | Loss: 0.262857\n",
      "Iteration 41025 | Loss: 0.262857\n",
      "Iteration 41050 | Loss: 0.262857\n",
      "Iteration 41075 | Loss: 0.262857\n",
      "Iteration 41100 | Loss: 0.262857\n",
      "Iteration 41125 | Loss: 0.262857\n",
      "Iteration 41150 | Loss: 0.262857\n",
      "Iteration 41175 | Loss: 0.262857\n",
      "Iteration 41200 | Loss: 0.262857\n",
      "Iteration 41225 | Loss: 0.262857\n",
      "Iteration 41250 | Loss: 0.262857\n",
      "Iteration 41275 | Loss: 0.262857\n",
      "Iteration 41300 | Loss: 0.262857\n",
      "Iteration 41325 | Loss: 0.262857\n",
      "Iteration 41350 | Loss: 0.262857\n",
      "Iteration 41375 | Loss: 0.262857\n",
      "Iteration 41400 | Loss: 0.262857\n",
      "Iteration 41425 | Loss: 0.262857\n",
      "Iteration 41450 | Loss: 0.262857\n",
      "Iteration 41475 | Loss: 0.262857\n",
      "Iteration 41500 | Loss: 0.262857\n",
      "Iteration 41525 | Loss: 0.262857\n",
      "Iteration 41550 | Loss: 0.262857\n",
      "Iteration 41575 | Loss: 0.262857\n",
      "Iteration 41600 | Loss: 0.262857\n",
      "Iteration 41625 | Loss: 0.262857\n",
      "Iteration 41650 | Loss: 0.262857\n",
      "Iteration 41675 | Loss: 0.262857\n",
      "Iteration 41700 | Loss: 0.262857\n",
      "Iteration 41725 | Loss: 0.262857\n",
      "Iteration 41750 | Loss: 0.262857\n",
      "Iteration 41775 | Loss: 0.262857\n",
      "Iteration 41800 | Loss: 0.262857\n",
      "Iteration 41825 | Loss: 0.262857\n",
      "Iteration 41850 | Loss: 0.262857\n",
      "Iteration 41875 | Loss: 0.262857\n",
      "Iteration 41900 | Loss: 0.262857\n",
      "Iteration 41925 | Loss: 0.262857\n",
      "Iteration 41950 | Loss: 0.262857\n",
      "Iteration 41975 | Loss: 0.262857\n",
      "Iteration 42000 | Loss: 0.262857\n",
      "Iteration 42025 | Loss: 0.262857\n",
      "Iteration 42050 | Loss: 0.262857\n",
      "Iteration 42075 | Loss: 0.262857\n",
      "Iteration 42100 | Loss: 0.262857\n",
      "Iteration 42125 | Loss: 0.262857\n",
      "Iteration 42150 | Loss: 0.262857\n",
      "Iteration 42175 | Loss: 0.262857\n",
      "Iteration 42200 | Loss: 0.262857\n",
      "Iteration 42225 | Loss: 0.262857\n",
      "Iteration 42250 | Loss: 0.262857\n",
      "Iteration 42275 | Loss: 0.262857\n",
      "Iteration 42300 | Loss: 0.262857\n",
      "Iteration 42325 | Loss: 0.262857\n",
      "Iteration 42350 | Loss: 0.262857\n",
      "Iteration 42375 | Loss: 0.262857\n",
      "Iteration 42400 | Loss: 0.262857\n",
      "Iteration 42425 | Loss: 0.262857\n",
      "Iteration 42450 | Loss: 0.262856\n",
      "Iteration 42475 | Loss: 0.262856\n",
      "Iteration 42500 | Loss: 0.262856\n",
      "Iteration 42525 | Loss: 0.262856\n",
      "Iteration 42550 | Loss: 0.262856\n",
      "Iteration 42575 | Loss: 0.262856\n",
      "Iteration 42600 | Loss: 0.262856\n",
      "Iteration 42625 | Loss: 0.262856\n",
      "Iteration 42650 | Loss: 0.262856\n",
      "Iteration 42675 | Loss: 0.262856\n",
      "Iteration 42700 | Loss: 0.262856\n",
      "Iteration 42725 | Loss: 0.262856\n",
      "Iteration 42750 | Loss: 0.262856\n",
      "Iteration 42775 | Loss: 0.262856\n",
      "Iteration 42800 | Loss: 0.262856\n",
      "Iteration 42825 | Loss: 0.262856\n",
      "Iteration 42850 | Loss: 0.262856\n",
      "Iteration 42875 | Loss: 0.262856\n",
      "Iteration 42900 | Loss: 0.262856\n",
      "Iteration 42925 | Loss: 0.262856\n",
      "Iteration 42950 | Loss: 0.262856\n",
      "Iteration 42975 | Loss: 0.262856\n",
      "Iteration 43000 | Loss: 0.262856\n",
      "Iteration 43025 | Loss: 0.262856\n",
      "Iteration 43050 | Loss: 0.262856\n",
      "Iteration 43075 | Loss: 0.262856\n",
      "Iteration 43100 | Loss: 0.262856\n",
      "Iteration 43125 | Loss: 0.262856\n",
      "Iteration 43150 | Loss: 0.262856\n",
      "Iteration 43175 | Loss: 0.262856\n",
      "Iteration 43200 | Loss: 0.262856\n",
      "Iteration 43225 | Loss: 0.262856\n",
      "Iteration 43250 | Loss: 0.262856\n",
      "Iteration 43275 | Loss: 0.262856\n",
      "Iteration 43300 | Loss: 0.262856\n",
      "Iteration 43325 | Loss: 0.262856\n",
      "Iteration 43350 | Loss: 0.262856\n",
      "Iteration 43375 | Loss: 0.262856\n",
      "Iteration 43400 | Loss: 0.262856\n",
      "Iteration 43425 | Loss: 0.262856\n",
      "Iteration 43450 | Loss: 0.262856\n",
      "Iteration 43475 | Loss: 0.262856\n",
      "Iteration 43500 | Loss: 0.262856\n",
      "Iteration 43525 | Loss: 0.262856\n",
      "Iteration 43550 | Loss: 0.262856\n",
      "Iteration 43575 | Loss: 0.262856\n",
      "Iteration 43600 | Loss: 0.262856\n",
      "Iteration 43625 | Loss: 0.262856\n",
      "Iteration 43650 | Loss: 0.262856\n",
      "Iteration 43675 | Loss: 0.262856\n",
      "Iteration 43700 | Loss: 0.262856\n",
      "Iteration 43725 | Loss: 0.262856\n",
      "Iteration 43750 | Loss: 0.262856\n",
      "Iteration 43775 | Loss: 0.262856\n",
      "Iteration 43800 | Loss: 0.262856\n",
      "Iteration 43825 | Loss: 0.262856\n",
      "Iteration 43850 | Loss: 0.262856\n",
      "Iteration 43875 | Loss: 0.262856\n",
      "Iteration 43900 | Loss: 0.262856\n",
      "Iteration 43925 | Loss: 0.262856\n",
      "Iteration 43950 | Loss: 0.262856\n",
      "Iteration 43975 | Loss: 0.262856\n",
      "Iteration 44000 | Loss: 0.262856\n",
      "Iteration 44025 | Loss: 0.262856\n",
      "Iteration 44050 | Loss: 0.262856\n",
      "Iteration 44075 | Loss: 0.262856\n",
      "Iteration 44100 | Loss: 0.262856\n",
      "Iteration 44125 | Loss: 0.262856\n",
      "Iteration 44150 | Loss: 0.262856\n",
      "Iteration 44175 | Loss: 0.262856\n",
      "Iteration 44200 | Loss: 0.262856\n",
      "Iteration 44225 | Loss: 0.262856\n",
      "Iteration 44250 | Loss: 0.262856\n",
      "Iteration 44275 | Loss: 0.262856\n",
      "Iteration 44300 | Loss: 0.262856\n",
      "Iteration 44325 | Loss: 0.262856\n",
      "Iteration 44350 | Loss: 0.262856\n",
      "Iteration 44375 | Loss: 0.262856\n",
      "Iteration 44400 | Loss: 0.262856\n",
      "Iteration 44425 | Loss: 0.262856\n",
      "Iteration 44450 | Loss: 0.262856\n",
      "Iteration 44475 | Loss: 0.262856\n",
      "Iteration 44500 | Loss: 0.262856\n",
      "Iteration 44525 | Loss: 0.262856\n",
      "Iteration 44550 | Loss: 0.262856\n",
      "Iteration 44575 | Loss: 0.262856\n",
      "Iteration 44600 | Loss: 0.262856\n",
      "Iteration 44625 | Loss: 0.262856\n",
      "Iteration 44650 | Loss: 0.262856\n",
      "Iteration 44675 | Loss: 0.262856\n",
      "Iteration 44700 | Loss: 0.262856\n",
      "Iteration 44725 | Loss: 0.262856\n",
      "Iteration 44750 | Loss: 0.262856\n",
      "Iteration 44775 | Loss: 0.262856\n",
      "Iteration 44800 | Loss: 0.262856\n",
      "Iteration 44825 | Loss: 0.262856\n",
      "Iteration 44850 | Loss: 0.262856\n",
      "Iteration 44875 | Loss: 0.262856\n",
      "Iteration 44900 | Loss: 0.262856\n",
      "Iteration 44925 | Loss: 0.262856\n",
      "Iteration 44950 | Loss: 0.262856\n",
      "Iteration 44975 | Loss: 0.262856\n",
      "Iteration 45000 | Loss: 0.262856\n",
      "Iteration 45025 | Loss: 0.262856\n",
      "Iteration 45050 | Loss: 0.262856\n",
      "Iteration 45075 | Loss: 0.262856\n",
      "Iteration 45100 | Loss: 0.262856\n",
      "Iteration 45125 | Loss: 0.262856\n",
      "Iteration 45150 | Loss: 0.262856\n",
      "Iteration 45175 | Loss: 0.262856\n",
      "Iteration 45200 | Loss: 0.262856\n",
      "Iteration 45225 | Loss: 0.262856\n",
      "Iteration 45250 | Loss: 0.262856\n",
      "Iteration 45275 | Loss: 0.262856\n",
      "Iteration 45300 | Loss: 0.262856\n",
      "Iteration 45325 | Loss: 0.262856\n",
      "Iteration 45350 | Loss: 0.262856\n",
      "Iteration 45375 | Loss: 0.262856\n",
      "Iteration 45400 | Loss: 0.262856\n",
      "Iteration 45425 | Loss: 0.262856\n",
      "Iteration 45450 | Loss: 0.262856\n",
      "Iteration 45475 | Loss: 0.262856\n",
      "Iteration 45500 | Loss: 0.262856\n",
      "Iteration 45525 | Loss: 0.262856\n",
      "Iteration 45550 | Loss: 0.262856\n",
      "Iteration 45575 | Loss: 0.262856\n",
      "Iteration 45600 | Loss: 0.262856\n",
      "Iteration 45625 | Loss: 0.262856\n",
      "Iteration 45650 | Loss: 0.262856\n",
      "Iteration 45675 | Loss: 0.262856\n",
      "Iteration 45700 | Loss: 0.262856\n",
      "Iteration 45725 | Loss: 0.262856\n",
      "Iteration 45750 | Loss: 0.262856\n",
      "Iteration 45775 | Loss: 0.262856\n",
      "Iteration 45800 | Loss: 0.262856\n",
      "Iteration 45825 | Loss: 0.262856\n",
      "Iteration 45850 | Loss: 0.262856\n",
      "Iteration 45875 | Loss: 0.262856\n",
      "Iteration 45900 | Loss: 0.262856\n",
      "Iteration 45925 | Loss: 0.262856\n",
      "Iteration 45950 | Loss: 0.262856\n",
      "Iteration 45975 | Loss: 0.262856\n",
      "Iteration 46000 | Loss: 0.262856\n",
      "Iteration 46025 | Loss: 0.262856\n",
      "Iteration 46050 | Loss: 0.262856\n",
      "Iteration 46075 | Loss: 0.262856\n",
      "Iteration 46100 | Loss: 0.262856\n",
      "Iteration 46125 | Loss: 0.262856\n",
      "Iteration 46150 | Loss: 0.262856\n",
      "Iteration 46175 | Loss: 0.262856\n",
      "Iteration 46200 | Loss: 0.262856\n",
      "Iteration 46225 | Loss: 0.262856\n",
      "Iteration 46250 | Loss: 0.262856\n",
      "Iteration 46275 | Loss: 0.262856\n",
      "Iteration 46300 | Loss: 0.262856\n",
      "Iteration 46325 | Loss: 0.262856\n",
      "Iteration 46350 | Loss: 0.262856\n",
      "Iteration 46375 | Loss: 0.262856\n",
      "Iteration 46400 | Loss: 0.262856\n",
      "Iteration 46425 | Loss: 0.262856\n",
      "Iteration 46450 | Loss: 0.262856\n",
      "Iteration 46475 | Loss: 0.262856\n",
      "Iteration 46500 | Loss: 0.262856\n",
      "Iteration 46525 | Loss: 0.262856\n",
      "Iteration 46550 | Loss: 0.262856\n",
      "Iteration 46575 | Loss: 0.262856\n",
      "Iteration 46600 | Loss: 0.262856\n",
      "Iteration 46625 | Loss: 0.262856\n",
      "Iteration 46650 | Loss: 0.262856\n",
      "Iteration 46675 | Loss: 0.262856\n",
      "Iteration 46700 | Loss: 0.262856\n",
      "Iteration 46725 | Loss: 0.262856\n",
      "Iteration 46750 | Loss: 0.262856\n",
      "Iteration 46775 | Loss: 0.262856\n",
      "Iteration 46800 | Loss: 0.262856\n",
      "Iteration 46825 | Loss: 0.262856\n",
      "Iteration 46850 | Loss: 0.262856\n",
      "Iteration 46875 | Loss: 0.262856\n",
      "Iteration 46900 | Loss: 0.262856\n",
      "Iteration 46925 | Loss: 0.262856\n",
      "Iteration 46950 | Loss: 0.262856\n",
      "Iteration 46975 | Loss: 0.262856\n",
      "Iteration 47000 | Loss: 0.262856\n",
      "Iteration 47025 | Loss: 0.262856\n",
      "Iteration 47050 | Loss: 0.262856\n",
      "Iteration 47075 | Loss: 0.262856\n",
      "Iteration 47100 | Loss: 0.262856\n",
      "Iteration 47125 | Loss: 0.262856\n",
      "Iteration 47150 | Loss: 0.262856\n",
      "Iteration 47175 | Loss: 0.262856\n",
      "Iteration 47200 | Loss: 0.262856\n",
      "Iteration 47225 | Loss: 0.262856\n",
      "Iteration 47250 | Loss: 0.262856\n",
      "Iteration 47275 | Loss: 0.262856\n",
      "Iteration 47300 | Loss: 0.262856\n",
      "Iteration 47325 | Loss: 0.262856\n",
      "Iteration 47350 | Loss: 0.262856\n",
      "Iteration 47375 | Loss: 0.262856\n",
      "Iteration 47400 | Loss: 0.262856\n",
      "Iteration 47425 | Loss: 0.262856\n",
      "Iteration 47450 | Loss: 0.262856\n",
      "Iteration 47475 | Loss: 0.262856\n",
      "Iteration 47500 | Loss: 0.262856\n",
      "Iteration 47525 | Loss: 0.262856\n",
      "Iteration 47550 | Loss: 0.262856\n",
      "Iteration 47575 | Loss: 0.262856\n",
      "Iteration 47600 | Loss: 0.262856\n",
      "Iteration 47625 | Loss: 0.262856\n",
      "Iteration 47650 | Loss: 0.262856\n",
      "Iteration 47675 | Loss: 0.262856\n",
      "Iteration 47700 | Loss: 0.262856\n",
      "Iteration 47725 | Loss: 0.262856\n",
      "Iteration 47750 | Loss: 0.262856\n",
      "Iteration 47775 | Loss: 0.262856\n",
      "Iteration 47800 | Loss: 0.262856\n",
      "Iteration 47825 | Loss: 0.262856\n",
      "Iteration 47850 | Loss: 0.262856\n",
      "Iteration 47875 | Loss: 0.262856\n",
      "Iteration 47900 | Loss: 0.262856\n",
      "Iteration 47925 | Loss: 0.262856\n",
      "Iteration 47950 | Loss: 0.262856\n",
      "Iteration 47975 | Loss: 0.262856\n",
      "Iteration 48000 | Loss: 0.262856\n",
      "Iteration 48025 | Loss: 0.262856\n",
      "Iteration 48050 | Loss: 0.262856\n",
      "Iteration 48075 | Loss: 0.262856\n",
      "Iteration 48100 | Loss: 0.262856\n",
      "Iteration 48125 | Loss: 0.262856\n",
      "Iteration 48150 | Loss: 0.262856\n",
      "Iteration 48175 | Loss: 0.262856\n",
      "Iteration 48200 | Loss: 0.262856\n",
      "Iteration 48225 | Loss: 0.262855\n",
      "Iteration 48250 | Loss: 0.262855\n",
      "Iteration 48275 | Loss: 0.262855\n",
      "Iteration 48300 | Loss: 0.262855\n",
      "Iteration 48325 | Loss: 0.262855\n",
      "Iteration 48350 | Loss: 0.262855\n",
      "Iteration 48375 | Loss: 0.262855\n",
      "Iteration 48400 | Loss: 0.262855\n",
      "Iteration 48425 | Loss: 0.262855\n",
      "Iteration 48450 | Loss: 0.262855\n",
      "Iteration 48475 | Loss: 0.262855\n",
      "Iteration 48500 | Loss: 0.262855\n",
      "Iteration 48525 | Loss: 0.262855\n",
      "Iteration 48550 | Loss: 0.262855\n",
      "Iteration 48575 | Loss: 0.262855\n",
      "Iteration 48600 | Loss: 0.262855\n",
      "Iteration 48625 | Loss: 0.262855\n",
      "Iteration 48650 | Loss: 0.262855\n",
      "Iteration 48675 | Loss: 0.262855\n",
      "Iteration 48700 | Loss: 0.262855\n",
      "Iteration 48725 | Loss: 0.262855\n",
      "Iteration 48750 | Loss: 0.262855\n",
      "Iteration 48775 | Loss: 0.262855\n",
      "Iteration 48800 | Loss: 0.262855\n",
      "Iteration 48825 | Loss: 0.262855\n",
      "Iteration 48850 | Loss: 0.262855\n",
      "Iteration 48875 | Loss: 0.262855\n",
      "Iteration 48900 | Loss: 0.262855\n",
      "Iteration 48925 | Loss: 0.262855\n",
      "Iteration 48950 | Loss: 0.262855\n",
      "Iteration 48975 | Loss: 0.262855\n",
      "Iteration 49000 | Loss: 0.262855\n",
      "Iteration 49025 | Loss: 0.262855\n",
      "Iteration 49050 | Loss: 0.262855\n",
      "Iteration 49075 | Loss: 0.262855\n",
      "Iteration 49100 | Loss: 0.262855\n",
      "Iteration 49125 | Loss: 0.262855\n",
      "Iteration 49150 | Loss: 0.262855\n",
      "Iteration 49175 | Loss: 0.262855\n",
      "Iteration 49200 | Loss: 0.262855\n",
      "Iteration 49225 | Loss: 0.262855\n",
      "Iteration 49250 | Loss: 0.262855\n",
      "Iteration 49275 | Loss: 0.262855\n",
      "Iteration 49300 | Loss: 0.262855\n",
      "Iteration 49325 | Loss: 0.262855\n",
      "Iteration 49350 | Loss: 0.262855\n",
      "Iteration 49375 | Loss: 0.262855\n",
      "Iteration 49400 | Loss: 0.262855\n",
      "Iteration 49425 | Loss: 0.262855\n",
      "Iteration 49450 | Loss: 0.262855\n",
      "Iteration 49475 | Loss: 0.262855\n",
      "Iteration 49500 | Loss: 0.262855\n",
      "Iteration 49525 | Loss: 0.262855\n",
      "Iteration 49550 | Loss: 0.262855\n",
      "Iteration 49575 | Loss: 0.262855\n",
      "Iteration 49600 | Loss: 0.262855\n",
      "Iteration 49625 | Loss: 0.262855\n",
      "Iteration 49650 | Loss: 0.262855\n",
      "Iteration 49675 | Loss: 0.262855\n",
      "Iteration 49700 | Loss: 0.262855\n",
      "Iteration 49725 | Loss: 0.262855\n",
      "Iteration 49750 | Loss: 0.262855\n",
      "Iteration 49775 | Loss: 0.262855\n",
      "Iteration 49800 | Loss: 0.262855\n",
      "Iteration 49825 | Loss: 0.262855\n",
      "Iteration 49850 | Loss: 0.262855\n",
      "Iteration 49875 | Loss: 0.262855\n",
      "Iteration 49900 | Loss: 0.262855\n",
      "Iteration 49925 | Loss: 0.262855\n",
      "Iteration 49950 | Loss: 0.262855\n",
      "Iteration 49975 | Loss: 0.262855\n",
      "Iteration 49999 | Loss: 0.262855\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y, \n",
    "    learning_rate= 0.0001, \n",
    "    n_iterations=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "True weights:    [ 2.  -3.5  1.5]\n",
      "Learned weights: [ 2.00857199 -3.51316039  1.48090083]\n",
      "\n",
      "True bias:    5.0\n",
      "Learned bias: 4.9908\n"
     ]
    }
   ],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAHWCAYAAACi1sL/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAATN1JREFUeJzt3Ql8FPX9//HPbk5yQggkQDjlFORGwAMop2gVlV/Fo0Wt9URbpdaKbS1Y6/kvnij+/KHW+6pgQUURFQSRU+RGkFPIwZWDhNzzf3y+ya4bCBBCsjO7+3q24+zOzu5+891hM+98j3FZlmUJAAAAAMBwV6wAAAAAAIQkAAAAADgKLUkAAAAA4IOQBAAAAAA+CEkAAAAA4IOQBAAAAAA+CEkAAAAA4IOQBAAAAAA+CEkAAAAA4IOQBABB4rrrrpM2bdrU6rmTJ08Wl8tV52VCcB4vABDsCEkAUM80fNRk+eqrr0L2ZD0uLs7uYgQFjjUAqBsuy7KsOnotAEA1Xn/99Sr3X331VZk3b5689tprVbaPGDFCUlJSal2HJSUlUl5eLlFRUaf83NLSUrNER0eLHSHp/fffl8OHD/v9vUP5WEtKSqr18QIAwY6QBAB+dvvtt8u0adPkZH+jKigokJiYGAl2hKRTl5+fL7GxsXV2rAEAqqK7HQA4wJAhQ6Rbt26ycuVKGTRokAlH9913n3nsww8/lIsuukiaN29u/up/xhlnyD/+8Q8pKys74RiTHTt2mO5X/+///T/53//9X/M8fX6/fv1k+fLlJx2TpPf1JHvWrFmmbPrcrl27yty5c48pv3YV7Nu3r2mJ0vd54YUX6nyc03vvvSd9+vSRBg0aSHJysvz617+WPXv2VNknIyNDrr/+eklLSzPlbdasmYwZM8bUhceKFStk1KhR5jX0tdq2bSu//e1va1SG5557ztSBvrZ+HhMmTJDs7Gzv41pf2nVQA+7RrrrqKklNTa3yuX3yySdy/vnnm8ATHx9vPuf169dX2x3xxx9/lAsvvNDsd80118jpOtHxosGqXbt25jgcOXKk7N692wQtPe60brXetF4PHjx4zOvW5GcCAKcLt7sAAIAKBw4ckNGjR8uVV15pAoCn690rr7xiTpInTpxo1l988YXcf//9kpubK48//vhJq+/NN9+UvLw8ufnmm81J8GOPPSaXX365bNu2TSIiIk743EWLFskHH3wgt912mznhffrpp2Xs2LGya9cuady4sdnnu+++kwsuuMAEkilTppgQ8MADD0iTJk3q7KPVOtDwowHv4YcflszMTHnqqadk8eLF5v0bNmxo9tOy6Qn5HXfcYQJAVlaW6W6m5fXc15N+Ldu9995rnqfhQH/Gk9HQpz/f8OHD5dZbb5XNmzfL888/bwKnlkPrcty4cSZgfPTRR/KrX/3K+1wNTbNnzzbBJCwszGzTLnDXXnutCWyPPvqo2Udf77zzzjM/k2+A0a6Qup8+piGmPlsY33jjDSkuLjZ1qCFIj5crrrhChg4dasLwn//8Z9m6das888wzcvfdd8tLL73kfe6p/EwA4Gg6JgkA4D8TJkzQvk9Vtg0ePNhsmz59+jH7FxQUHLPt5ptvtmJiYqzCwkLvtmuvvdZq3bq19/727dvNazZu3Ng6ePCgd/uHH35ots+ePdu77e9///sxZdL7kZGR1tatW73bvv/+e7P9mWee8W67+OKLTVn27Nnj3bZlyxYrPDz8mNesjpY7Njb2uI8XFxdbTZs2tbp162YdOXLEu33OnDnm9e+//35z/9ChQ+b+448/ftzXmjlzptln+fLl1qnIysoydTFy5EirrKzMu/3ZZ581r/fSSy+Z++Xl5VaLFi2ssWPHVnn+u+++a/ZbuHChuZ+Xl2c1bNjQuvHGG6vsl5GRYSUmJlbZrvWjz7333nutujjWTna8NGnSxMrOzvZunzRpktneo0cPq6SkxLv9qquuMnXiOQZP5WcCAKejux0AOIR24dLWkqNp1yYPbRHav3+/6c6kf6XftGnTSV9XWzcaNWrkva/PVdqSdDLaaqLd5zy6d+8uCQkJ3udqq9Hnn38ul156qel+5tG+fXvTKlYXtHuctgBpa5bvxBLajatz586m1cZTT5GRkaa149ChQ9W+lqfFac6cOWaii5rSn1FbV+68805xu3/+1XnjjTea+vCUQVvqtAXp448/rjIRxTvvvCMtWrQwLSpKW7e0m552wdPP07NoK1P//v3lyy+/PKYM2nrlD1r+xMRE730tj9LWzfDw8CrbtU48XR5r8zMBgFMRkgDAIfQkWk/yj6bdxy677DJz4qon5NpVTE9YVU5Ozklft1WrVlXuewLT8YLEiZ7reb7nuRpejhw5YkLR0arbVhs7d+40606dOh3zmIYkz+MaMrWLl46J0a6KOrZLu4rpOCWPwYMHmy552m1OxyTpuJqXX35ZioqKalUG/bx07I7ncU8o1Tr573//a+5rWNLQpOHDM0Zry5YtZq1d2PTz9F0+++wzU6++NJzoWCB/OPoz9wSmli1bVrvdcyyc6s8EAE7GmCQAcAjfFiMP/cu8nthrONJxPtqqo60pq1atMmNDdArnk/GMgTlaTWY8O53n2kFbei6++GIz2cSnn34qf/vb38wYJh3H1atXLxNSdLrxb7/91owR0n100oZ//etfZltdXK9pwIABZuzNu+++K1dffbV5Hw1NGp48PJ+bjuHRyRyO5tti4wmAvi1Y9el4n/nJjoVT/ZkAwMn4xgIAB9OuYzqhg04soC0jHtu3bxcnaNq0qQltOpD/aNVtq43WrVubtU6UoK0UvnSb53EPDZJ//OMfzaKtGz179jQhyPcaQhpkdPnnP/9pJrbQ2eLefvtt+d3vfnfSMmjLkYd2N9PPQrsl+tKJDnRiCZ1cQ7vaaWjS9/Mto6f+jn5uoArGnwlA6KK7HQA4mOev974tN3pirlNRO6V8ekKsLTd79+6tEpC021td0KnF9cR7+vTpVbrF6etv3LjRjE1SOkarsLDwmBN3nZXP8zztGnZ0K5iGKHWiLnf6M2rXOp3dz/f5M2bMMF0ePWXw0FYjfb1///vfZsp0DU2+dPY3bR186KGHqh0btW/fPgk0wfgzAQhdtCQBgIOdc845ZgyQTqv8+9//3nQX0+5MTuruplNj65iTc88910wuoJM5PPvss+baSqtXr67Ra+hJ9YMPPnjM9qSkJDNhg4410kkttOuhTgzgmQJcW2juuusus+8PP/wgw4YNM4HkzDPPNN27Zs6cafbVadWVhhYNmDrGSwOUToTx4osvmpN7vQbR8ei4mkmTJpmxTDrd+SWXXGJalfS1dFpyzxgxj969e5sxWX/5y19MWPLtaqf0/XRq7N/85jdmXy2fvodOVa6TQGhdah0GkmD8mQCELkISADiYXotIZ2LTrmN//etfTWDSE3INA/qXeyfQC7xqq45eM0fHAOkAfx0/pa08NZl9z9M6ps89mgYZDUl6fSG9NtAjjzxixmLphUo16Gh48sxYp++rAWr+/PkmSGpI0okddGyQTtagNGQtW7bMdK3T8KSTD5x99tnm2kB6UdmThUE96dcTfQ1mGuBuuukm03JS3fWmNBhpdz4NSxoajqbjlXRGQP2Z9HpXGqZ08g6dfbC6WQ4DQTD+TABCk0vnAbe7EACA4KPTguvMfJ5ZzwAACBSMSQIAnDadvc2XBiOd9nrIkCHULgAg4NCSBAA4bc2aNTNd4jzXDNKxKdrV6rvvvpMOHTpQwwCAgMKYJADAadPJDN566y1z4Va9ps/AgQPNWB0CEgAgENGSBAAAAAA+GJMEAAAAAD4ISQAAAAAQSmOSysvLzVXg9YrrehFGAAAAAKHJsixzIXG9ppvb7Q7dkKQBSS8wCAAAAABq9+7dkpaWJiEbkrQFyVMRCQkJtpalpKREPvvsMxk5cmS1V2cH9etkHL/UbyDj+KV+Ax3HMPUbyEocdA6cm5trGlA8GSFkQ5Kni50GJCeEpJiYGFMOuw+QYET9Ur+BjOOX+g1kHL/UcaDjGA69+nWdZBgOEzcAAAAAgA9CEgAAAAD4ICQBAAAAgA9CEgAAAAD4ICQBAAAAgA9CEgAAAAD4ICQBAAAAgA9CEgAAAAD4ICQBAAAAgA9CEgAAAAD4ICQBAAAAgA9CEgAAAAD4ICQBAAAAgA9Ckp9YliXXzFguD6wKk8zcQn+9LQAAAIBTREjyE5fLJTsPFMiBIpdk5RX5620BAAAAnCJCkh81TYgy66xcQhIAAADgVIQkP2oaXxmSDhOSAAAAAKciJPlRE09IoiUJAAAAcCxCkg0tSftoSQIAAAAci5DkRymVISmTliQAAADAsQhJdnS3Y3Y7AAAAwLEISXZ0tyMkAQAAAI5FSLIhJO3PL5bSsnJ/vjUAAACAGiIk+VFSbKS4xRLLEjmQX+zPtwYAAABQQ4QkPwpzuyQ+suJ2Zm6hP98aAAAAQA0RkvwsIaJizQx3AAAAgDMRkvwsMdIy66w8WpIAAAAAJyIk+Vmit7tdkb/fGgAAAEANEJL8LCGisiWJMUkAAACAIxGSbGpJ4oKyAAAAgDMRkvwsgdntAAAAAEcjJNk2cQNjkgAAAAAnIiTZNAX4/sNFUlpW7u+3BwAAAHAShCQ/i4uouKisZWlQKvb32wMAAAA4CUKSn7ldIslxFQOTuFYSAAAA4DyEJBukxEeZNddKAgAAAJyHkGSDJpUhiZYkAAAAwHkISTZoSksSAAAA4FiEJBtDUlZuoR1vDwAAAOAECEl2hiSulQQAAAA4DiHJBk0TPBM30JIEAAAAOA0hyQZN4mhJAgAAAJyKkGSDlMqWpP2Hi6SkrNyOIgAAAAA4DkKSDZJiIiUizCWWJbKPcUkAAACAoxCS7Kh0t0tSEqLN7fQcxiUBAAAATkJIskmzxIqQlEFIAgAAAByFkGSTn1uSjthVBAAAAADVICTZhJYkAAAAwJkISTZJTWxg1hlcKwkAAABwFEKSTWhJAgAAAJyJkGST1MqJG5jdDgAAAHAWQpLNLUmZuYVSXm7ZVQwAAAAARyEk2aRJXJS4XSKl5Zbszy+yqxgAAAAAjkJIskl4mFuaxEeZ21wrCQAAAHAOQpITZrjjgrIAAACAYzgmJD3yyCPicrnkzjvv9G4rLCyUCRMmSOPGjSUuLk7Gjh0rmZmZEiyaVV5QlmnAAQAAAOdwREhavny5vPDCC9K9e/cq2++66y6ZPXu2vPfee7JgwQLZu3evXH755RIsmOEOAAAAcB7bQ9Lhw4flmmuukRdffFEaNWrk3Z6TkyMzZsyQqVOnytChQ6VPnz7y8ssvyzfffCPffvutBFNIorsdAAAA4BzhdhdAu9NddNFFMnz4cHnwwQe921euXCklJSVmu0fnzp2lVatWsmTJEhkwYEC1r1dUVGQWj9zcXLPW19LFTp7396ybxEaY9d7sAtvLFgyOrl9Qv4GE45f6DWQcv9RxoOMYDp36LalhGWwNSW+//basWrXKdLc7WkZGhkRGRkrDhg2rbE9JSTGPHc/DDz8sU6ZMOWb7Z599JjExMeIE8+bNM+sdJr+Fy4/pB+Xjjz+2u1hBw1O/oH4DEccv9RvIOH6p40DHMRz89VtQUODskLR79275wx/+YCorOrqi21ldmDRpkkycOLFKS1LLli1l5MiRkpCQIHYnV/15R4wYIREREbLrYIE8s36RHC4Lk9GjR5qJK1B39Yu6Rf3WL+qX+g1kHL/UcaDjGA6d+s2t7GXm2JCk3emysrKkd+/e3m1lZWWycOFCefbZZ+XTTz+V4uJiyc7OrtKapLPbpaamHvd1o6KizHI0/UDs/lCOLkuLpDhzv7CkXApKRRrGOKN8gc5Jn3Uwon6p30DG8Uv9BjqOYeo3kEU44Bytpu9v28QNw4YNk7Vr18rq1au9S9++fc0kDp7b+kPMnz/f+5zNmzfLrl27ZODAgRIMoiPCpHFspLmdzrWSAAAAAEewrSUpPj5eunXrVmVbbGysuSaSZ/sNN9xgus4lJSWZrnJ33HGHCUjHm7QhEKUkRMuB/GIzw12XZvZ2BwQAAADggNntTuSJJ54Qt9ttLiKrM9aNGjVKnnvuOQkmzRKjZUN6Li1JAAAAgEM4KiR99dVXVe7rhA7Tpk0zS7D6+VpJR+wuCgAAAAAnXEw21GlLksrILbS7KAAAAAAISfZLTWxg1kzcAAAAADgDLUkOaUnam013OwAAAMAJCEk2a97w55Yky7LsLg4AAAAQ8ghJDmlJKiguk+yCEruLAwAAAIQ8QpIDLiibHFdxQdk9dLkDAAAAbEdIcoAWlV3uGJcEAAAA2I+Q5KBxSYQkAAAAwH6EJAeFJLrbAQAAAPYjJDmqux0XlAUAAADsRkhyAFqSAAAAAOcgJDkAEzcAAAAAzkFIcoAWjSq622XlFUlRaZndxQEAAABCGiHJARrFREh0RMVHkZHDuCQAAADAToQkB3C5XIxLAgAAAByCkOSwcUl7Dh2xuygAAABASCMkOQTTgAMAAADOQEhy2DTge7NpSQIAAADsREhyWkjKISQBAAAAdiIkOQRjkgAAAABnICQ5LSRlHxHLsuwuDgAAABCyCEkOkZoYLS6XSFFpuRzML7a7OAAAAEDIIiQ5RGS4W5rGR3lbkwAAAADYg5DkIMxwBwAAANiPkOTAkLQnu9DuogAAAAAhi5DkyAvK0t0OAAAAsAshyYEh6adDBXYXBQAAAAhZhCQHaZlUEZJ2H6QlCQAAALALIclBWjaKMevdtCQBAAAAtiEkOUhaZUjKKyyVnIISu4sDAAAAhCRCkoM0iAyT5LiKayXRmgQAAADYg5Dk2HFJTN4AAAAA2IGQ5DCMSwIAAADsRUhyGGa4AwAAAOxFSHIYWpIAAAAAexGSHKZlUuU04IxJAgAAAGxBSHKYtEYVEzf8dOiIWJZld3EAAACAkENIcpjmDRuI2yVSVFou+/KK7C4OAAAAEHIISQ4TEeaWZomV04AfYhpwAAAAwN8ISQ7vcgcAAADAvwhJDsTkDQAAAIB9CElOngb8IC1JAAAAgL8Rkpx8QVnGJAEAAAB+R0hycnc7QhIAAADgd4QkB3e325tdKKVl5XYXBwAAAAgphCQHahofJZHhbikrtyQ9p9Du4gAAAAAhhZDkQG63S9IaMi4JAAAAsAMhyaHSKscl/cQMdwAAAIBfEZIcqlXlDHe7DhbYXRQAAAAgpBCSHKpN41iz3nEg3+6iAAAAACGFkORQrStD0s4DtCQBAAAA/kRIcqg2jWO8LUmWZdldHAAAACBkEJIcfEFZl0skr7BUDhWU2F0cAAAAIGQQkhwqOiJMUhOize2djEsCAAAA/IaQ5GCtK7vcMS4JAAAA8B9CkoMxwx0AAADgf4QkB2OGOwAAAMD/CEkBMsMdAAAAAP8gJDkYLUkAAACA/xGSHKxVZUvSwfxiyS1kGnAAAADAHwhJDhYXFS7JcVHm9q4DBXYXBwAAAAgJhCSHY1wSAAAA4F+EJIdjXBIAAADgX4SkQGlJ2s8MdwAAAIA/EJIcrnVyrFnvPMiYJAAAAMAfCEkO1zqpoiVpJ9dKAgAAAII/JD3//PPSvXt3SUhIMMvAgQPlk08+8T5eWFgoEyZMkMaNG0tcXJyMHTtWMjMzJZS0aVzRkpSZWyQFxaV2FwcAAAAIeraGpLS0NHnkkUdk5cqVsmLFChk6dKiMGTNG1q9fbx6/6667ZPbs2fLee+/JggULZO/evXL55ZdLKEmMiZCGMRHm9i663AEAAAD1LlxsdPHFF1e5/89//tO0Ln377bcmQM2YMUPefPNNE57Uyy+/LF26dDGPDxgwQEJphrvsgmzZvi9fOqcm2F0cAAAAIKjZGpJ8lZWVmRaj/Px80+1OW5dKSkpk+PDh3n06d+4srVq1kiVLlhw3JBUVFZnFIzc316z1tXSxk+f9T7UcbZMayPe7s2VLZq4M75xcT6ULfLWtX1C/TsDxS/0GMo5f6jjQcQyHTv2W1LAMLsuyLLHR2rVrTSjS8Uc67khbji688EKzvv7666sEHnX22WfLL37xC3n00Uerfb3JkyfLlClTjtmurxcTUzEJQqD57CeXfLQ7TPo1KZdfty+3uzgAAABAQCooKJCrr75acnJyzJwIjm1J6tSpk6xevdoU9P3335drr73WjD+qrUmTJsnEiROrtCS1bNlSRo4cecKK8FdynTdvnowYMUIiIirGGdWEa12GfPTOGimOaiQXXti/XssYyGpbv6B+nYDjl/oNZBy/1HGg4xgOnfrNrexldjK2h6TIyEhp3769ud2nTx9Zvny5PPXUUzJu3DgpLi6W7OxsadiwoXd/nd0uNTX1uK8XFRVllqPpB2L3h1LbsnRslmjW2/fnS3h4uLhcrnosXeBz0mcdjKhf6jeQcfxSv4GOY5j6DWQRDjhHq+n7O+46SeXl5aaLnQYm/SHmz5/vfWzz5s2ya9cu0z0v1KYB11yUW1gqB/KL7S4OAAAAENRsbUnSrnGjR482kzHk5eWZcUNfffWVfPrpp5KYmCg33HCD6TqXlJRkusrdcccdJiCF0sx2KjoiTFo0bCA/HTpiWpOS445tKQMAAAAQBCEpKytLxo8fL+np6SYU6YVlNSBpf0X1xBNPiNvtNheR1dalUaNGyXPPPSehqF2TOBOStu07LP3aJNldHAAAACBo2RqS9DpIJxIdHS3Tpk0zS6hrlxwrC3/YJ9v25dtdFAAAACCoOW5MEqp3RpNYs/6RkAQAAADUK0JSAHW3U9v2H7a7KAAAAEBQIyQFiHaVLUm7DhRISRkXlAUAAADqCyEpQKQmREtMZJiUlluy+2CB3cUBAAAAghYhKUDoBWTbJle0JjF5AwAAAFB/CEkBhHFJAAAAQP0jJAXYNOCKliQAAACg/hCSAnDyBkISAAAAUH8ISQGkXTLTgAMAAAD1jZAUgC1J+w8XS05Bid3FAQAAAIISISmAxEaFS/PEaHN76748u4sDAAAABCVCUoBpnxJv1lsyD9tdFAAAACAoEZICTMemFeOSfiAkAQAAAPWCkBRgOqRUhKQtWXS3AwAAAOoDISnAtG9KdzsAAACgPhGSArQlKSO3UHILmeEOAAAAsD0kHTlyRAoKCrz3d+7cKU8++aR89tlndV02VCMhOkJSEypnuMti8gYAAADA9pA0ZswYefXVV83t7Oxs6d+/v/zrX/8y259//vk6LyBOMC4pk3FJAAAAgO0hadWqVXL++eeb2++//76kpKSY1iQNTk8//XSdFxDH6sC4JAAAAMA5IUm72sXHV0weoF3sLr/8cnG73TJgwAATluC/lqQf6G4HAAAA2B+S2rdvL7NmzZLdu3fLp59+KiNHjjTbs7KyJCEhoe5LiGN0rAxJW+luBwAAANgfku6//365++67pU2bNmY80sCBA72tSr169ar7EuIY7ZtUtOTtzSmUPGa4AwAAAOpU+Kk+4X/+53/kvPPOk/T0dOnRo4d3+7Bhw+Syyy6r29KhWokxEdI0Pkqy8orMDHe9WjWipgAAAAA7r5OUmppqWo10LFJubq7pfqfjlDp37lxX5cJJdEypvKgs45IAAAAAe0PSFVdcIc8++6z3mkl9+/Y127p37y7/+c9/6rZ0OK72TZkGHAAAAHBESFq4cKF3CvCZM2eKZVnmekk6/feDDz5YH2VENWhJAgAAABwSknJyciQpKcncnjt3rowdO1ZiYmLkoosuki1bttRHGXHCC8oepn4AAAAAO0NSy5YtZcmSJZKfn29CkmcK8EOHDkl0dHRdlg01aEnak31EcpnhDgAAALAvJN15551yzTXXSFpamjRv3lyGDBni7YZ31lln1V3JcEKJDSKkRcMG5vbmjDxqCwAAALBrCvDbbrtNzj77bHMx2REjRpgZ7lS7du0Yk+RnnVPjTUvSpvRc6demogskAAAAAD+HJKUz2umikzbo4nK5zJgk+FfnZvEyf1OWbEinJQkAAACw9TpJr776qula16BBA7Po9N+vvfZanRUKNdM5NcGsN2XkUmUAAACAXS1JU6dOlb/97W9y++23y7nnnmu2LVq0SG655RbZv3+/3HXXXXVVNpxEl2bx3jFJ5eWWuN0u6gwAAADwd0h65pln5Pnnn5fx48d7t11yySXStWtXmTx5MiHJj9o0jpXIcLcUFJfJ7kMF0rpxrD/fHgAAAAhKp9zdLj09Xc4555xjtus2fQz+Ex7mlo6V10vayLgkAAAAwJ6Q1L59e3n33XeP2f7OO+9Ihw4d6qZUqDHGJQEAAAA2d7ebMmWKjBs3zlwXyTMmafHixTJ//vxqwxPqfxpwtYmWJAAAAMCelqSxY8fK0qVLJTk5WWbNmmUWvb1s2TK57LLL6qZUqLEzmzHDHQAAAGD7dZL69Okjr7/+epVtWVlZ8tBDD8l9991XV2VDDXSqbEnaebBA8otKJTaqVh8pAAAAgNO5TlJ1dNIGnRoc/tU4LkqaxkeJZYn8kMlFZQEAAADHhCTYp7O3yx0hCQAAADhdhKQg0KWyy93G9Fy7iwIAAAAEPEJSEOjcrCIkbdhLSAIAAABOV41H+U+cOPGEj+/bt++0C4Pa6dY80aw3pOdKebklbreLqgQAAADqOyR99913J91n0KBBtS0HTkO7JnESHeGWguIy2X4gX85oEkd9AgAAAPUdkr788svavgfqWZjbZa6XtGpXtqzbk0NIAgAAAE4DY5KCRLcWFV3uNCQBAAAAqD1CUpCNS1q3h8kbAAAAgNNBSAoSXVtUXCtp3d4csfTKsgAAAABqhZAUJDo0jZfIMLfkFZbK7oNH7C4OAAAAELAISUEiMtztvV6StiYBAAAAqOeQ9Nhjj8mRIz+3UCxevFiKioq89/Py8uS2226rZTFQF7p6xyURkgAAAIB6D0mTJk0yQchj9OjRsmfPHu/9goICeeGFF2pdEJy+bpXjktYSkgAAAID6D0lHTwbA5ADOneFu/d5cPh8AAACglhiTFEQ6pcabC8sezC+W9JxCu4sDAAAABCRCUhCJjgiTDk3jzG3GJQEAAAC1E34qO//f//2fxMVVnISXlpbKK6+8IsnJyea+73gl2Kdbi0TZlJFnQtLIrql8FAAAAEB9haRWrVrJiy++6L2fmpoqr7322jH7wF7d0xLl/ZU/yeqfmOEOAAAAqNeQtGPHjlq9AfyrZ8uGZv397mwzeYPL5eIjAAAAAE4BY5KCTOfUBHNh2ZwjJbLjQIHdxQEAAACCNyQtWbJE5syZU2Xbq6++Km3btpWmTZvKTTfdVOXisrCHBqSuzRO8rUkAAAAA6ikkPfDAA7J+/Xrv/bVr18oNN9wgw4cPl3vvvVdmz54tDz/88Cm+Peqzy91qQhIAAABQfyFp9erVMmzYMO/9t99+W/r3728mc5g4caI8/fTT8u677556CVDnCEkAAACAH0LSoUOHJCUlxXt/wYIFMnr0aO/9fv36ye7du0+jKKjrkLRhb64UlZZRsQAAAEB9hCQNSNu3bze3i4uLZdWqVTJgwADv43qdpIiIiFN5b9STVkkx0igmQorLymVTOtevAgAAAOolJF144YVm7NHXX38tkyZNkpiYGDn//PO9j69Zs0bOOOOMU3pz1A+d9rsH45IAAACA+g1J//jHPyQ8PFwGDx5sxiHpEhkZ6X38pZdekpEjR57Sm+tED9pNLz4+3syQd+mll8rmzZur7FNYWCgTJkyQxo0bS1xcnIwdO1YyMzNP6X1C/XpJAAAAAOrhYrLJycmycOFCycnJMWElLCysyuPvvfee2X4qdFyTBiANSqWlpXLfffeZoLVhwwaJjY01+9x1113y0UcfmddPTEyU22+/XS6//HJZvHjxKb1XqKElCQAAAKjnkOShQaU6SUlJp/zmc+fOrXL/lVdeMS1KK1eulEGDBplANmPGDHnzzTdl6NChZp+XX35ZunTpIt9++22VMVGoqmdaRUvStv35klNQIokxjBcDAAAA6jQk/fa3v63RftrtrrY0FPkGLg1LJSUl5lpMHp07d5ZWrVqZi9tWF5L0gra+F7XNzc01a30dXezkeX9/lCMu0iWtkhrIroNHZOXO/XJ++2QJdv6s31BE/VK/gYzjl/oNdBzD1G8gK3HQOVpNy+CyLMuqyY5ut1tat24tvXr1khM9ZebMmVIb5eXlcskll0h2drYsWrTIbNMWpOuvv75K6FFnn322/OIXv5BHH330mNeZPHmyTJky5Zjt+lo62UQoeXWLW1bud8sFaeUyumW53cUBAAAAbFVQUCBXX321aZxJSEg4/ZakW2+9Vd566y0zDbgGl1//+te16mJ3PDo2ad26dd6AVFs6855e3Na3Jally5ZmrNOJKsJfyXXevHkyYsQIv0yXfmjZblk5e6PkRiXLhRf2lWDn7/oNNdQv9RvIOH6p30DHMUz9BrISB52jeXqZnUyNQ9K0adNk6tSp8sEHH5gudRpGLrroIrnhhhtMANFpp2tLJ2OYM2eOmRgiLS3Nuz01NdVck0lblxo2rBhjo3R2O32sOlFRUWY5mn4gdn8o/i5L/3YVXey+350jLneYhIfVeDLDgOakzzoYUb/UbyDj+KV+Ax3HMPUbyCIccI5W0/c/pbNmDR9XXXWVSYI6A13Xrl3ltttukzZt2sjhw4dPuZDabU8DknbR++KLL6Rt27ZVHu/Tp4/5QebPn+/dplOE79q1SwYOHHjK7xdqOqbES3x0uOQXl8mmDC4qCwAAANTL7Ha+Y5S09UiDTllZWa272OlYoQ8//NBcKykjI8M7g16DBg3MWluqtPucdu3T7nJ33HGHCUjMbHdyYW6X9G7VSBb8sE9W7Dgo3VpUPzMhAAAAgFq2JOkECjouSfsTduzYUdauXSvPPvusadk51Wskqeeff94MmhoyZIg0a9bMu7zzzjvefZ544gn55S9/aS4iq9OCazc77fKHmunbupFZr9h5iCoDAAAA6rIlSbvVvf3222YSBJ0OXMOSXmD2dNRkYr3o6GgzHkoXnLo+bSpD0o5Dpr5PZ+wYAAAAEApqHJKmT59urk/Url07WbBggVmqQyuPs/Rs2VDC3S7JyC2UPdlHJK1RaE2DDgAAANRbSBo/fjytEAEoJjJcujZPkO9/ypGVOw8RkgAAAIC6CkmvvPJKTXeFw/RpnWRC0vIdB2VMzxZ2FwcAAABwtNC4cE6I6+szLgkAAADAiRGSQmiGu82ZeZJbWGJ3cQAAAABHIySFgKYJ0dK6cYzoZIJ6vSQAAAAAx0dIChED2jY262+3EZIAAACAEyEkhYiBZ1SEpCU/HrC7KAAAAICjEZJCxIB2FSFp/d4cyTnCuCQAAADgeAhJISI1MVraJsdKuSWyfDtd7gAAAIDjISSFkAHtksz62210uQMAAACOh5AUgl3ulhCSAAAAgOMiJIWQgZUhaUN6ruQUMC4JAAAAqA4hKcSul9SuSay5XtLS7XS5AwAAAKpDSArRLndcLwkAAACoHiEpRLvcMXkDAAAAUD1CUoi2JOm4pAOHi+wuDgAAAOA4hKQQ0yQ+Sjqnxpvbi39kXBIAAABwNEJSCBrUsYlZf/3DPruLAgAAADgOISkEnd8h2awXbtknlk51BwAAAMCLkBSC+rVJkqhwt2TmFsmWrMN2FwcAAABwFEJSCIqOCJOz2yaZ2wvpcgcAAABUQUgKUYM945K27Le7KAAAAICjEJJC1PkdKkLS0u0HpLCkzO7iAAAAAI5BSApRHVPipGl8lBSWlMvKnYfsLg4AAADgGISkEOVyubytSTrLHQAAAIAKhKQQNqhjxVTgCzYTkgAAAAAPQlII05Ykt0tkU0ae7Mk+YndxAAAAAEcgJIWwpNhI6d2qkbn9xaYsu4sDAAAAOAIhKcQN7dLUrL/YmGl3UQAAAABHICSFuOFdUsx68Y8HpKC41O7iAAAAALYjJIW4Dk3jJK1RAykuLZdvth6wuzgAAACA7QhJIU6nAh/WuaLL3fxNdLkDAAAACEmQoZVd7uZvzBLLsqgRAAAAhDRCEmRAuySJiQyTrLwiWb83lxoBAABASCMkQaLCw+T8DhUXlv2cWe4AAAAQ4ghJMIZVdrn7dD3jkgAAABDaCEkwRnRJkTC3Szam58rOA/nUCgAAAEIWIQlGo9hIMzZJfbIug1oBAABAyCIkweuCbs3MmpAEAACAUEZIgteorinicol8vztb9mYfoWYAAAAQkghJ8GoaHy19Wzcyt+fS5Q4AAAAhipCEarvcEZIAAAAQqghJqOKCbqlmvXznQcnKK6R2AAAAEHIISaiiRcMG0iMtUSyLayYBAAAgNBGScIyLuld0uZu9ei+1AwAAgJBDSMIxLu7R3Mxyt2zHQdnDLHcAAAAIMYQkHKNZYgM5u03FhWVnf09rEgAAAEILIQnVGtOzhVnP+m4PNQQAAICQQkhCtS48K1UiwlyyKSNPNmfkUUsAAAAIGYQkVKthTKQM7tjU3P7v97QmAQAAIHQQknBcY3o2N+sPV+8VS+cEBwAAAEIAIQnHNbxLisREhslPh47Iyp2HqCkAAACEBEISjqtBZJiM7lZxzaT3VvxETQEAACAkEJJwQlf0TTPrOWv2Sn5RKbUFAACAoEdIwgmd3TZJ2jSOkfziMvl4bTq1BQAAgKBHSMIJuVwu+VXflub2uyt2U1sAAAAIeoQknNTY3mnidoks33FItu07TI0BAAAgqBGScFKpidEyuGMTc/u9lUzgAAAAgOBGSEKNXFHZ5e4/K3+S0rJyag0AAABBi5CEGhnWJUUax0ZKVl6RfL4xk1oDAABA0CIkoUYiw91yRb+K1qTXvt1JrQEAACBoEZJQY9f0b2UmcFi89YBszWICBwAAAAQnQhJqLK1RjAztnGJuv05rEgAAAIIUIQmnZPzA1t4JHPKLSqk9AAAABB1CEk7Jee2TpW1yrOQVlcqs1XuoPQAAAAQdW0PSwoUL5eKLL5bmzZuLy+WSWbNmVXncsiy5//77pVmzZtKgQQMZPny4bNmyxbbyQsTtdpmxSerVb3aazwgAAAAIJraGpPz8fOnRo4dMmzat2scfe+wxefrpp2X69OmydOlSiY2NlVGjRklhYaHfy4qf/apvS4mJDJPNmXmyaOt+qgYAAABBJdzONx89erRZqqMtFE8++aT89a9/lTFjxphtr776qqSkpJgWpyuvvLLa5xUVFZnFIzc316xLSkrMYifP+9tdjtMVEy7yqz4t5N9LdskLC36UAW0aihMES/06FfVL/QYyjl/qN9BxDFO/gazEQedoNS2Dy3JIfyntbjdz5ky59NJLzf1t27bJGWecId9995307NnTu9/gwYPN/aeeeqra15k8ebJMmTLlmO1vvvmmxMTE1ONPEFoOFIr847swscQl93QvlRaxdpcIAAAAOLGCggK5+uqrJScnRxISEpzZknQiGRkZZq0tR770vuex6kyaNEkmTpxYpSWpZcuWMnLkyBNWhL+S67x582TEiBESEREhgW5lyRr5aF2GbHG3lBsvPMvu4gRd/ToN9Uv9BjKOX+o30HEMU7+BrMRB52ieXmYn49iQVFtRUVFmOZp+IHZ/KE4sy+m4ecgZJiTNXpMh94zuIs0SG4gTBEv9OhX1S/0GMo5f6jfQcQxTv4EswgHnaDV9f8dOAZ6ammrWmZmZVbbrfc9jsFf3tIbSv22SlJZb8vLiHXwcAAAACAqODUlt27Y1YWj+/PlVmsd0lruBAwfaWjb87ObB7cz69W93ysH8YqoGAAAAAc/WkHT48GFZvXq1WdT27dvN7V27dpmJHO6880558MEH5b///a+sXbtWxo8fb66p5JncAfb7Raem0rV5ghQUl8mMRdvsLg4AAAAQ2CFpxYoV0qtXL7MonXBBb+sFZNU999wjd9xxh9x0003Sr18/E6rmzp0r0dHRdhYbPjTM/n5YB3P739/slOwCWpMAAAAQ2GyduGHIkCHmekgnOgF/4IEHzALnGtElRTqnxsumjDx5afEOmTiio91FAgAAAIJvTBICh9v9c2vSy4u3S84R+y8UBgAAANQWIQl14oKuqdIxJU7yCktlxqLt1CoAAAACFiEJdXMguV3yh2EV3exmfL1N9h8uomYBAAAQkAhJqDOju6VK97REyS8uk2e/2ErNAgAAICARklB3B5PbJX++oLO5/cbSnbLrQAG1CwAAgIBDSEKdOrd9spzfIVlKyiyZOm8ztQsAAICAQ0hCnfO0Js1avVfW7cmhhgEAABBQCEmoc91aJMolPZqb2w/M3nDCa2EBAAAATkNIQr24d3RniY5wy7IdB2XOmnRqGQAAAAGDkIR60bxhA7l1cHtz++GPN0pBcSk1DQAAgIBASEK9uXlwO2nRsIHszSmU6V/9SE0DAAAgIBCSUG+iI8Lkrxd1MbenL9wmOw/kU9sAAABwPEIS6tUF3VLlvPbJUlxaLvfNXMskDgAAAHA8QhLqlcvlkgcv7SZR4W5ZvPWA/GfVHmocAAAAjkZIQr1rkxwrdw7vaG4/+NEG2X+4iFoHAACAYxGS4Be/O7+tdGmWINkFJfKPORuodQAAADgWIQl+ERHmlkfHniVul8iHq/fK/I2Z1DwAAAAciZAEv+me1lBuOK+tuf3n/6yh2x0AAAAciZAEv/rjyE7SKSVe9h8ulnv/s4bZ7gAAAOA4hCT4/dpJT17ZUyLD3PL5xix5e/luPgEAAAA4CiEJfqcTONw9qmK2uwdmb5Dt+7nILAAAAJyDkARb/O68djKwXWM5UlImt72xSgpLyvgkAAAA4AiEJNhz4Lld8sS4ntI4NlI2pufK/R+u45MAAACAIxCSYJvUxGh5+qpeZlrwd1f8JO8yPgkAAAAOQEiCrc5tnywTR1SMT/rbh+tk3Z4cPhEAAADYipAE2902pL0M7dxUikrL5ebXVkpWXqHdRQIAAEAIIyTBGeOTrugp7ZJjZU/2Ebnp1ZVM5AAAAADbEJLgCIkxETLjun6S2CBCVu/Olrvf+54LzQIAAMAWhCQ4RtvkWJn+6z4SEeaSOWvSZeq8H+wuEgAAAEIQIQmOMvCMxvLPy84yt5/5Yqu8vHi73UUCAABAiCEkwXGu6NtS7hpeMePdlNkb5INVP9ldJAAAAIQQQhIc6ffD2sv157Yxt//0/hqZtyHT7iIBAAAgRBCS4Egul0v+dtGZcnnvFlJWbsmEN1bJ/I0EJQAAANQ/QhIcPTX4Y2O7y+huqVJcVi63vL5SPl2fYXexAAAAEOQISXC08DC3PH1VL7moezMpKatoUfp4bbrdxQIAAEAQIyTB8SLC3PLUuJ4ypmdzKS235PY3V8lby3bZXSwAAAAEKUISAqZFaeoVPeWKvmlSbolM+mCtTP1sMxecBQAAQJ0jJCFghLld8ujY7vL7oe3N/ae/2Cr3vL9GSsrK7S4aAAAAggghCQE3693EkZ3kocvOErdL5L2VP8l1Ly+Tg/nFdhcNAAAAQYKQhIB0df9W8uL4vhITGSaLtx6Qi59ZJOv35tpdLAAAAAQBQhIC1rAuKTLztnOldeMY2ZN9RMa9uExW7HPZXSwAAAAEOEISAlqn1Hj574TzZEinJlJUWi6vbQ2T+2atl4LiUruLBgAAgABFSELAS4yJkBnX9pMJQ9qJSyx5b+Ue+eXTi2Tdnhy7iwYAAIAAREhC0Mx8d+ew9jLhzHJJSYiSbfvz5bLnFsu0L7cy+x0AAABOCSEJQaVDoiWzJwyUUV1TpKTMksc/3Sxjnl1MqxIAAABqjJCEoNMoJlKm/7qP/OtXPaRhTIRsSM+VMdMWy8OfbJQjxWV2Fw8AAAAOR0hC0F5PaWyfNJl312D5ZfdmUlZuyQsLtsmwf30l//1+r1iWZXcRAQAA4FCEJAS1JvFR8uzVvc01lVo0bCB7cwrl9299J7+avkTW/JRtd/EAAADgQIQkhIQRZ6bI/D8OlokjOkqDiDBZsfOQ6YJ3+5urZGvWYbuLBwAAAAchJCFkREeEye+HdZAv7x4il/dqIdrjbs6adBn5xAK5653Vsn1/vt1FBAAAgAMQkhByUhOjZeq4nvLJH86XkWemSLklMvO7PTJ86gL5w9vfMRMeAABAiCMkIWR1aZYg/zu+r8y+/TwZ1rmpmdzhw9V75ZfPLJIr/3eJzN+YKeWaoAAAABBSwu0uAGC3s9ISZcZ1/UwL0otfbzNd8L7ddtAsbRrHyLh+reR/+qSZSSAAAAAQ/GhJAip1a5EoT13ZS76+5xdy06B2Eh8VLjsOFMijczfJwIfnyy2vrZSvNmdJaVk5dQYAABDEaEkCjtK8YQO578Iu8odhHWTOmr3y1rLdsnp3tsxdn2GW5LhIufCsZvLL7s2lb+tG4na7qEMAAIAgQkgCjiM2Ktx0tdNlU0auvL1st8xavUf2Hy6WV5fsNEtqQrRc1L2ZDO+SIn3bNJKIMBpnAQAAAh0hCaiBzqkJMvmSrvKXi7rIoq37Zc736fLZ+gzJyC2UGYu2myU+OlwGdWwiQzs1lSGdmkjjOMYwAQAABCJCEnAKtKXoF52amqWwpJss/GGfzF2XIV/9sE8O5hfLR2vSzeJyiXRtniAD2jaWAe0aS7+2SZLYIIK6BgAACACEJOA0Lk47smuqWXT68O9/ypYvN2XJ/I1ZsiE9V9btqVj+b9F2b2jq37ax9GzZ0CxpjRqISx8AAACAoxCSgDoQ5nZJ71aNzPLHkZ0kK7dQlmw7YKYRX7rtgGzbn+8NTR5JsZHSIy1Ruqc1lB4tE02XvmaJ0QQnAAAAmxGSgHrQNCFaxvRsYRaVmVso3247IMt3HJQ1P+XIxvRc0z3vy837zOKh45o6pcRLp9TKJSVeOqTES6OYCMITAACAnxCSAD9IOSo0FZWWycb0PPl+d7ZZ1u7JMa1NeYWlsmLnIbP4SogOlzbJsdK6cay0bRxj1m2SY6RlUowkx0YxDTkAAEAdIiQBNogKD/OOTfLQ4LRtX778kJknmzLy5IeMivWe7COSW1hqWqB0OVpkmFtSE6NNVz29xpOumzVsIM0To004axIfZbr2MT05AABAzRCSAAcFpy7NEswyxmd7YUmZ7DxQIDsO5MvOA/myfX+BWe/Yny/puYVSXFYuuw4WmOVEtMueTkuuF8PVdZO4KGkcW3FbZ97zXRIahEt8dIQZawUAABBqCElAAMyi5xmjdLSSsnIz3ik9p1D2Zh+Rvdl6++e1PqZjn8otkUMFJWbZmlWz99WJ9+KiwitCU7QuYZKf7ZYvCtZKXHSEudhuTGSYxEaGS0xU5Vrve7ZHhUuDiDCJinCbABgVrms3Y6sAAIDjBURImjZtmjz++OOSkZEhPXr0kGeeeUbOPvtsu4sF2E670KU1ijHL8ej05NkFxXIgv1j25xXJ/sr1gfwi2Z9XsT33SInkFpZIzpGKpaC4TCxLzBgpXUSOVL6aW9YeSj+tMkdWhiVvcDoqREVpsAp3m/0i3C4JD3NLRJhLwt1uCQ9zmZ9ZW7g8j5ltlY+Fe7bp45WPmee5XeY5Gvx0HebS2xXbtLHMbdae7RX7mPtuDYsV2/W+2/3zYxVLxX3f13Lp/yob4Fw+93WT7lexrrgNAACcyfEh6Z133pGJEyfK9OnTpX///vLkk0/KqFGjZPPmzdK0aVO7iwc4np68a5c6XTqmHNsaVZ3i0nJvaNIApeuDhwtl6crV0q5jF9HclF9cKvlFpSZQedfFpVJQVLmu3K7dATVw+b62Lnmi4QvKE6LECpM/Lp3nDVeV/z9u2Pr58aqPVT5UGdZ8H6sa4nwdL7KdSpg73q7Vv1/1O59u2Y5fWksOHw6Tp7Ys8r7Jqf7MrlP5mU9QkmBkWZbk5YXJtB+/4Q8A1HFA4hiu//pNdrnlQgkcjg9JU6dOlRtvvFGuv/56c1/D0kcffSQvvfSS3HvvvXYXDwhK2oqTbMYvRXm3lZSUSNhP38mF57aRiIiIU/piLCmzzMQURaXlFUvJcW7rPiU/39ZWMH1uaVm5lJRXrEvN2pLS8nLvY7pNux569y8vN/uUePYvt6RcF8sy+2hoK7Mq7ldsr2hx07JWbJcq+5v71TzXN/ydDn2dipdymfet3Fo3Lw4fLskqPPHYPZwOl6QfOUwV1ivqmPoNXO5ECSiODknFxcWycuVKmTRpkneb2+2W4cOHy5IlS6p9TlFRkVk8cnNzvSd4utjJ8/52lyNYUb/OrV/9m3p0mC5ukSi30796akxD1dEByxN4KgJU1fuWz3298fP2inpdsGChnD/ofAkPjzCvV+3zKl72523e16/Yv8pjRz2/8m2r/1mOE8qOv//x66Sm+0odvPaJ9//5dmlpqaxYsVz69u0nYeFhJ92/Jk61LoNZaWmZ+X3dp08fCT9O/cL/dVxXf8gJBfodsXLVKunTu7eEhwfH7yin1e+mNSsdcQ5c0zK4rOP91nGAvXv3SosWLeSbb76RgQMHerffc889smDBAlm6dOkxz5k8ebJMmTLlmO1vvvmmxMQcf9wGAAAAgOBWUFAgV199teTk5EhCQsJx9wu6qKytTjqGybclqWXLljJy5MgTVoS/kuu8efNkxIgRp9RdCdSvE3D8Ur+BjOOX+g10HMPUbyArcdA5sKeX2ck4OiQlJydLWFiYZGZmVtmu91NTU6t9TlRUlFmOph+I3R+KE8sSjKhf6jeQcfxSv4GM45c6DnQcw8FfvxE1fH8dHOBYkZGRpu/t/PnzvdvKy8vNfd/udwAAAABQVxzdkqS069y1114rffv2NddG0inA8/PzvbPdAQAAAEBIhaRx48bJvn375P777zcXk+3Zs6fMnTtXUlJS7C4aAAAAgCDk+JCkbr/9drMAAAAAQH1z9JgkAAAAAPA3QhIAAAAA+CAkAQAAAIAPQhIAAAAA+CAkAQAAAIAPQhIAAAAA+CAkAQAAAIAPQhIAAAAABNrFZE+HZVlmnZuba3dRpKSkRAoKCkxZIiIi7C5O0KF+qd9AxvFL/QYyjl/qONBxDIdO/eZWZgJPRgjZkJSXl2fWLVu2tLsoAAAAABySERITE4/7uMs6WYwKcOXl5bJ3716Jj48Xl8tle3LVsLZ7925JSEiwtSzBiPqlfgMZxy/1G8g4fqnjQMcxHDr1a1mWCUjNmzcXt9sdui1J+sOnpaWJk+jBYfcBEsyoX+o3kHH8Ur+BjOOXOg50HMOhUb+JJ2hB8mDiBgAAAADwQUgCAAAAAB+EJD+KioqSv//972YN6jfQcPxSv4GM45f6DXQcw9RvIIsKwHPgoJ+4AQAAAABOBS1JAAAAAOCDkAQAAAAAPghJAAAAAOCDkAQAAAAAPghJfjRt2jRp06aNREdHS//+/WXZsmX+fPuA9PDDD0u/fv0kPj5emjZtKpdeeqls3ry5yj5DhgwRl8tVZbnllluq7LNr1y656KKLJCYmxrzOn/70JyktLZVQN3ny5GPqrnPnzt7HCwsLZcKECdK4cWOJi4uTsWPHSmZmZpXXoG6PT/+9H12/umidKo7dU7Nw4UK5+OKLzVXStR5nzZpV5XGdh+j++++XZs2aSYMGDWT48OGyZcuWKvscPHhQrrnmGnMxw4YNG8oNN9wghw8frrLPmjVr5Pzzzzff1XqF+Mcee0xCvX5LSkrkz3/+s5x11lkSGxtr9hk/frzs3bv3pMf8I488UmWfUK3fmhzD11133TH1d8EFF1TZh2O49vVb3fexLo8//rh3H47h2p+PFdbROcNXX30lvXv3NjPhtW/fXl555RWxhc5uh/r39ttvW5GRkdZLL71krV+/3rrxxhuthg0bWpmZmVT/CYwaNcp6+eWXrXXr1lmrV6+2LrzwQqtVq1bW4cOHvfsMHjzY1Gd6erp3ycnJ8T5eWlpqdevWzRo+fLj13XffWR9//LGVnJxsTZo0KeTr/u9//7vVtWvXKnW3b98+b73ccsstVsuWLa358+dbK1assAYMGGCdc8451G0NZWVlVanbefPm6Wyi1pdffsmxWwv6b/cvf/mL9cEHH5h6nDlzZpXHH3nkESsxMdGaNWuW9f3331uXXHKJ1bZtW+vIkSPefS644AKrR48e1rfffmt9/fXXVvv27a2rrrrK+7h+d6SkpFjXXHON+d556623rAYNGlgvvPBCSNdvdna2+Q595513rE2bNllLliyxzj77bKtPnz5VXqN169bWAw88UOW49/2+DuX6rckxfO2115pj1Lf+Dh48WGUfjuHa169vveqi52Qul8v68ccfvftwDNf+fOyWOjhn2LZtmxUTE2NNnDjR2rBhg/XMM89YYWFh1ty5cy1/IyT5if4ymTBhgvd+WVmZ1bx5c+vhhx/2VxGC5qRTv/gWLFhQJST94Q9/OO5z9B+h2+22MjIyvNuef/55KyEhwSoqKrJCPSTpCWN19KQoIiLCeu+997zbNm7caOpfT5AUdXtq9Dg944wzrPLycnOfY7f2jj4B0jpNTU21Hn/88SrHcFRUlDkRV/oLV5+3fPly7z6ffPKJOUnas2ePuf/cc89ZjRo1qvLd8Oc//9nq1KmTFUqqO8E82rJly8x+O3furHKC+cQTTxz3OdTvietYQ9KYMWOOW38cw3V7DGtdDx06tMo2juHanY9l19E5wz333GP+eOtr3LhxJqT5G93t/KC4uFhWrlxpun54uN1uc3/JkiX+KELQyMnJMeukpKQq29944w1JTk6Wbt26yaRJk6SgoMD7mNaxdhFJSUnxbhs1apTk5ubK+vXrJdRpdyTtmtCuXTvTDUmbwpUes9rFxve41a54rVq18h631O2pfQ+8/vrr8tvf/tZ07/Dg2K0b27dvl4yMjCrHa2Jiouna7Hu8ahe7vn37evfR/fX7eOnSpd59Bg0aJJGRkVW+L7RbyaFDh+qotMHzfazHstapL+1ep91tevXqZbox+XaloX5PTrsaaTekTp06ya233ioHDhyoUn8cw3VDu4F99NFHpsvt0TiGT+7o87G6OmfQfXxfw7OPHefL4X5/xxC0f/9+KSsrq3JQKL2/adMm28oVaMrLy+XOO++Uc88914Qhj6uvvlpat25tTvS1r7v2m9cTmg8++MA8ridO1dW957FQpieQ2tdXfxmnp6fLlClTzFiBdevWmbrRE8WjT4C07jz1Rt3WnPaNz87ONmMOPDh2647nmKzu37rv8aonn77Cw8PNL3nffdq2bXvMa3gea9SoUR2WOnDp2AP9rr3qqqvM+C6P3//+92YsgdbpN998Y/5opd8tU6dONY9Tvyem448uv/xycwz++OOPct9998no0aPNCWJYWBjHcB3697//bcbXaH374hiu3flYRh2dMxxvHw1SR44cMeNN/YWQhIChgwH15H3RokVVtt90003e2/oXCh20PWzYMPML5owzzrChpIFDf/l6dO/e3YQmDZzvvvuuX7+IQsGMGTNMfWuY9+DYRSDSvxZfccUVZqKM559/vspjEydOrPKdoidNN998sxn0rYOwcWJXXnllld9nWof6e0xbl/T3GurOSy+9ZHpP6AQiHMN1cz4WbOhu5wfaDUz/AnT0DB96PzU11R9FCHi33367zJkzR7788ktJS0s74b56oq+2bt1q1lrH1dW95zH8TP8C1LFjR1N3WjfaRUxbP46uO0+9Ubc1s3PnTvn888/ld7/7HcduPfEckyf6ntV1VlZWlce1K5jOFsYxfWoBSY/pefPmVWlFOt73sdbxjh07vJ8B38c1p92g9RzC9/cZx/Dp+/rrr02Pk5N9JyuO4Zqdj9XVOcPx9tHvGn//8ZaQ5Af6l7Q+ffrI/PnzqzRV6v2BAwf6owgBS/9Sqf8gZ86cKV988cUx3WCqs3r1arPWFiWldbx27doqv1g8v9zPPPPMeix94NGpkLUFTutOj9mIiIgqx63+UtExS57jlrqtmZdfftl089JpT0+EY7f29LtBf7n6Hq/aPUPHGvker/oLXPvOe+j3in4fe/64ovvoNMIaBny/L7RLaqh3tfMEJB3HqKFfxx2djB7TOubL082R+j01P/30kxmT5Pv7jGO4blr29Xdcjx49Trovx3DNzsfq6pxB9/F9Dc8+tpwv+32qiBCeAlxnWXrllVfM7DQ33XSTmQLcd4YPHOvWW281U/p+9dVXVabtLCgoMI9v3brVTDerU01u377d+vDDD6127dpZgwYNOmbKyZEjR5ppK3UaySZNmjAFuGVZf/zjH03dat0tXrzYTMup03HqrDWe6Tx1is8vvvjC1PHAgQPNQt3WnM5kqXWoM6T54tg9dXl5eWbaWF3019fUqVPNbc/sajoFuH6v6vfAmjVrzMxV1U0B3qtXL2vp0qXWokWLrA4dOlSZAlxnaNIpqn/zm9+YqW71u1unow2FKapPVL/FxcVmSvW0tDTzPer7feyZleqbb74xM9vp4zql8uuvv26+a8ePH+99j1Cu35PVsT529913m5nA9Dv5888/t3r37m2O0cLCQu9rcAzXrn59p6HXY05nVTsax3Dtz8fq6pzBMwX4n/70JzM73rRp05gCPBToXO968Oj1knRKcL1OB05Mv+SqW3SufrVr1y4TiJKSkkwI1Wue6D8s3+skqR07dlijR4821+PQEKDhoKSkJOSrX6fVbNasmTkmW7RoYe7rybuHnlzedtttZkpk/dK67LLLzJcidVtzn376qTlmN2/eXGU7x+6p0+tLVfd9oNMme6YB/9vf/mZOwvX7YNiwYcfU+4EDB0woiouLM9POXn/99ebEypdeY+m8884zr6H/LjR8hXr96kn78b6PPdf9WrlypdW/f39zIhUdHW116dLFeuihh6qc4Idy/Z6sjvVkU08e9aRRp1LWqaj1GoBH/zGVY7h29euhgVzPBTSwH41juPbnY3V5zqCfY8+ePc25if7h2/c9/Mml//F/+xUAAAAAOBNjkgAAAADAByEJAAAAAHwQkgAAAADAByEJAAAAAHwQkgAAAADAByEJAAAAAHwQkgAAAADAByEJAAAAAHwQkgAAqNSmTRt58sknqQ8ACHGEJACALa677jq59NJLze0hQ4bInXfe6bf3fuWVV6Rhw4bHbF++fLncdNNNfisHAMCZwu0uAAAAdaW4uFgiIyNr/fwmTZrwYQAAaEkCANjforRgwQJ56qmnxOVymWXHjh3msXXr1sno0aMlLi5OUlJS5De/+Y3s37/f+1xtgbr99ttNK1RycrKMGjXKbJ86daqcddZZEhsbKy1btpTbbrtNDh8+bB776quv5Prrr5ecnBzv+02ePLna7na7du2SMWPGmPdPSEiQK664QjIzM72P6/N69uwpr732mnluYmKiXHnllZKXl+e3+gMA1D262wEAbKXhaODAgXLjjTdKenq6WTTYZGdny9ChQ6VXr16yYsUKmTt3rgkoGlR8/fvf/zatR4sXL5bp06ebbW63W55++mlZv369efyLL76Qe+65xzx2zjnnmCCkocfzfnffffcx5SovLzcB6eDBgybEzZs3T7Zt2ybjxo2rst+PP/4os2bNkjlz5phF933kkUfqtc4AAPWL7nYAAFtp64uGnJiYGElNTfVuf/bZZ01Aeuihh7zbXnrpJROgfvjhB+nYsaPZ1qFDB3nssceqvKbv+CZt4XnwwQfllltukeeee868l76ntiD5vt/R5s+fL2vXrpXt27eb91SvvvqqdO3a1Yxd6tevnzdM6Rin+Ph4c19bu/S5//znP+usjgAA/kVLEgDAkb7//nv58ssvTVc3z9K5c2dv641Hnz59jnnu559/LsOGDZMWLVqY8KLB5cCBA1JQUFDj99+4caMJR56ApM4880wz4YM+5hvCPAFJNWvWTLKysmr1MwMAnIGWJACAI+kYoosvvlgeffTRYx7TIOKh44586XimX/7yl3Lrrbea1pykpCRZtGiR3HDDDWZiB22xqksRERFV7msLlbYuAQACFyEJAGA77QJXVlZWZVvv3r3lP//5j2mpCQ+v+a+rlStXmpDyr3/9y4xNUu++++5J3+9oXbp0kd27d5vF05q0YcMGM1ZKW5QAAMGL7nYAANtpEFq6dKlpBdLZ6zTkTJgwwUyacNVVV5kxQNrF7tNPPzUz050o4LRv315KSkrkmWeeMRMt6MxzngkdfN9PW6p07JC+X3Xd8IYPH25myLvmmmtk1apVsmzZMhk/frwMHjxY+vbtWy/1AABwBkISAMB2OrtcWFiYaaHRaxXp1NvNmzc3M9ZpIBo5cqQJLDohg44J8rQQVadHjx5mCnDtptetWzd544035OGHH66yj85wpxM56Ex1+n5HT/zg6Tb34YcfSqNGjWTQoEEmNLVr107eeeedeqkDAIBzuCzLsuwuBAAAAAA4BS1JAAAAAOCDkAQAAAAAPghJAAAAAOCDkAQAAAAAPghJAAAAAOCDkAQAAAAAPghJAAAAAOCDkAQAAAAAPghJAAAAAOCDkAQAAAAAPghJAAAAACA/+//5p8Xpu6N+6QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges:\n",
      "  Feature 1: [-3241.27, 3852.73]\n",
      "  Feature 2: [-0.00, 0.00]\n",
      "  Feature 3: [-2.90, 2.60]\n"
     ]
    }
   ],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + 5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 85.696115\n",
      "Iteration   25 | Loss: 871155159202081570790882330567899404980854883804682973980431429497306482535225646649793004666526648360372005865209514740151916062986505086677560150619014463327877463090655543097643819610102059610109434140114141839360.000000\n",
      "Iteration   50 | Loss: inf\n",
      "Iteration   75 | Loss: nan\n",
      "Iteration   99 | Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkhyo\\AppData\\Local\\Temp\\ipykernel_16116\\1217784266.py:14: RuntimeWarning: overflow encountered in square\n",
      "  mse = np.mean(errors ** 2)\n",
      "C:\\Users\\pkhyo\\AppData\\Local\\Temp\\ipykernel_16116\\987864616.py:17: RuntimeWarning: overflow encountered in matmul\n",
      "  gradient_w = (2/n_samples) * (X.T @ errors)\n",
      "C:\\Users\\pkhyo\\AppData\\Local\\Temp\\ipykernel_16116\\987864616.py:17: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient_w = (2/n_samples) * (X.T @ errors)\n",
      "d:\\HS\\Python2\\HS-Python2\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\pkhyo\\AppData\\Local\\Temp\\ipykernel_16116\\3951635002.py:41: RuntimeWarning: invalid value encountered in subtract\n",
      "  w -= learning_rate * grad_w\n"
     ]
    }
   ],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_standardized = (X - mean) / std\n",
    "    return X_standardized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature ranges:\n",
      "  Feature 1: [-3.31, 3.92]\n",
      "  Feature 2: [-2.79, 2.66]\n",
      "  Feature 3: [-2.98, 2.47]\n",
      "Iteration    0 | Loss: 29.787905\n",
      "Iteration   25 | Loss: 0.240939\n",
      "Iteration   50 | Loss: 0.240479\n",
      "Iteration   75 | Loss: 0.240479\n",
      "Iteration  100 | Loss: 0.240479\n",
      "Iteration  125 | Loss: 0.240479\n",
      "Iteration  150 | Loss: 0.240479\n",
      "Iteration  175 | Loss: 0.240479\n",
      "Iteration  200 | Loss: 0.240479\n",
      "Iteration  225 | Loss: 0.240479\n",
      "Iteration  250 | Loss: 0.240479\n",
      "Iteration  275 | Loss: 0.240479\n",
      "Iteration  300 | Loss: 0.240479\n",
      "Iteration  325 | Loss: 0.240479\n",
      "Iteration  350 | Loss: 0.240479\n",
      "Iteration  375 | Loss: 0.240479\n",
      "Iteration  400 | Loss: 0.240479\n",
      "Iteration  425 | Loss: 0.240479\n",
      "Iteration  450 | Loss: 0.240479\n",
      "Iteration  475 | Loss: 0.240479\n",
      "Iteration  499 | Loss: 0.240479\n"
     ]
    }
   ],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.1, n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        batch_index = np.random.choice(n_samples, size= batch_size, replace= False)\n",
    "\n",
    "        X_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "\n",
    "        y_pred = predict(X_batch, w, b)\n",
    "        grad_w, grad_b = compute_gradients(X_batch, y_batch, y_pred)\n",
    "        \n",
    "        w = w - (learning_rate * grad_w)\n",
    "        b = b - (learning_rate * grad_b)\n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            y_true_pred = predict(X, w, b)\n",
    "            loss = compute_mse(y, y_true_pred)\n",
    "            loss_history.append(loss)\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.8f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 44.13951975\n",
      "Iteration   50 | Loss: 5.42962386\n",
      "Iteration  100 | Loss: 0.87961001\n",
      "Iteration  150 | Loss: 0.33301492\n",
      "Iteration  199 | Loss: 0.27349148\n"
     ]
    }
   ],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "    for t in range(n_iterations):\n",
    "        if schedule == 'step':\n",
    "            learning_rate = initial_lr * (0.9 ** (t // 100))\n",
    "        elif schedule == 'exponential':\n",
    "            learning_rate = initial_lr * np.exp(-decay_constant * t)\n",
    "        elif schedule == 'inverse':\n",
    "            learning_rate = initial_lr / (1.0 + decay_constant * t)\n",
    "        else:\n",
    "            learning_rate = None\n",
    "\n",
    "        y_pred = predict(X, w, b)\n",
    "\n",
    "        grad_w, grad_b = compute_gradients(X, y, y_pred)\n",
    "\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "\n",
    "        y_pred_full = predict(X, w, b)\n",
    "        loss = compute_mse(y, y_pred_full)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if t % 100 == 0 or t == n_iterations - 1:\n",
    "            print(\n",
    "                f\"Iter {t:4d} | lr={learning_rate:.6f} | loss={loss:.6f}\"\n",
    "            )\n",
    "\n",
    "    return w, b, loss_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step decay:\n",
      "Iter    0 | lr=0.010000 | loss=43.900117\n",
      "Iter  100 | lr=0.009000 | loss=0.900192\n",
      "Iter  200 | lr=0.008100 | loss=0.277439\n",
      "Iter  300 | lr=0.007290 | loss=0.263357\n",
      "Iter  400 | lr=0.006561 | loss=0.262880\n",
      "Iter  499 | lr=0.006561 | loss=0.262857\n",
      "Exponential decay:\n",
      "Iter    0 | lr=0.010000 | loss=44.083421\n",
      "Iter  100 | lr=0.009900 | loss=0.913127\n",
      "Iter  200 | lr=0.009802 | loss=0.273178\n",
      "Iter  300 | lr=0.009704 | loss=0.263033\n",
      "Iter  400 | lr=0.009608 | loss=0.262859\n",
      "Iter  499 | lr=0.009513 | loss=0.262855\n",
      "Inverse time decay:\n",
      "Iter    0 | lr=0.010000 | loss=43.865707\n",
      "Iter  100 | lr=0.009901 | loss=0.910340\n",
      "Iter  200 | lr=0.009804 | loss=0.273141\n",
      "Iter  300 | lr=0.009709 | loss=0.263033\n",
      "Iter  400 | lr=0.009615 | loss=0.262859\n",
      "Iter  499 | lr=0.009525 | loss=0.262855\n"
     ]
    }
   ],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mse = compute_mse(y_true, y_pred)\n",
    "    return mse + (reg_lambda * np.sum(w ** 2))\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    n_samples = X.shape[0]\n",
    "    errors = y_pred - y\n",
    "\n",
    "    grad_w_mse = (2.0 / n_samples) * (X.T @ errors)\n",
    "    grad_b = (2.0 / n_samples) * np.sum(errors)\n",
    "\n",
    "    grad_w = grad_w_mse + 2.0 * reg_lambda * w\n",
    "\n",
    "    return grad_w, grad_b\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        y_pred = predict(X, w, b)\n",
    "\n",
    "        grad_w, grad_b = calculate_ridge_gradients(X, y, y_pred, w, reg_lambda)\n",
    "\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "        loss = calculate_ridge_loss(y, predict(X, w, b), w, reg_lambda)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if i % 100 == 0 or i == n_iterations - 1:\n",
    "            print(f\"Iter {i:4d} | loss={loss:.6f}\")\n",
    "\n",
    "    return w, b, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter    0 | loss=43.845166\n",
      "Iter  100 | loss=2.485052\n",
      "Iter  200 | loss=1.958549\n",
      "Iter  300 | loss=1.951139\n",
      "Iter  400 | loss=1.951022\n",
      "Iter  499 | loss=1.951020\n"
     ]
    }
   ],
   "source": [
    "_, _, _ =train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.752853\n",
      "Iteration    1 | Loss: 43.864218\n",
      "Iteration    2 | Loss: 42.054061\n",
      "Iteration    3 | Loss: 40.319117\n",
      "Iteration    4 | Loss: 38.656260\n",
      "Iteration    5 | Loss: 37.062492\n",
      "Iteration    6 | Loss: 35.534939\n",
      "Iteration    7 | Loss: 34.070850\n",
      "Iteration    8 | Loss: 32.667586\n",
      "Iteration    9 | Loss: 31.322617\n",
      "Iteration   10 | Loss: 30.033520\n",
      "Iteration   11 | Loss: 28.797972\n",
      "Iteration   12 | Loss: 27.613747\n",
      "Iteration   13 | Loss: 26.478712\n",
      "Iteration   14 | Loss: 25.390822\n",
      "Iteration   15 | Loss: 24.348117\n",
      "Iteration   16 | Loss: 23.348718\n",
      "Iteration   17 | Loss: 22.390827\n",
      "Iteration   18 | Loss: 21.472717\n",
      "Iteration   19 | Loss: 20.592735\n",
      "Iteration   20 | Loss: 19.749296\n",
      "Iteration   21 | Loss: 18.940882\n",
      "Iteration   22 | Loss: 18.166036\n",
      "Iteration   23 | Loss: 17.423365\n",
      "Iteration   24 | Loss: 16.711531\n",
      "Iteration   25 | Loss: 16.029252\n",
      "Iteration   26 | Loss: 15.375300\n",
      "Iteration   27 | Loss: 14.748499\n",
      "Iteration   28 | Loss: 14.147719\n",
      "Iteration   29 | Loss: 13.571880\n",
      "Iteration   30 | Loss: 13.019946\n",
      "Iteration   31 | Loss: 12.490923\n",
      "Iteration   32 | Loss: 11.983860\n",
      "Iteration   33 | Loss: 11.497844\n",
      "Iteration   34 | Loss: 11.032000\n",
      "Iteration   35 | Loss: 10.585492\n",
      "Iteration   36 | Loss: 10.157515\n",
      "Iteration   37 | Loss: 9.747299\n",
      "Iteration   38 | Loss: 9.354108\n",
      "Iteration   39 | Loss: 8.977234\n",
      "Iteration   40 | Loss: 8.615998\n",
      "Iteration   41 | Loss: 8.269752\n",
      "Iteration   42 | Loss: 7.937874\n",
      "Iteration   43 | Loss: 7.619765\n",
      "Iteration   44 | Loss: 7.314855\n",
      "Iteration   45 | Loss: 7.022595\n",
      "Iteration   46 | Loss: 6.742461\n",
      "Iteration   47 | Loss: 6.473948\n",
      "Iteration   48 | Loss: 6.216573\n",
      "Iteration   49 | Loss: 5.969876\n",
      "Iteration   50 | Loss: 5.733411\n",
      "Iteration   51 | Loss: 5.506755\n",
      "Iteration   52 | Loss: 5.289499\n",
      "Iteration   53 | Loss: 5.081255\n",
      "Iteration   54 | Loss: 4.881647\n",
      "Iteration   55 | Loss: 4.690317\n",
      "Iteration   56 | Loss: 4.506922\n",
      "Iteration   57 | Loss: 4.331131\n",
      "Iteration   58 | Loss: 4.162631\n",
      "Iteration   59 | Loss: 4.001117\n",
      "Iteration   60 | Loss: 3.846300\n",
      "Iteration   61 | Loss: 3.697901\n",
      "Iteration   62 | Loss: 3.555656\n",
      "Iteration   63 | Loss: 3.419307\n",
      "Iteration   64 | Loss: 3.288611\n",
      "Iteration   65 | Loss: 3.163333\n",
      "Iteration   66 | Loss: 3.043248\n",
      "Iteration   67 | Loss: 2.928141\n",
      "Iteration   68 | Loss: 2.817804\n",
      "Iteration   69 | Loss: 2.712041\n",
      "Iteration   70 | Loss: 2.610661\n",
      "Iteration   71 | Loss: 2.513482\n",
      "Iteration   72 | Loss: 2.420331\n",
      "Iteration   73 | Loss: 2.331040\n",
      "Iteration   74 | Loss: 2.245448\n",
      "Iteration   75 | Loss: 2.163404\n",
      "Iteration   76 | Loss: 2.084758\n",
      "Iteration   77 | Loss: 2.009371\n",
      "Iteration   78 | Loss: 1.937108\n",
      "Iteration   79 | Loss: 1.867838\n",
      "Iteration   80 | Loss: 1.801437\n",
      "Iteration   81 | Loss: 1.737787\n",
      "Iteration   82 | Loss: 1.676774\n",
      "Iteration   83 | Loss: 1.618288\n",
      "Iteration   84 | Loss: 1.562224\n",
      "Iteration   85 | Loss: 1.508482\n",
      "Iteration   86 | Loss: 1.456965\n",
      "Iteration   87 | Loss: 1.407582\n",
      "Iteration   88 | Loss: 1.360244\n",
      "Iteration   89 | Loss: 1.314867\n",
      "Iteration   90 | Loss: 1.271368\n",
      "Iteration   91 | Loss: 1.229670\n",
      "Iteration   92 | Loss: 1.189698\n",
      "Iteration   93 | Loss: 1.151381\n",
      "Iteration   94 | Loss: 1.114651\n",
      "Iteration   95 | Loss: 1.079440\n",
      "Iteration   96 | Loss: 1.045688\n",
      "Iteration   97 | Loss: 1.013332\n",
      "Iteration   98 | Loss: 0.982315\n",
      "Iteration   99 | Loss: 0.952583\n",
      "Iteration  100 | Loss: 0.924080\n",
      "Iteration  101 | Loss: 0.896757\n",
      "Iteration  102 | Loss: 0.870565\n",
      "Iteration  103 | Loss: 0.845456\n",
      "Iteration  104 | Loss: 0.821387\n",
      "Iteration  105 | Loss: 0.798313\n",
      "Iteration  106 | Loss: 0.776194\n",
      "Iteration  107 | Loss: 0.754990\n",
      "Iteration  108 | Loss: 0.734662\n",
      "Iteration  109 | Loss: 0.715176\n",
      "Iteration  110 | Loss: 0.696496\n",
      "Iteration  111 | Loss: 0.678588\n",
      "Iteration  112 | Loss: 0.661421\n",
      "Iteration  113 | Loss: 0.644964\n",
      "Iteration  114 | Loss: 0.629188\n",
      "Iteration  115 | Loss: 0.614063\n",
      "Iteration  116 | Loss: 0.599565\n",
      "Iteration  117 | Loss: 0.585665\n",
      "Iteration  118 | Loss: 0.572340\n",
      "Iteration  119 | Loss: 0.559567\n",
      "Iteration  120 | Loss: 0.547321\n",
      "Iteration  121 | Loss: 0.535581\n",
      "Iteration  122 | Loss: 0.524327\n",
      "Iteration  123 | Loss: 0.513537\n",
      "Iteration  124 | Loss: 0.503194\n",
      "Iteration  125 | Loss: 0.493278\n",
      "Iteration  126 | Loss: 0.483772\n",
      "Iteration  127 | Loss: 0.474659\n",
      "Iteration  128 | Loss: 0.465922\n",
      "Iteration  129 | Loss: 0.457546\n",
      "Iteration  130 | Loss: 0.449516\n",
      "Iteration  131 | Loss: 0.441818\n",
      "Iteration  132 | Loss: 0.434438\n",
      "Iteration  133 | Loss: 0.427363\n",
      "Iteration  134 | Loss: 0.420580\n",
      "Iteration  135 | Loss: 0.414077\n",
      "Iteration  136 | Loss: 0.407843\n",
      "Iteration  137 | Loss: 0.401866\n",
      "Iteration  138 | Loss: 0.396136\n",
      "Iteration  139 | Loss: 0.390642\n",
      "Iteration  140 | Loss: 0.385376\n",
      "Iteration  141 | Loss: 0.380327\n",
      "Iteration  142 | Loss: 0.375486\n",
      "Iteration  143 | Loss: 0.370845\n",
      "Iteration  144 | Loss: 0.366395\n",
      "Iteration  145 | Loss: 0.362130\n",
      "Iteration  146 | Loss: 0.358040\n",
      "Iteration  147 | Loss: 0.354119\n",
      "Iteration  148 | Loss: 0.350360\n",
      "Iteration  149 | Loss: 0.346756\n",
      "Iteration  150 | Loss: 0.343301\n",
      "Iteration  151 | Loss: 0.339988\n",
      "Iteration  152 | Loss: 0.336812\n",
      "Iteration  153 | Loss: 0.333767\n",
      "Iteration  154 | Loss: 0.330847\n",
      "Iteration  155 | Loss: 0.328048\n",
      "Iteration  156 | Loss: 0.325365\n",
      "Iteration  157 | Loss: 0.322792\n",
      "Iteration  158 | Loss: 0.320325\n",
      "Iteration  159 | Loss: 0.317960\n",
      "Iteration  160 | Loss: 0.315692\n",
      "Iteration  161 | Loss: 0.313518\n",
      "Iteration  162 | Loss: 0.311434\n",
      "Iteration  163 | Loss: 0.309435\n",
      "Iteration  164 | Loss: 0.307519\n",
      "Iteration  165 | Loss: 0.305682\n",
      "Iteration  166 | Loss: 0.303920\n",
      "Iteration  167 | Loss: 0.302231\n",
      "Iteration  168 | Loss: 0.300612\n",
      "Iteration  169 | Loss: 0.299060\n",
      "Iteration  170 | Loss: 0.297571\n",
      "Iteration  171 | Loss: 0.296144\n",
      "Iteration  172 | Loss: 0.294775\n",
      "Iteration  173 | Loss: 0.293463\n",
      "Iteration  174 | Loss: 0.292205\n",
      "Iteration  175 | Loss: 0.290999\n",
      "Iteration  176 | Loss: 0.289842\n",
      "Iteration  177 | Loss: 0.288733\n",
      "Iteration  178 | Loss: 0.287670\n",
      "Iteration  179 | Loss: 0.286651\n",
      "Iteration  180 | Loss: 0.285673\n",
      "Iteration  181 | Loss: 0.284736\n",
      "Iteration  182 | Loss: 0.283837\n",
      "Iteration  183 | Loss: 0.282975\n",
      "Iteration  184 | Loss: 0.282149\n",
      "Iteration  185 | Loss: 0.281357\n",
      "Iteration  186 | Loss: 0.280597\n",
      "Iteration  187 | Loss: 0.279869\n",
      "Iteration  188 | Loss: 0.279170\n",
      "Iteration  189 | Loss: 0.278501\n",
      "Iteration  190 | Loss: 0.277859\n",
      "Iteration  191 | Loss: 0.277243\n",
      "Iteration  192 | Loss: 0.276652\n",
      "Iteration  193 | Loss: 0.276086\n",
      "Iteration  194 | Loss: 0.275543\n",
      "Iteration  195 | Loss: 0.275023\n",
      "Iteration  196 | Loss: 0.274524\n",
      "Iteration  197 | Loss: 0.274045\n",
      "Iteration  198 | Loss: 0.273586\n",
      "Iteration  199 | Loss: 0.273146\n",
      "Iteration  200 | Loss: 0.272724\n",
      "Iteration  201 | Loss: 0.272320\n",
      "Iteration  202 | Loss: 0.271932\n",
      "Iteration  203 | Loss: 0.271560\n",
      "Iteration  204 | Loss: 0.271203\n",
      "Iteration  205 | Loss: 0.270861\n",
      "Iteration  206 | Loss: 0.270533\n",
      "Iteration  207 | Loss: 0.270218\n",
      "Iteration  208 | Loss: 0.269916\n",
      "Iteration  209 | Loss: 0.269627\n",
      "Iteration  210 | Loss: 0.269350\n",
      "Iteration  211 | Loss: 0.269084\n",
      "Iteration  212 | Loss: 0.268829\n",
      "Iteration  213 | Loss: 0.268584\n",
      "Iteration  214 | Loss: 0.268350\n",
      "Iteration  215 | Loss: 0.268125\n",
      "Iteration  216 | Loss: 0.267909\n",
      "Iteration  217 | Loss: 0.267702\n",
      "Iteration  218 | Loss: 0.267504\n",
      "Iteration  219 | Loss: 0.267313\n",
      "Iteration  220 | Loss: 0.267131\n",
      "Iteration  221 | Loss: 0.266956\n",
      "Iteration  222 | Loss: 0.266788\n",
      "Iteration  223 | Loss: 0.266627\n",
      "Iteration  224 | Loss: 0.266473\n",
      "Iteration  225 | Loss: 0.266325\n",
      "Iteration  226 | Loss: 0.266183\n",
      "Iteration  227 | Loss: 0.266047\n",
      "Iteration  228 | Loss: 0.265917\n",
      "Iteration  229 | Loss: 0.265792\n",
      "Iteration  230 | Loss: 0.265672\n",
      "Iteration  231 | Loss: 0.265556\n",
      "Iteration  232 | Loss: 0.265446\n",
      "Iteration  233 | Loss: 0.265340\n",
      "Iteration  234 | Loss: 0.265239\n",
      "Iteration  235 | Loss: 0.265141\n",
      "Iteration  236 | Loss: 0.265048\n",
      "Iteration  237 | Loss: 0.264958\n",
      "Iteration  238 | Loss: 0.264872\n",
      "Iteration  239 | Loss: 0.264790\n",
      "Iteration  240 | Loss: 0.264711\n",
      "Iteration  241 | Loss: 0.264635\n",
      "Iteration  242 | Loss: 0.264563\n",
      "Iteration  243 | Loss: 0.264493\n",
      "Iteration  244 | Loss: 0.264426\n",
      "Iteration  245 | Loss: 0.264362\n",
      "Iteration  246 | Loss: 0.264301\n",
      "Iteration  247 | Loss: 0.264242\n",
      "Iteration  248 | Loss: 0.264185\n",
      "Iteration  249 | Loss: 0.264131\n",
      "Iteration    0 | Loss: 43.64602844\n",
      "Iteration    1 | Loss: 42.27928791\n",
      "Iteration    2 | Loss: 40.00832525\n",
      "Iteration    3 | Loss: 38.57494957\n",
      "Iteration    4 | Loss: 37.20319643\n",
      "Iteration    5 | Loss: 36.03542083\n",
      "Iteration    6 | Loss: 34.49524641\n",
      "Iteration    7 | Loss: 33.46329980\n",
      "Iteration    8 | Loss: 32.37929517\n",
      "Iteration    9 | Loss: 31.21544350\n",
      "Iteration   10 | Loss: 30.03876222\n",
      "Iteration   11 | Loss: 28.87275045\n",
      "Iteration   12 | Loss: 27.44382339\n",
      "Iteration   13 | Loss: 26.48423626\n",
      "Iteration   14 | Loss: 25.56555760\n",
      "Iteration   15 | Loss: 24.53177710\n",
      "Iteration   16 | Loss: 23.51638490\n",
      "Iteration   17 | Loss: 22.85897714\n",
      "Iteration   18 | Loss: 21.58895973\n",
      "Iteration   19 | Loss: 20.56095881\n",
      "Iteration   20 | Loss: 19.85617884\n",
      "Iteration   21 | Loss: 19.39380449\n",
      "Iteration   22 | Loss: 18.73716522\n",
      "Iteration   23 | Loss: 18.11878855\n",
      "Iteration   24 | Loss: 17.47955206\n",
      "Iteration   25 | Loss: 17.00362878\n",
      "Iteration   26 | Loss: 16.32641299\n",
      "Iteration   27 | Loss: 15.56320490\n",
      "Iteration   28 | Loss: 15.01734161\n",
      "Iteration   29 | Loss: 14.45148706\n",
      "Iteration   30 | Loss: 13.74212649\n",
      "Iteration   31 | Loss: 13.33223415\n",
      "Iteration   32 | Loss: 12.69144399\n",
      "Iteration   33 | Loss: 12.25292148\n",
      "Iteration   34 | Loss: 11.66055011\n",
      "Iteration   35 | Loss: 11.23739565\n",
      "Iteration   36 | Loss: 10.81563425\n",
      "Iteration   37 | Loss: 10.57180265\n",
      "Iteration   38 | Loss: 10.17583861\n",
      "Iteration   39 | Loss: 9.89277773\n",
      "Iteration   40 | Loss: 9.45181350\n",
      "Iteration   41 | Loss: 8.93557517\n",
      "Iteration   42 | Loss: 8.51927316\n",
      "Iteration   43 | Loss: 8.15674056\n",
      "Iteration   44 | Loss: 7.86791768\n",
      "Iteration   45 | Loss: 7.63670512\n",
      "Iteration   46 | Loss: 7.36928655\n",
      "Iteration   47 | Loss: 7.02505016\n",
      "Iteration   48 | Loss: 6.65128148\n",
      "Iteration   49 | Loss: 6.38862692\n",
      "Iteration   50 | Loss: 6.09471867\n",
      "Iteration   51 | Loss: 5.78298117\n",
      "Iteration   52 | Loss: 5.61184560\n",
      "Iteration   53 | Loss: 5.43830358\n",
      "Iteration   54 | Loss: 5.27202497\n",
      "Iteration   55 | Loss: 5.11632596\n",
      "Iteration   56 | Loss: 4.91711396\n",
      "Iteration   57 | Loss: 4.70737377\n",
      "Iteration   58 | Loss: 4.52932174\n",
      "Iteration   59 | Loss: 4.33649518\n",
      "Iteration   60 | Loss: 4.18133876\n",
      "Iteration   61 | Loss: 4.04383537\n",
      "Iteration   62 | Loss: 3.86242108\n",
      "Iteration   63 | Loss: 3.71534496\n",
      "Iteration   64 | Loss: 3.56850798\n",
      "Iteration   65 | Loss: 3.42046342\n",
      "Iteration   66 | Loss: 3.31958179\n",
      "Iteration   67 | Loss: 3.16846122\n",
      "Iteration   68 | Loss: 3.04168154\n",
      "Iteration   69 | Loss: 2.92002786\n",
      "Iteration   70 | Loss: 2.77423539\n",
      "Iteration   71 | Loss: 2.63997129\n",
      "Iteration   72 | Loss: 2.55710208\n",
      "Iteration   73 | Loss: 2.47767846\n",
      "Iteration   74 | Loss: 2.39411566\n",
      "Iteration   75 | Loss: 2.30689720\n",
      "Iteration   76 | Loss: 2.20390110\n",
      "Iteration   77 | Loss: 2.13299175\n",
      "Iteration   78 | Loss: 2.08447825\n",
      "Iteration   79 | Loss: 2.05320733\n",
      "Iteration   80 | Loss: 1.98099956\n",
      "Iteration   81 | Loss: 1.91021939\n",
      "Iteration   82 | Loss: 1.83650699\n",
      "Iteration   83 | Loss: 1.77340359\n",
      "Iteration   84 | Loss: 1.70613125\n",
      "Iteration   85 | Loss: 1.63971256\n",
      "Iteration   86 | Loss: 1.58923430\n",
      "Iteration   87 | Loss: 1.52554680\n",
      "Iteration   88 | Loss: 1.46557376\n",
      "Iteration   89 | Loss: 1.41343813\n",
      "Iteration   90 | Loss: 1.37225362\n",
      "Iteration   91 | Loss: 1.32470429\n",
      "Iteration   92 | Loss: 1.27974581\n",
      "Iteration   93 | Loss: 1.23434083\n",
      "Iteration   94 | Loss: 1.19229057\n",
      "Iteration   95 | Loss: 1.15681570\n",
      "Iteration   96 | Loss: 1.12104986\n",
      "Iteration   97 | Loss: 1.08667914\n",
      "Iteration   98 | Loss: 1.05002107\n",
      "Iteration   99 | Loss: 1.01484220\n",
      "Iteration  100 | Loss: 0.98614307\n",
      "Iteration  101 | Loss: 0.95613368\n",
      "Iteration  102 | Loss: 0.91632617\n",
      "Iteration  103 | Loss: 0.90097029\n",
      "Iteration  104 | Loss: 0.86776056\n",
      "Iteration  105 | Loss: 0.84197739\n",
      "Iteration  106 | Loss: 0.82384651\n",
      "Iteration  107 | Loss: 0.78901274\n",
      "Iteration  108 | Loss: 0.76526763\n",
      "Iteration  109 | Loss: 0.74432943\n",
      "Iteration  110 | Loss: 0.72063160\n",
      "Iteration  111 | Loss: 0.70282908\n",
      "Iteration  112 | Loss: 0.68960082\n",
      "Iteration  113 | Loss: 0.67199733\n",
      "Iteration  114 | Loss: 0.66095397\n",
      "Iteration  115 | Loss: 0.63713335\n",
      "Iteration  116 | Loss: 0.62112670\n",
      "Iteration  117 | Loss: 0.60256643\n",
      "Iteration  118 | Loss: 0.59188642\n",
      "Iteration  119 | Loss: 0.58058746\n",
      "Iteration  120 | Loss: 0.56238941\n",
      "Iteration  121 | Loss: 0.54387321\n",
      "Iteration  122 | Loss: 0.53123229\n",
      "Iteration  123 | Loss: 0.52263625\n",
      "Iteration  124 | Loss: 0.51375775\n",
      "Iteration  125 | Loss: 0.49979570\n",
      "Iteration  126 | Loss: 0.49370542\n",
      "Iteration  127 | Loss: 0.48450113\n",
      "Iteration  128 | Loss: 0.47066090\n",
      "Iteration  129 | Loss: 0.46164811\n",
      "Iteration  130 | Loss: 0.45484242\n",
      "Iteration  131 | Loss: 0.44733319\n",
      "Iteration  132 | Loss: 0.44099647\n",
      "Iteration  133 | Loss: 0.43343045\n",
      "Iteration  134 | Loss: 0.42931884\n",
      "Iteration  135 | Loss: 0.42682686\n",
      "Iteration  136 | Loss: 0.42154333\n",
      "Iteration  137 | Loss: 0.41494128\n",
      "Iteration  138 | Loss: 0.40850267\n",
      "Iteration  139 | Loss: 0.40336195\n",
      "Iteration  140 | Loss: 0.39834852\n",
      "Iteration  141 | Loss: 0.39236692\n",
      "Iteration  142 | Loss: 0.38786017\n",
      "Iteration  143 | Loss: 0.37844138\n",
      "Iteration  144 | Loss: 0.37000980\n",
      "Iteration  145 | Loss: 0.36593751\n",
      "Iteration  146 | Loss: 0.36362370\n",
      "Iteration  147 | Loss: 0.36003210\n",
      "Iteration  148 | Loss: 0.35460794\n",
      "Iteration  149 | Loss: 0.35103629\n",
      "Iteration  150 | Loss: 0.34944030\n",
      "Iteration  151 | Loss: 0.34597978\n",
      "Iteration  152 | Loss: 0.34238014\n",
      "Iteration  153 | Loss: 0.33957446\n",
      "Iteration  154 | Loss: 0.33650656\n",
      "Iteration  155 | Loss: 0.33099285\n",
      "Iteration  156 | Loss: 0.32791780\n",
      "Iteration  157 | Loss: 0.32499528\n",
      "Iteration  158 | Loss: 0.32369804\n",
      "Iteration  159 | Loss: 0.32393197\n",
      "Iteration  160 | Loss: 0.32183078\n",
      "Iteration  161 | Loss: 0.31941717\n",
      "Iteration  162 | Loss: 0.31727994\n",
      "Iteration  163 | Loss: 0.31574925\n",
      "Iteration  164 | Loss: 0.31408572\n",
      "Iteration  165 | Loss: 0.31143683\n",
      "Iteration  166 | Loss: 0.30898808\n",
      "Iteration  167 | Loss: 0.30681601\n",
      "Iteration  168 | Loss: 0.30579531\n",
      "Iteration  169 | Loss: 0.30337196\n",
      "Iteration  170 | Loss: 0.30231040\n",
      "Iteration  171 | Loss: 0.30103770\n",
      "Iteration  172 | Loss: 0.29928158\n",
      "Iteration  173 | Loss: 0.29804038\n",
      "Iteration  174 | Loss: 0.29631440\n",
      "Iteration  175 | Loss: 0.29557981\n",
      "Iteration  176 | Loss: 0.29355055\n",
      "Iteration  177 | Loss: 0.29280084\n",
      "Iteration  178 | Loss: 0.29107493\n",
      "Iteration  179 | Loss: 0.29079834\n",
      "Iteration  180 | Loss: 0.28988292\n",
      "Iteration  181 | Loss: 0.28798144\n",
      "Iteration  182 | Loss: 0.28744323\n",
      "Iteration  183 | Loss: 0.28645919\n",
      "Iteration  184 | Loss: 0.28577241\n",
      "Iteration  185 | Loss: 0.28449772\n",
      "Iteration  186 | Loss: 0.28293529\n",
      "Iteration  187 | Loss: 0.28260407\n",
      "Iteration  188 | Loss: 0.28120257\n",
      "Iteration  189 | Loss: 0.28079286\n",
      "Iteration  190 | Loss: 0.28036246\n",
      "Iteration  191 | Loss: 0.27895657\n",
      "Iteration  192 | Loss: 0.27871632\n",
      "Iteration  193 | Loss: 0.27729708\n",
      "Iteration  194 | Loss: 0.27661330\n",
      "Iteration  195 | Loss: 0.27636582\n",
      "Iteration  196 | Loss: 0.27532614\n",
      "Iteration  197 | Loss: 0.27482203\n",
      "Iteration  198 | Loss: 0.27404489\n",
      "Iteration  199 | Loss: 0.27334475\n",
      "Iteration  200 | Loss: 0.27274241\n",
      "Iteration  201 | Loss: 0.27194018\n",
      "Iteration  202 | Loss: 0.27088704\n",
      "Iteration  203 | Loss: 0.27072551\n",
      "Iteration  204 | Loss: 0.27019629\n",
      "Iteration  205 | Loss: 0.26987052\n",
      "Iteration  206 | Loss: 0.26943642\n",
      "Iteration  207 | Loss: 0.26899922\n",
      "Iteration  208 | Loss: 0.26879404\n",
      "Iteration  209 | Loss: 0.26856470\n",
      "Iteration  210 | Loss: 0.26852319\n",
      "Iteration  211 | Loss: 0.26878013\n",
      "Iteration  212 | Loss: 0.26814889\n",
      "Iteration  213 | Loss: 0.26788304\n",
      "Iteration  214 | Loss: 0.26777101\n",
      "Iteration  215 | Loss: 0.26787997\n",
      "Iteration  216 | Loss: 0.26758272\n",
      "Iteration  217 | Loss: 0.26750082\n",
      "Iteration  218 | Loss: 0.26692478\n",
      "Iteration  219 | Loss: 0.26648813\n",
      "Iteration  220 | Loss: 0.26636835\n",
      "Iteration  221 | Loss: 0.26634848\n",
      "Iteration  222 | Loss: 0.26617417\n",
      "Iteration  223 | Loss: 0.26595567\n",
      "Iteration  224 | Loss: 0.26592187\n",
      "Iteration  225 | Loss: 0.26572136\n",
      "Iteration  226 | Loss: 0.26572613\n",
      "Iteration  227 | Loss: 0.26562716\n",
      "Iteration  228 | Loss: 0.26538537\n",
      "Iteration  229 | Loss: 0.26541856\n",
      "Iteration  230 | Loss: 0.26552721\n",
      "Iteration  231 | Loss: 0.26552333\n",
      "Iteration  232 | Loss: 0.26582268\n",
      "Iteration  233 | Loss: 0.26529965\n",
      "Iteration  234 | Loss: 0.26524693\n",
      "Iteration  235 | Loss: 0.26481379\n",
      "Iteration  236 | Loss: 0.26475681\n",
      "Iteration  237 | Loss: 0.26454696\n",
      "Iteration  238 | Loss: 0.26437451\n",
      "Iteration  239 | Loss: 0.26457279\n",
      "Iteration  240 | Loss: 0.26445471\n",
      "Iteration  241 | Loss: 0.26439394\n",
      "Iteration  242 | Loss: 0.26420945\n",
      "Iteration  243 | Loss: 0.26430241\n",
      "Iteration  244 | Loss: 0.26432073\n",
      "Iteration  245 | Loss: 0.26411060\n",
      "Iteration  246 | Loss: 0.26422742\n",
      "Iteration  247 | Loss: 0.26417644\n",
      "Iteration  248 | Loss: 0.26387540\n",
      "Iteration  249 | Loss: 0.26370038\n",
      "Iteration    0 | Loss: 45.63464161\n",
      "Iteration    1 | Loss: 45.37982901\n",
      "Iteration    2 | Loss: 44.35523199\n",
      "Iteration    3 | Loss: 42.93473153\n",
      "Iteration    4 | Loss: 40.66839400\n",
      "Iteration    5 | Loss: 39.77067890\n",
      "Iteration    6 | Loss: 39.32066122\n",
      "Iteration    7 | Loss: 39.31669601\n",
      "Iteration    8 | Loss: 37.96584621\n",
      "Iteration    9 | Loss: 30.57597347\n",
      "Iteration   10 | Loss: 27.62058673\n",
      "Iteration   11 | Loss: 27.57425097\n",
      "Iteration   12 | Loss: 26.69173374\n",
      "Iteration   13 | Loss: 26.69327772\n",
      "Iteration   14 | Loss: 26.42728547\n",
      "Iteration   15 | Loss: 26.18291368\n",
      "Iteration   16 | Loss: 26.04946509\n",
      "Iteration   17 | Loss: 25.97876392\n",
      "Iteration   18 | Loss: 22.62398973\n",
      "Iteration   19 | Loss: 21.76556217\n",
      "Iteration   20 | Loss: 20.25544995\n",
      "Iteration   21 | Loss: 20.24433607\n",
      "Iteration   22 | Loss: 16.41404162\n",
      "Iteration   23 | Loss: 15.41739473\n",
      "Iteration   24 | Loss: 15.25637478\n",
      "Iteration   25 | Loss: 14.15433902\n",
      "Iteration   26 | Loss: 14.01939803\n",
      "Iteration   27 | Loss: 13.00251571\n",
      "Iteration   28 | Loss: 12.62411098\n",
      "Iteration   29 | Loss: 12.35546422\n",
      "Iteration   30 | Loss: 9.67852932\n",
      "Iteration   31 | Loss: 9.43019065\n",
      "Iteration   32 | Loss: 9.27090948\n",
      "Iteration   33 | Loss: 9.26222599\n",
      "Iteration   34 | Loss: 8.77075491\n",
      "Iteration   35 | Loss: 8.45656345\n",
      "Iteration   36 | Loss: 8.26050885\n",
      "Iteration   37 | Loss: 8.25596767\n",
      "Iteration   38 | Loss: 7.64329692\n",
      "Iteration   39 | Loss: 7.34411726\n",
      "Iteration   40 | Loss: 7.03777538\n",
      "Iteration   41 | Loss: 6.87859251\n",
      "Iteration   42 | Loss: 6.64286970\n",
      "Iteration   43 | Loss: 6.14544522\n",
      "Iteration   44 | Loss: 6.09160438\n",
      "Iteration   45 | Loss: 6.00849372\n",
      "Iteration   46 | Loss: 6.01101370\n",
      "Iteration   47 | Loss: 5.24632870\n",
      "Iteration   48 | Loss: 5.02751047\n",
      "Iteration   49 | Loss: 5.01119856\n",
      "Iteration   50 | Loss: 4.47798428\n",
      "Iteration   51 | Loss: 4.07729857\n",
      "Iteration   52 | Loss: 4.06526147\n",
      "Iteration   53 | Loss: 4.05986110\n",
      "Iteration   54 | Loss: 4.02992467\n",
      "Iteration   55 | Loss: 3.71970108\n",
      "Iteration   56 | Loss: 3.70298446\n",
      "Iteration   57 | Loss: 3.53311997\n",
      "Iteration   58 | Loss: 3.50012680\n",
      "Iteration   59 | Loss: 3.28210025\n",
      "Iteration   60 | Loss: 3.27804693\n",
      "Iteration   61 | Loss: 2.90938248\n",
      "Iteration   62 | Loss: 2.71327595\n",
      "Iteration   63 | Loss: 2.60111453\n",
      "Iteration   64 | Loss: 2.47348873\n",
      "Iteration   65 | Loss: 2.37252420\n",
      "Iteration   66 | Loss: 2.29114372\n",
      "Iteration   67 | Loss: 2.27683811\n",
      "Iteration   68 | Loss: 2.07132533\n",
      "Iteration   69 | Loss: 1.99960553\n",
      "Iteration   70 | Loss: 1.98760293\n",
      "Iteration   71 | Loss: 1.97616919\n",
      "Iteration   72 | Loss: 1.98507513\n",
      "Iteration   73 | Loss: 1.91762773\n",
      "Iteration   74 | Loss: 1.82994840\n",
      "Iteration   75 | Loss: 1.65658418\n",
      "Iteration   76 | Loss: 1.61425713\n",
      "Iteration   77 | Loss: 1.59984232\n",
      "Iteration   78 | Loss: 1.57920184\n",
      "Iteration   79 | Loss: 1.56302138\n",
      "Iteration   80 | Loss: 1.50709908\n",
      "Iteration   81 | Loss: 1.50048442\n",
      "Iteration   82 | Loss: 1.47604479\n",
      "Iteration   83 | Loss: 1.42706528\n",
      "Iteration   84 | Loss: 1.40778669\n",
      "Iteration   85 | Loss: 1.40434898\n",
      "Iteration   86 | Loss: 1.39245704\n",
      "Iteration   87 | Loss: 1.37861407\n",
      "Iteration   88 | Loss: 1.25584280\n",
      "Iteration   89 | Loss: 1.20943042\n",
      "Iteration   90 | Loss: 1.20104679\n",
      "Iteration   91 | Loss: 1.17075083\n",
      "Iteration   92 | Loss: 1.16502996\n",
      "Iteration   93 | Loss: 1.03807033\n",
      "Iteration   94 | Loss: 1.03088523\n",
      "Iteration   95 | Loss: 1.00553952\n",
      "Iteration   96 | Loss: 1.00228516\n",
      "Iteration   97 | Loss: 0.98938185\n",
      "Iteration   98 | Loss: 0.98741219\n",
      "Iteration   99 | Loss: 0.97062223\n",
      "Iteration  100 | Loss: 0.96522367\n",
      "Iteration  101 | Loss: 0.89723186\n",
      "Iteration  102 | Loss: 0.87167711\n",
      "Iteration  103 | Loss: 0.88161593\n",
      "Iteration  104 | Loss: 0.88349431\n",
      "Iteration  105 | Loss: 0.88433212\n",
      "Iteration  106 | Loss: 0.84830339\n",
      "Iteration  107 | Loss: 0.83799420\n",
      "Iteration  108 | Loss: 0.80087423\n",
      "Iteration  109 | Loss: 0.80046745\n",
      "Iteration  110 | Loss: 0.79918292\n",
      "Iteration  111 | Loss: 0.79562684\n",
      "Iteration  112 | Loss: 0.78226303\n",
      "Iteration  113 | Loss: 0.78232100\n",
      "Iteration  114 | Loss: 0.66001315\n",
      "Iteration  115 | Loss: 0.62973706\n",
      "Iteration  116 | Loss: 0.60704753\n",
      "Iteration  117 | Loss: 0.60757470\n",
      "Iteration  118 | Loss: 0.62526765\n",
      "Iteration  119 | Loss: 0.58906579\n",
      "Iteration  120 | Loss: 0.59187902\n",
      "Iteration  121 | Loss: 0.57084536\n",
      "Iteration  122 | Loss: 0.53322446\n",
      "Iteration  123 | Loss: 0.52496532\n",
      "Iteration  124 | Loss: 0.50694659\n",
      "Iteration  125 | Loss: 0.50666537\n",
      "Iteration  126 | Loss: 0.49421306\n",
      "Iteration  127 | Loss: 0.45915399\n",
      "Iteration  128 | Loss: 0.45279743\n",
      "Iteration  129 | Loss: 0.43665291\n",
      "Iteration  130 | Loss: 0.43757833\n",
      "Iteration  131 | Loss: 0.43874575\n",
      "Iteration  132 | Loss: 0.42145300\n",
      "Iteration  133 | Loss: 0.41126258\n",
      "Iteration  134 | Loss: 0.41074772\n",
      "Iteration  135 | Loss: 0.40984904\n",
      "Iteration  136 | Loss: 0.38907194\n",
      "Iteration  137 | Loss: 0.38782135\n",
      "Iteration  138 | Loss: 0.38667840\n",
      "Iteration  139 | Loss: 0.37560494\n",
      "Iteration  140 | Loss: 0.37740008\n",
      "Iteration  141 | Loss: 0.37422528\n",
      "Iteration  142 | Loss: 0.37188710\n",
      "Iteration  143 | Loss: 0.36949482\n",
      "Iteration  144 | Loss: 0.37177953\n",
      "Iteration  145 | Loss: 0.37294875\n",
      "Iteration  146 | Loss: 0.37190031\n",
      "Iteration  147 | Loss: 0.37188797\n",
      "Iteration  148 | Loss: 0.36533048\n",
      "Iteration  149 | Loss: 0.36592850\n",
      "Iteration  150 | Loss: 0.36615031\n",
      "Iteration  151 | Loss: 0.36026135\n",
      "Iteration  152 | Loss: 0.35584039\n",
      "Iteration  153 | Loss: 0.35598088\n",
      "Iteration  154 | Loss: 0.35692997\n",
      "Iteration  155 | Loss: 0.35384529\n",
      "Iteration  156 | Loss: 0.34469594\n",
      "Iteration  157 | Loss: 0.34676804\n",
      "Iteration  158 | Loss: 0.34691879\n",
      "Iteration  159 | Loss: 0.35238218\n",
      "Iteration  160 | Loss: 0.35267898\n",
      "Iteration  161 | Loss: 0.35141870\n",
      "Iteration  162 | Loss: 0.35155315\n",
      "Iteration  163 | Loss: 0.34432766\n",
      "Iteration  164 | Loss: 0.34432079\n",
      "Iteration  165 | Loss: 0.34359015\n",
      "Iteration  166 | Loss: 0.34258391\n",
      "Iteration  167 | Loss: 0.34235751\n",
      "Iteration  168 | Loss: 0.34506886\n",
      "Iteration  169 | Loss: 0.34643340\n",
      "Iteration  170 | Loss: 0.33464139\n",
      "Iteration  171 | Loss: 0.34292751\n",
      "Iteration  172 | Loss: 0.33744715\n",
      "Iteration  173 | Loss: 0.33945673\n",
      "Iteration  174 | Loss: 0.33546578\n",
      "Iteration  175 | Loss: 0.33536267\n",
      "Iteration  176 | Loss: 0.33510594\n",
      "Iteration  177 | Loss: 0.33524719\n",
      "Iteration  178 | Loss: 0.34475706\n",
      "Iteration  179 | Loss: 0.34782062\n",
      "Iteration  180 | Loss: 0.34351064\n",
      "Iteration  181 | Loss: 0.33841672\n",
      "Iteration  182 | Loss: 0.31917559\n",
      "Iteration  183 | Loss: 0.29926019\n",
      "Iteration  184 | Loss: 0.29614042\n",
      "Iteration  185 | Loss: 0.30197773\n",
      "Iteration  186 | Loss: 0.30967257\n",
      "Iteration  187 | Loss: 0.30866099\n",
      "Iteration  188 | Loss: 0.30627468\n",
      "Iteration  189 | Loss: 0.30419407\n",
      "Iteration  190 | Loss: 0.30501637\n",
      "Iteration  191 | Loss: 0.30453431\n",
      "Iteration  192 | Loss: 0.30277268\n",
      "Iteration  193 | Loss: 0.29620378\n",
      "Iteration  194 | Loss: 0.29061055\n",
      "Iteration  195 | Loss: 0.28883249\n",
      "Iteration  196 | Loss: 0.28539322\n",
      "Iteration  197 | Loss: 0.28579331\n",
      "Iteration  198 | Loss: 0.28537724\n",
      "Iteration  199 | Loss: 0.28539405\n",
      "Iteration  200 | Loss: 0.28254568\n",
      "Iteration  201 | Loss: 0.28262449\n",
      "Iteration  202 | Loss: 0.28079246\n",
      "Iteration  203 | Loss: 0.28287908\n",
      "Iteration  204 | Loss: 0.27761481\n",
      "Iteration  205 | Loss: 0.28311357\n",
      "Iteration  206 | Loss: 0.28401104\n",
      "Iteration  207 | Loss: 0.28314664\n",
      "Iteration  208 | Loss: 0.28494668\n",
      "Iteration  209 | Loss: 0.28557599\n",
      "Iteration  210 | Loss: 0.28497169\n",
      "Iteration  211 | Loss: 0.28795885\n",
      "Iteration  212 | Loss: 0.29229072\n",
      "Iteration  213 | Loss: 0.29734607\n",
      "Iteration  214 | Loss: 0.29843955\n",
      "Iteration  215 | Loss: 0.29248331\n",
      "Iteration  216 | Loss: 0.29241111\n",
      "Iteration  217 | Loss: 0.29200730\n",
      "Iteration  218 | Loss: 0.28978244\n",
      "Iteration  219 | Loss: 0.29318156\n",
      "Iteration  220 | Loss: 0.29331686\n",
      "Iteration  221 | Loss: 0.29218944\n",
      "Iteration  222 | Loss: 0.28805161\n",
      "Iteration  223 | Loss: 0.28673843\n",
      "Iteration  224 | Loss: 0.28658681\n",
      "Iteration  225 | Loss: 0.28644973\n",
      "Iteration  226 | Loss: 0.28786742\n",
      "Iteration  227 | Loss: 0.28895067\n",
      "Iteration  228 | Loss: 0.28826417\n",
      "Iteration  229 | Loss: 0.28770286\n",
      "Iteration  230 | Loss: 0.28830139\n",
      "Iteration  231 | Loss: 0.28762296\n",
      "Iteration  232 | Loss: 0.28816076\n",
      "Iteration  233 | Loss: 0.28759169\n",
      "Iteration  234 | Loss: 0.28494853\n",
      "Iteration  235 | Loss: 0.27888122\n",
      "Iteration  236 | Loss: 0.27798410\n",
      "Iteration  237 | Loss: 0.27902303\n",
      "Iteration  238 | Loss: 0.27873512\n",
      "Iteration  239 | Loss: 0.27213259\n",
      "Iteration  240 | Loss: 0.27304624\n",
      "Iteration  241 | Loss: 0.27462903\n",
      "Iteration  242 | Loss: 0.27513897\n",
      "Iteration  243 | Loss: 0.27492696\n",
      "Iteration  244 | Loss: 0.27443636\n",
      "Iteration  245 | Loss: 0.27484383\n",
      "Iteration  246 | Loss: 0.27477767\n",
      "Iteration  247 | Loss: 0.27399296\n",
      "Iteration  248 | Loss: 0.27431627\n",
      "Iteration  249 | Loss: 0.27573710\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_sgd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 250,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 1,\n",
    ") -> tuple:\n",
    "    return train_minibatch_gd(\n",
    "        X, y,\n",
    "        batch_size=1,\n",
    "        learning_rate=learning_rate,\n",
    "        n_iterations=n_iterations,\n",
    "        verbose=verbose,\n",
    "        log_every_n_step=log_every_n_step,\n",
    "    )\n",
    "\n",
    "\n",
    "iteration = 250\n",
    "lr = 0.01\n",
    "\n",
    "# Batch GD\n",
    "np.random.seed(0)\n",
    "_, _, loss_batch_gd = train_linear_regression(\n",
    "    X, y,\n",
    "    lr,\n",
    "    iteration,\n",
    "    True,\n",
    "    1,\n",
    ")\n",
    "\n",
    "# Mini-batch GD\n",
    "np.random.seed(0)\n",
    "_, _, loss_batch_mini_batch_gd = train_minibatch_gd(\n",
    "    X, y,\n",
    "    32,\n",
    "    lr,\n",
    "    iteration,\n",
    "    True,\n",
    "    1,\n",
    ")\n",
    "\n",
    "# SGD\n",
    "np.random.seed(0)\n",
    "_, _, loss_batch_sgd = train_sgd(\n",
    "    X, y,\n",
    "    lr,\n",
    "    iteration,\n",
    "    True,\n",
    "    1,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(loss_batch_gd, label='Batch GD')\n",
    "plt.plot(loss_batch_mini_batch_gd, label='Mini-batch GD (32)')\n",
    "plt.plot(loss_batch_sgd, label='SGD (batch_size=1)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss (log scale)')\n",
    "plt.title('Training Loss over Time: Batch GD vs Mini-batch GD vs SGD')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
