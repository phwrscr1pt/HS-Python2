{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[-1.52533979 -0.34456174 -0.26328669]\n",
      " [-0.3866013   1.39683547  0.02544633]\n",
      " [-0.91252505  0.31617937 -3.55318463]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "random = np.random.randn(3, 3) # 3x3 matrix of random values (normal distribution)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [ 7.44888617e+00  8.51439109e+00  4.76819250e+00  7.00848738e+00\n",
      "  9.59302865e+00  7.89170486e+00  1.03254880e+01  2.17497640e+00\n",
      "  1.79651512e+00  7.41609083e+00 -2.70663271e+00 -1.82559453e+00\n",
      "  1.02842936e+01  3.06614186e+00  3.60478338e+00  6.75823062e+00\n",
      "  1.23440031e+01  7.51657687e+00  2.54369234e+00  4.68547093e+00\n",
      "  3.03195548e+00  1.79810642e+00  1.88606865e+00  4.75193072e+00\n",
      " -4.47752253e+00  5.89062924e+00  1.18105049e+01 -2.35809341e-01\n",
      "  6.51226562e+00  8.28156408e+00  7.50817991e-01  3.52178182e+00\n",
      "  4.68621729e+00  8.85365535e+00  6.88061284e+00 -5.31682221e-01\n",
      "  2.89750466e+00  8.43102968e+00  3.50779472e+00 -2.09336377e-01\n",
      "  1.18691115e+01  3.42798148e+00  5.15044645e+00  9.52321516e+00\n",
      " -1.61317286e-01  1.03581631e+01  1.12762494e+01  1.35174276e+01\n",
      "  9.27959471e-01  9.77768304e-01  3.26837989e+00  3.36722661e+00\n",
      "  5.28617855e+00  1.09051200e+01  1.16346435e+01  5.79354077e+00\n",
      "  5.81302870e+00  4.14996338e+00  2.67774306e+00  1.29136208e+01\n",
      "  7.64504740e+00  7.81806532e+00  4.93118587e+00  4.81770607e+00\n",
      "  1.00480446e+01  9.09472345e+00  9.65349452e+00  3.91159278e+00\n",
      "  6.29960180e+00  1.00040022e+01  3.59830393e+00  8.54467869e+00\n",
      "  3.55516937e+00 -5.73845449e+00  1.13091281e+01  5.33630396e+00\n",
      "  9.52505126e-02  4.29601522e+00  4.03145772e+00  8.96781126e+00\n",
      "  4.57401699e+00  1.04307960e+01  8.98494305e+00  1.15998502e+01\n",
      "  3.35162903e+00 -1.46416137e+00  1.78602396e+00  1.46888035e+01\n",
      "  1.13107218e+01  3.87598381e+00  1.46528097e+01  9.14890161e+00\n",
      "  7.60358159e+00  4.82525811e+00  1.57039379e+01  2.50954804e+00\n",
      "  7.42924868e+00  7.35117793e+00  9.48861437e+00  3.71402489e+00\n",
      "  6.42358403e+00  6.46988718e+00  1.04464757e+01  7.00566418e+00\n",
      "  5.76176718e+00  2.18977215e+00  6.24932734e+00  1.21926674e+01\n",
      "  8.97479169e+00  5.34142429e+00  8.22960359e+00  1.21714378e-02\n",
      "  5.09325323e+00  7.11585310e+00  6.55767680e+00  8.42119941e+00\n",
      "  4.77653794e+00  4.70899596e+00  8.44305066e+00  2.05232135e+00\n",
      "  5.10966610e-01  2.78608877e+00  5.55062571e+00  4.11519226e+00\n",
      "  7.77461301e+00  6.87195654e-01  1.09490671e+01  1.04462172e+01\n",
      "  3.46799039e+00  4.56050276e+00 -4.98896254e-01  3.83111363e+00\n",
      " -3.33663214e+00  1.21570669e+01  4.17095711e+00  9.76880214e+00\n",
      "  4.50747766e+00  1.00366306e+01  1.06110589e+01  1.20508685e+00\n",
      "  2.60435668e+00  6.73727341e+00  2.25660970e+00  2.56507755e+00\n",
      "  4.20332401e+00 -2.62211704e+00  2.36375024e+00  1.20369778e+01\n",
      "  5.97104447e-01  3.32995436e+00  5.30546142e-02  6.67468545e+00\n",
      "  7.57881345e+00 -4.21939842e+00  2.68312722e+00  3.54473892e+00\n",
      "  5.54178400e+00  7.74963779e+00  1.00263970e+01 -3.47422015e+00\n",
      "  5.67888688e+00  5.92077432e+00  1.00229847e+01  9.01109997e+00\n",
      "  1.04539174e+01  9.42267361e+00  9.47782887e+00  1.45582736e+01\n",
      "  4.51506282e+00  1.85576759e+00  5.35832496e+00  6.33665917e+00\n",
      "  9.57227619e+00  1.31607330e+01  5.32768407e-01  7.15876175e+00\n",
      "  9.64244188e+00  9.17148711e-02  2.23981239e+00 -1.56473725e-01\n",
      "  1.14200645e+01  1.53385501e+01  7.42152192e+00  3.25544952e+00\n",
      "  6.95149516e+00  5.23403057e+00  7.69701225e+00  5.61282984e+00\n",
      "  5.77350485e+00  9.00432888e+00 -3.40302774e-01 -1.65782356e+00\n",
      "  5.55321569e-01  5.47983879e+00 -1.40906189e+00  5.82943777e+00\n",
      "  9.76431438e+00  2.86874887e+00  8.45532488e+00  5.16344972e+00\n",
      "  1.10459647e+01  9.07944728e+00  5.14063269e+00  5.61843182e+00\n",
      "  1.31677769e+01  5.96421052e+00  3.99271270e+00  1.37398105e+00\n",
      "  8.49122089e+00  6.19467624e+00 -3.97536723e+00  5.20631817e+00\n",
      "  4.45478693e+00  6.08862119e+00  9.37320304e+00  1.38122138e+01\n",
      "  4.87484281e+00  6.53445161e+00  9.96034903e+00  4.04191255e+00\n",
      "  5.71755238e+00  6.56775604e+00 -4.75038461e+00 -2.88369819e-01\n",
      "  3.50340030e+00  9.95978775e+00  3.50342205e+00  5.31177923e+00\n",
      " -3.00238221e-01  2.42839346e+00 -1.40118163e+00  3.69169715e+00\n",
      "  1.14172041e+01  8.03324014e+00  6.18598340e+00  1.33257139e+01\n",
      " -4.78492990e+00  7.31249475e+00  1.23451548e+00  6.48782783e+00\n",
      "  9.57891478e+00  5.61252415e+00  3.01069238e+00  1.29037299e+00\n",
      "  1.33192816e+01  6.52312737e+00  2.01934231e+00  4.66684766e+00\n",
      "  8.85557153e+00  3.51955480e+00  6.42061975e+00  1.43787528e+01\n",
      "  4.05217270e+00  8.59836687e+00  1.21802887e+01  6.37230782e+00\n",
      " -1.53587617e+00  3.12047104e+00  1.06487007e+01  5.69648788e+00\n",
      "  1.05994040e+01  5.88307601e+00  4.95546546e+00  4.85945897e-01\n",
      "  8.39019288e+00  8.26699979e+00  6.35991597e+00  2.93807489e+00\n",
      "  5.54631063e+00 -1.69455464e+00  8.33578224e+00 -4.46686895e-01\n",
      "  2.54381873e+00  1.10554915e+01  1.01994774e+01  1.59397640e+00\n",
      "  3.12919673e+00  5.52549244e+00  6.43662940e+00 -7.62819742e-01\n",
      "  8.55121326e-01  5.92091366e+00 -3.97182318e+00  5.28578225e+00\n",
      "  1.69442426e+00  7.47887525e+00  1.98467708e+00 -7.53939925e-01\n",
      "  5.56017616e+00  6.37931543e+00  5.37116706e+00  6.34592112e+00\n",
      "  3.15688523e+00 -2.26378010e+00  1.38939528e+00  6.11072787e+00\n",
      "  3.53157040e+00  4.03513003e+00  5.24315626e+00 -1.54757516e-01\n",
      "  7.15714970e+00  6.95771181e+00  6.36626509e-01  1.07324588e+01\n",
      " -1.65262410e-01  3.71031895e+00  5.03826542e+00  1.43139432e+00\n",
      "  1.18828647e+01  1.76464256e-01 -4.86646965e+00 -1.69074382e+00\n",
      "  8.43987871e-01  9.66159413e+00  9.41747337e+00  7.16130646e+00\n",
      "  3.69528581e+00  1.22501457e+01  1.43925025e+00  4.59088380e+00\n",
      "  1.92819342e+00  8.89225827e+00 -2.71631305e+00  6.68506857e-01\n",
      "  8.29619588e+00 -8.31828334e-01  3.49443294e-01  9.16376459e+00\n",
      "  1.13021688e+01  7.56842795e+00  1.21920716e+01  6.72005110e+00\n",
      "  5.49565455e+00  2.63437206e+00  8.43087443e+00  3.60655220e+00\n",
      "  1.09485198e+01 -2.90199557e+00  6.80211286e+00  5.78969283e+00\n",
      "  8.01708910e+00  3.89262513e+00  5.57617427e+00  1.35235664e+00\n",
      "  2.80848076e+00  1.02415910e+01  4.30995112e+00  2.49085853e-01\n",
      "  2.54041994e+00 -1.25053851e+00  3.10460751e+00  6.43998052e+00\n",
      "  1.65974052e+00 -2.71498808e+00  1.28428682e+01  9.03328556e+00\n",
      "  2.78145314e+00  3.76071670e+00  6.38580883e+00  1.66443024e-01\n",
      "  5.04264512e+00  8.46178436e+00  5.83624163e+00  4.80207136e+00\n",
      "  7.37240860e+00  1.56604265e+00  7.11696085e+00 -8.31120667e+00\n",
      "  1.17751972e+01  7.94740727e+00  5.16272174e+00  6.01233911e+00\n",
      "  1.03101455e+01  5.01251349e+00  1.27281249e+01  5.16671311e-01\n",
      "  4.20309069e+00  8.76153519e-01  2.92158737e+00  1.50564324e+00\n",
      " -6.35184234e-01  5.88027286e+00  1.02393855e+01  3.54586536e+00\n",
      "  1.08159135e+01  1.28068141e+01  4.35133224e+00  1.19772907e+01\n",
      "  5.82416670e-01  5.32630138e+00  5.68998885e+00  7.09267719e+00\n",
      "  4.75388405e-01  4.25119527e+00  1.08489431e+01  9.98329853e+00\n",
      "  9.64737305e+00  4.91640867e+00  2.42894252e-01  7.36749108e+00\n",
      "  6.93681465e+00  5.97647335e+00  3.81226193e+00  1.18133170e+01\n",
      " -1.14743679e-02  1.02711926e+01 -5.96011368e+00  4.02212098e+00\n",
      "  5.14951434e+00  4.64820359e+00  8.54690379e+00  1.54309549e+01\n",
      "  1.11464351e+01  9.27836980e+00  6.41437041e+00  1.27786748e+00\n",
      "  1.14095269e+01  1.42062175e+01  1.41979401e+01  8.22192590e+00\n",
      "  6.65876894e+00  4.45318184e-01  6.83792971e+00  4.56630811e+00\n",
      "  5.74245059e+00 -2.24424280e+00  6.66631721e+00  3.65173343e+00\n",
      "  5.33302836e+00  6.64842989e+00 -2.95744747e+00  6.24224431e+00\n",
      "  8.25520234e+00  9.46977572e+00  8.09995331e+00  3.09075149e+00\n",
      "  5.47280688e+00  2.60652164e+00  8.01446156e+00  6.24238634e+00\n",
      "  3.15362704e+00  2.78414299e+00  1.81109354e+00  2.67037232e+00\n",
      " -4.21426035e-01  6.01903285e+00  3.86805056e+00  5.26277786e+00\n",
      "  8.26536946e+00  1.13155672e+01  9.77889957e+00  4.64953328e+00\n",
      "  3.17829731e-01  6.28924156e+00  1.04397593e+01  3.37860480e+00\n",
      "  6.95082591e+00  1.08047289e+01  7.67136442e+00  1.19384783e+01\n",
      "  1.12995040e+01  1.64017522e+00  2.18619961e+00  8.66688704e-01\n",
      "  8.00091851e+00  8.62688157e+00  8.12959132e+00  3.00177093e+00\n",
      "  7.50816218e+00  8.75579722e+00  6.39341491e+00  1.29740787e+01\n",
      "  6.46665591e+00  5.74039624e+00  5.64011459e+00 -7.78425068e+00\n",
      "  5.98744935e+00  2.74520106e+00  3.30369006e+00  7.59859550e+00\n",
      "  2.49013637e+00  3.03929323e-02  3.74791718e+00  6.51495003e+00\n",
      " -2.77612007e+00 -5.86483615e-01  5.01568680e+00  2.00720425e+00\n",
      "  4.28378544e+00  1.06418378e+01  9.32647739e+00  8.74894114e+00\n",
      "  1.95850188e+00  1.03749892e+00  1.01896814e+01  9.18824851e+00\n",
      " -3.29814421e+00  1.23680921e+01  4.15515016e+00  4.72130404e+00\n",
      "  7.25857383e+00  4.90905519e+00  3.18181699e+00  1.16604829e+01\n",
      "  9.97521249e+00  4.05648409e-01 -1.70614709e+00  5.51773090e+00\n",
      "  3.27184574e+00  8.11803388e+00  1.19560618e+01  3.94386798e+00\n",
      "  6.13677790e+00  2.01015514e+00  9.01903724e+00  9.88458540e+00\n",
      "  2.08766316e+00  6.42527102e+00  4.63163988e+00 -2.23745329e+00\n",
      "  3.93614590e+00  3.76714582e+00  9.84375387e-01 -7.19465868e-01\n",
      "  1.10347972e+01  1.23287874e+01  6.66755417e+00  1.09451708e+01\n",
      "  3.69642571e+00  7.29172592e+00  1.37352884e+01  1.35635371e+01\n",
      " -9.31735060e-01  6.24644629e+00  5.78112838e+00  5.10101437e+00\n",
      "  6.09167890e+00 -1.33402163e+00 -6.08870695e+00  6.71133805e+00\n",
      "  1.16393571e+01 -1.27275829e+00 -2.76417792e-01  3.73944751e+00\n",
      " -3.94125524e-01  8.17103070e+00  1.28977442e+01  8.78174367e+00\n",
      "  1.09542171e+00  6.69995427e+00  5.31055569e+00  2.66587185e+00\n",
      " -7.84483270e-01  2.49275160e+00  4.82915898e+00  7.69465931e+00\n",
      "  4.40927442e+00  4.25388942e+00  1.11846108e+01  3.90111349e+00\n",
      "  4.53659201e+00  1.15568769e+01  3.79163842e+00  1.71640626e+00\n",
      "  1.16430928e+01  1.13624002e+01  5.92522461e+00  5.42139888e+00\n",
      "  7.67289966e+00  7.80218803e+00  4.37649109e+00  8.43269208e+00\n",
      "  6.06138951e+00  1.94383695e+00  2.92067671e+00  1.02599230e+01\n",
      " -1.03426722e+00  3.91456535e+00  1.06764473e+01  2.10423764e+00\n",
      "  5.48705584e+00  1.09270460e+01  5.48093809e+00  1.74802182e+01\n",
      "  1.08658677e+01  9.65769871e+00  6.08230272e+00  3.81728853e+00\n",
      "  1.07156522e+01  5.04423962e+00 -1.77944779e+00  6.00782819e+00\n",
      "  3.26213727e+00  8.41261080e+00 -5.45976992e-01  2.03889416e+00\n",
      " -1.86598624e+00  6.79752706e+00  7.57970501e+00 -3.26109226e-01\n",
      "  7.95208407e+00  5.90735617e+00  1.38192967e+00  8.70097983e+00\n",
      "  4.68270768e+00  9.67023947e+00  7.83598242e+00  4.96569583e+00\n",
      "  2.15491845e+00  8.19519766e+00  7.02987734e+00  8.26003443e+00\n",
      "  2.28274363e+00  2.21087401e+00  3.81737762e+00 -2.78459000e+00\n",
      "  4.47439317e+00  1.41070820e+01  1.14370333e+01  3.87996203e+00\n",
      "  3.08378664e+00 -5.55599278e+00  1.48517478e+00  7.72595622e+00\n",
      "  3.62058269e+00  9.44512290e+00  5.90094630e+00  1.16626903e+01\n",
      "  2.75221915e+00  7.48488140e+00  5.41551172e+00  4.53503420e+00\n",
      " -2.71478050e-01  1.46896971e+00  5.89154839e+00  9.70334918e+00\n",
      "  4.71062165e+00  7.98068815e+00  2.11556720e+00  3.63867230e+00\n",
      " -3.18540006e+00 -3.15987250e+00  2.59187918e+00  1.39651673e+01\n",
      "  8.40671335e+00  1.80751029e+00  7.29070742e+00  2.75997016e+00\n",
      "  8.10243055e+00  9.23207910e-01 -4.32138864e+00  7.83893899e+00\n",
      " -8.66938308e+00  6.06693794e+00  4.61739700e+00  5.64293591e+00\n",
      "  1.53232318e+00 -2.88432570e+00  8.00856615e+00  1.02107032e+01\n",
      "  6.94527427e+00  4.15847742e+00  3.68134518e+00  4.19384875e+00\n",
      "  5.42593559e+00  5.91031291e+00  6.26825792e+00  7.02249009e+00\n",
      "  4.68050903e-01  2.46481810e+00  3.30070347e+00 -7.37073571e-02\n",
      "  1.24376156e+01  2.72778441e+00  7.77292035e+00  9.24329484e-01\n",
      "  8.35072938e+00  2.14386065e+00 -1.94976637e+00  1.41796898e+00\n",
      "  4.25692967e+00  9.86472551e+00  4.54112318e+00  1.12184212e+00\n",
      "  2.72800174e+00  6.33984111e+00  6.12390491e+00  3.20379682e+00\n",
      "  3.91215426e+00  1.14114630e+00  2.73732383e+00  7.08230376e+00\n",
      "  1.80877736e+00  4.99958374e+00  9.32311843e+00  4.11539536e+00\n",
      " -5.00016042e-01  1.71497404e+00  8.34911950e+00  9.52127570e+00\n",
      "  7.60550487e+00  1.38778206e+01  4.71459077e-01  7.86658451e+00\n",
      "  9.70073096e+00  3.28214851e+00  4.16285298e+00  6.24228366e+00\n",
      "  1.35879566e+00  5.80208737e+00  6.81758503e+00  6.56236810e+00\n",
      " -2.07651855e+00  1.56461808e+00  4.15804375e+00  1.23401453e+01\n",
      "  1.02808697e+01  8.59588898e+00  4.07414083e+00  7.15353115e-01\n",
      "  2.66164998e+00  2.25764386e-01  7.42877770e+00  2.36311941e+00\n",
      "  2.57643763e+00  1.50830019e+01  8.73835443e+00  6.97575275e+00\n",
      " -6.92667751e+00  6.85188072e+00  3.22261140e+00  6.16707137e-01\n",
      "  4.82252683e+00 -6.25829502e-01  9.53131667e+00  1.28382648e+00\n",
      "  2.50075242e+00  2.44272944e+00  2.47426230e+00 -2.85183004e+00\n",
      "  7.66600397e+00  8.89231334e+00  7.14087856e+00  1.05971702e+01\n",
      " -3.88105621e+00  4.98066337e+00  1.94265445e+00 -3.09507850e+00\n",
      "  6.52100330e+00  4.38376928e+00 -2.72090403e+00  9.90464698e+00\n",
      "  4.34012027e+00  4.47575349e+00  1.22405764e+01  1.40895493e+01\n",
      "  4.79848442e+00  6.44174978e+00  7.64495600e+00  1.06420512e+00\n",
      "  5.71102693e+00  5.35617520e+00  6.55090183e+00  3.22687798e+00\n",
      "  6.75389970e+00  6.68303233e+00  1.31396027e+01  4.72307851e+00\n",
      " -9.83295378e-01  4.09432013e+00 -1.57946886e+00  4.29713121e+00\n",
      "  8.00081452e+00  2.42984198e+00  4.64281873e-02  9.57778682e+00\n",
      "  2.97826771e+00  9.63381581e+00  5.51854735e+00  4.08434461e+00\n",
      "  5.66227981e+00  1.31303596e+01  8.10225863e+00  1.43170630e+00\n",
      "  1.29658996e+01  7.03005375e+00 -1.52681257e+00  9.29194914e+00\n",
      " -2.43027597e+00  7.34683705e+00  6.84723652e+00  4.26541846e+00\n",
      "  8.30143187e+00  7.45799088e+00  5.90705449e+00  2.90306211e+00\n",
      "  3.87742357e-01  7.40379477e+00  8.52409261e+00  2.37066611e+00\n",
      "  2.11763171e+00  6.10299041e-01  7.13868895e-01  2.14464955e+00\n",
      "  7.49266246e+00  4.12370748e+00  1.19950209e+01  8.07683216e+00\n",
      "  9.95979511e-01  4.39265360e-01  9.66270881e+00  3.30842336e+00\n",
      "  1.19074335e+00 -2.26451675e+00 -7.52964864e-01  4.14557828e+00\n",
      "  4.47507448e+00  2.49783962e+00  2.88662726e-01  4.43791452e+00\n",
      "  3.27589550e+00  8.13565186e-01  4.36563886e+00  4.37175794e+00\n",
      "  8.71741194e+00  9.43104831e+00 -1.01121801e+00  3.14967195e-01\n",
      "  3.11632149e-01  1.94080836e+00  6.64668748e+00  1.35048459e+01\n",
      "  6.60679257e+00  5.13523409e+00  4.85248601e+00 -5.10563218e+00\n",
      "  7.64925321e+00  3.79100248e+00  3.73035265e+00  8.85883101e-02\n",
      " -4.54676117e+00  1.87066720e+00  6.80102725e+00 -3.09060834e-01\n",
      "  8.70037786e+00  2.37214332e+00  1.25192608e+00  2.77982567e+00\n",
      "  6.27164733e+00  3.42827462e+00  1.12142165e+01  9.62019655e+00\n",
      "  2.08422008e-01  1.05257069e+01  9.12288119e-01  8.64585657e+00\n",
      "  7.59543122e+00  3.07637851e+00  1.63297366e+01  8.64942086e+00\n",
      "  4.30247278e+00  2.67097952e+00  1.87188000e+00 -1.44035103e-01\n",
      "  5.98391999e+00  2.94659790e+00  6.30698742e+00  6.67869126e+00\n",
      "  3.49003098e+00 -6.38551241e-01  9.98270653e+00  3.89368774e+00\n",
      "  3.93058856e+00  6.62945679e+00  8.45690442e-01 -5.87115807e-01\n",
      "  6.23700066e+00  5.70280967e+00  4.93546710e+00  1.10933454e+01\n",
      "  8.11032244e+00  6.33239499e+00  9.20189587e+00  1.22606269e+01\n",
      "  1.42780030e+00  1.18661904e+01  1.64390750e+00 -3.02960455e-01\n",
      "  1.19836562e+01  1.54034999e+00  1.96002677e+00  4.50831914e+00\n",
      "  1.13191261e+01  1.64303560e+00  2.28214401e+00  1.68651717e+00\n",
      "  2.84491189e+00  4.44766938e+00  2.28058629e+00  2.19171663e+00\n",
      "  4.97359122e+00  7.74247172e+00  4.67899366e+00  1.32898786e+01\n",
      "  1.14487738e+01 -6.14838497e-02  6.31410730e+00  9.91717825e+00\n",
      "  1.04294341e+01  9.55368789e+00  2.40852034e+00 -4.51079894e+00\n",
      "  5.46278265e+00  5.95085550e+00 -6.60799080e-01  1.83563718e+00\n",
      "  9.12150946e+00  9.55010237e+00  6.72545051e+00  8.37465637e+00\n",
      "  1.04171975e+01  4.08128223e+00  2.56653721e+00  4.20770732e+00\n",
      "  9.32988510e+00  4.96463198e+00  4.51020190e+00  6.34860592e+00\n",
      "  4.10232467e+00  4.27632950e+00  7.52154288e+00  1.98399753e+00\n",
      "  1.18573438e+01  8.99834437e+00  9.07059477e+00  4.80022370e-01\n",
      "  8.57902264e+00 -2.83895956e+00  6.53868301e+00  1.46766688e+00\n",
      "  1.41083454e+00  4.92919416e+00  9.13797138e+00  8.84383753e+00\n",
      "  8.71967735e+00  1.74783917e-01  1.04376554e+01 -4.95422492e+00\n",
      "  8.39080973e+00  4.64435450e+00 -6.70500471e-01  4.08431274e-01\n",
      "  6.14058622e+00  7.97328027e+00  1.02356838e+00  5.98941197e+00\n",
      "  8.12241293e-01  1.23444414e+01  6.71701453e+00  2.36077281e+00\n",
      "  7.03569030e+00  1.47820449e+01 -1.48921065e+00  2.07572826e+00\n",
      "  6.54492962e+00  2.27339157e+01  3.35118543e+00  6.64020569e+00\n",
      " -1.28566609e+00  6.20063015e+00  1.57830775e+00  6.88947986e+00\n",
      "  2.45568160e+00  1.12251649e+01  4.97289275e+00  6.23335222e+00\n",
      "  6.01089410e+00  8.63421468e+00  6.91775281e-01  9.69039060e+00\n",
      "  9.58659454e+00  1.76325710e+00  1.53590506e+01 -1.31581627e+00\n",
      " -8.13554612e-01  4.87722299e+00  9.90649635e+00  4.04940607e+00\n",
      "  8.88091399e+00  1.06364949e+01 -6.36763348e-01  5.94341316e+00\n",
      "  1.13047383e+01  6.38408083e+00  2.97212023e+00  3.64944724e+00\n",
      " -3.42668473e+00  5.63379417e+00  3.21161008e+00  1.11397282e+00]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, true_weights, true_bias)\n",
    "print(\"y_pred:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "   # Your code here\n",
    "    errors = y_true - y_pred\n",
    "    mse = np.mean(errors ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.26357415817355806\n"
     ]
    }
   ],
   "source": [
    "loss = compute_mse(y, predict(X, true_weights, true_bias))\n",
    "print(f\"MSE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n_samples = X.shape[0]\n",
    "    errors = y_pred - y\n",
    "    \n",
    "    gradient_w = (2/n_samples) * (X.T @ errors)\n",
    "    gradient_b = (2/n_samples) * np.sum(errors)\n",
    "    return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01697225  0.02900164  0.03877828] 0.018719217786772317\n"
     ]
    }
   ],
   "source": [
    "gradient_w, gradient_b = compute_gradients(X, y, predict(X, true_weights, true_bias))\n",
    "print(gradient_w, gradient_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), ())"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, gradient_w.shape, gradient_b.shape # Should be ((1000, 3), (3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-4.64490711,  7.34766733, -2.93368645]), np.float64(-10.345117900587644))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, gradient_w, gradient_b) # Initial predictions\n",
    "print(compute_gradients(X, y, y_pred)) # Should print gradients (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step = 25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y, \n",
    "    learning_rate=0.01, \n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + 5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.1, n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ =train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
